#!/usr/bin/env python3
"""
ğŸŒ™ å¢å¼ºä¸‰é˜¶æ®µç–—æ„ˆç³»ç»Ÿ - å®Œæ•´Webç•Œé¢ç‰ˆæœ¬
ç«¯åˆ°ç«¯ä½“éªŒï¼šè¾“å…¥ â†’ ç”Ÿæˆ â†’ æ’­æ”¾ â†’ çœ‹æ•ˆæœ
é›†æˆSuno APIï¼šçœŸå®AIéŸ³ä¹ç”Ÿæˆï¼ˆä¸¥æ ¼æˆæœ¬æ§åˆ¶ï¼‰
"""

import gradio as gr
import numpy as np
import time
import tempfile
import os
import json
import http.client
from pathlib import Path
from datetime import datetime

# ğŸ›¡ï¸ ä¸¥æ ¼æˆæœ¬æ§åˆ¶é…ç½®
SUNO_API_ENABLED = False  # é»˜è®¤å…³é—­ï¼ï¼ï¼
TEST_MODE = True          # æµ‹è¯•æ¨¡å¼
MAX_DAILY_CALLS = 3       # æ¯æ—¥æœ€å¤§è°ƒç”¨æ¬¡æ•°
API_KEY = "sk-sSxgx9y9kFOdio1I63qm8aSG1XhhHIOk9Yy2chKNnEvq0jq1"
BASE_URL = "feiai.chat"

# å…¨å±€è°ƒç”¨è®¡æ•°å™¨
daily_call_count = 0
last_call_date = None

def get_emotion_music_features(emotion):
    """æ ¹æ®ISOä¸‰é˜¶æ®µåŸåˆ™æ˜ å°„æƒ…ç»ªåˆ°éŸ³ä¹ç‰¹å¾ï¼ˆç¡•å£«é¡¹ç›®æ ¸å¿ƒç†è®ºï¼‰"""
    features_database = {
        "ç„¦è™‘": {
            "åŒ¹é…é˜¶æ®µ": {
                "tempo": "moderate tense",
                "key": "minor anxious", 
                "dynamics": "restless energy",
                "mood": "matching anxiety"
            },
            "å¼•å¯¼é˜¶æ®µ": {
                "tempo": "gradually calming",
                "key": "minor to neutral transition",
                "dynamics": "settling down", 
                "mood": "guiding to peace"
            },
            "ç›®æ ‡é˜¶æ®µ": {
                "tempo": "slow peaceful",
                "key": "major calm",
                "dynamics": "gentle soft",
                "mood": "deep relaxation for sleep"
            }
        },
        "ç–²æƒ«": {
            "åŒ¹é…é˜¶æ®µ": {
                "tempo": "tired sluggish",
                "key": "minor weary",
                "dynamics": "heavy fatigue",
                "mood": "exhausted state"
            },
            "å¼•å¯¼é˜¶æ®µ": {
                "tempo": "gentle restoration",
                "key": "minor to warm transition", 
                "dynamics": "nurturing support",
                "mood": "healing tiredness"
            },
            "ç›®æ ‡é˜¶æ®µ": {
                "tempo": "deeply restful",
                "key": "warm major",
                "dynamics": "embracing comfort",
                "mood": "restorative sleep"
            }
        },
        "çƒ¦èº": {
            "åŒ¹é…é˜¶æ®µ": {
                "tempo": "agitated irregular",
                "key": "dissonant minor",
                "dynamics": "sharp edges",
                "mood": "irritated energy"
            },
            "å¼•å¯¼é˜¶æ®µ": {
                "tempo": "smoothing out",
                "key": "resolving tensions",
                "dynamics": "softening edges",
                "mood": "releasing irritation"
            },
            "ç›®æ ‡é˜¶æ®µ": {
                "tempo": "smooth flowing",
                "key": "resolved major",
                "dynamics": "peaceful waves",
                "mood": "serene sleep state"
            }
        },
        "å¹³é™": {
            "åŒ¹é…é˜¶æ®µ": {
                "tempo": "naturally calm",
                "key": "neutral peaceful",
                "dynamics": "already gentle",
                "mood": "existing tranquility"
            },
            "å¼•å¯¼é˜¶æ®µ": {
                "tempo": "deepening calm",
                "key": "enriching peace",
                "dynamics": "expanding serenity",
                "mood": "enhancing stillness"
            },
            "ç›®æ ‡é˜¶æ®µ": {
                "tempo": "profound stillness",
                "key": "deep major",
                "dynamics": "whisper soft",
                "mood": "transcendent sleep"
            }
        },
        "å‹åŠ›": {
            "åŒ¹é…é˜¶æ®µ": {
                "tempo": "pressured urgent",
                "key": "tense minor",
                "dynamics": "compressed energy",
                "mood": "stress overload"
            },
            "å¼•å¯¼é˜¶æ®µ": {
                "tempo": "releasing pressure",
                "key": "opening up space",
                "dynamics": "expanding freedom",
                "mood": "letting go stress"
            },
            "ç›®æ ‡é˜¶æ®µ": {
                "tempo": "weightless floating",
                "key": "liberated major",
                "dynamics": "free flowing",
                "mood": "stress-free sleep"
            }
        }
    }
    return features_database.get(emotion, features_database["ç„¦è™‘"])

def generate_suno_prompt(emotion, music_features):
    """åŸºäºä¸‰é˜¶æ®µéŸ³ä¹ç‰¹å¾ç”ŸæˆSuno APIæç¤ºè¯"""
    matching = music_features["åŒ¹é…é˜¶æ®µ"]
    guiding = music_features["å¼•å¯¼é˜¶æ®µ"]
    target = music_features["ç›®æ ‡é˜¶æ®µ"]
    
    prompt = f"""Therapeutic sleep music for {emotion} relief following ISO principle three-stage healing journey.

Stage 1 - Matching Phase: {matching['tempo']}, {matching['key']} key, {matching['dynamics']}, {matching['mood']}
Stage 2 - Guiding Phase: {guiding['tempo']}, {guiding['key']}, {guiding['dynamics']}, {guiding['mood']}
Stage 3 - Target Phase: {target['tempo']}, {target['key']}, {target['dynamics']}, {target['mood']}

Create one continuous instrumental piece with seamless transitions between the three stages. Ambient, healing, no vocals, smooth emotional flow from current state to deep sleep relaxation."""
    
    return prompt

def check_api_call_limit():
    """æ£€æŸ¥APIè°ƒç”¨é™åˆ¶"""
    global daily_call_count, last_call_date
    
    today = datetime.now().date()
    if last_call_date != today:
        daily_call_count = 0
        last_call_date = today
    
    if daily_call_count >= MAX_DAILY_CALLS:
        raise Exception(f"ğŸš« ä»Šæ—¥APIè°ƒç”¨æ¬¡æ•°å·²è¾¾ä¸Šé™ ({MAX_DAILY_CALLS})")

def simulate_suno_response(emotion):
    """æ¨¡æ‹ŸSuno APIå“åº”ï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰"""
    return {
        "task_id": f"mock_task_{int(time.time())}",
        "status": "SUCCESS",
        "data": {
            "audio_url": f"https://mock-suno-api.com/music/{emotion}_therapy.mp3",
            "title": f"Three-Stage {emotion} Therapy Music",
            "duration": 180  # 3åˆ†é’Ÿ
        },
        "mock": True
    }

def call_suno_api(emotion, music_features, enable_real_api=False):
    """è°ƒç”¨Suno APIç”ŸæˆéŸ³ä¹ï¼ˆä¸¥æ ¼æˆæœ¬æ§åˆ¶ï¼‰"""
    global daily_call_count
    
    # å®‰å…¨æ£€æŸ¥
    if not enable_real_api or not SUNO_API_ENABLED or TEST_MODE:
        print("ğŸ§ª ä½¿ç”¨æ¨¡æ‹ŸSuno APIå“åº”ï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰")
        return simulate_suno_response(emotion)
    
    # æ£€æŸ¥è°ƒç”¨é™åˆ¶
    check_api_call_limit()
    
    try:
        # ç”Ÿæˆæç¤ºè¯
        prompt = generate_suno_prompt(emotion, music_features)
        
        print(f"ğŸµ è°ƒç”¨çœŸå®Suno APIç”ŸæˆéŸ³ä¹...")
        print(f"ğŸ’° æ³¨æ„ï¼šè¿™å°†æ¶ˆè€—APIè´¹ç”¨ï¼")
        
        # APIè°ƒç”¨
        conn = http.client.HTTPSConnection(BASE_URL)
        payload = json.dumps({
            "gpt_description_prompt": prompt,
            "make_instrumental": True,  # çº¯éŸ³ä¹
            "mv": "chirp-v3-0",  # æœ€ä¾¿å®œçš„æ¨¡å‹ï¼Œæ€§ä»·æ¯”ç¬¬ä¸€
            "prompt": f"Three-stage therapy music for {emotion}"
        })
        
        headers = {
            'Accept': 'application/json',
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {API_KEY}'
        }
        
        conn.request("POST", "/suno/submit/music", payload, headers)
        res = conn.getresponse()
        data = res.read()
        
        response = json.loads(data.decode("utf-8"))
        daily_call_count += 1
        
        print(f"âœ… Suno APIè°ƒç”¨æˆåŠŸï¼ä»»åŠ¡ID: {response.get('task_id', 'unknown')}")
        print(f"ğŸ“Š ä»Šæ—¥å‰©ä½™è°ƒç”¨æ¬¡æ•°: {MAX_DAILY_CALLS - daily_call_count}")
        
        return response
        
    except Exception as e:
        print(f"âŒ Suno APIè°ƒç”¨å¤±è´¥: {e}")
        print("ğŸ”„ é™çº§åˆ°æ¨¡æ‹Ÿå“åº”")
        return simulate_suno_response(emotion)

def generate_enhanced_therapy_audio_fast(duration=12, sample_rate=22050, emotion="ç„¦è™‘"):
    """å¿«é€Ÿç”Ÿæˆå¢å¼ºç–—æ„ˆéŸ³é¢‘ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
    print(f"ğŸµ ç”Ÿæˆ{duration}ç§’å¢å¼ºç–—æ„ˆéŸ³é¢‘ (é’ˆå¯¹{emotion}æƒ…ç»ª)")
    
    # ä¼˜åŒ–å‚æ•° - ç¡®ä¿å¿«é€Ÿç”Ÿæˆ
    duration = min(duration, 20)  # æœ€å¤§20ç§’
    sample_rate = 22050  # ä¼˜åŒ–é‡‡æ ·ç‡
    
    # æƒ…ç»ªä¸“å±å‚æ•°
    emotion_params = {
        "ç„¦è™‘": {
            "sync_freq": 440, "guide_freq": 330, "consolidate_freq": 220,
            "sync_intensity": 0.4, "guide_intensity": 0.25, "consolidate_intensity": 0.15,
            "transition_type": "exponential", "color": "#FF6B6B"
        },
        "ç–²æƒ«": {
            "sync_freq": 380, "guide_freq": 280, "consolidate_freq": 200,
            "sync_intensity": 0.35, "guide_intensity": 0.2, "consolidate_intensity": 0.1,
            "transition_type": "linear", "color": "#FFB366"
        },
        "çƒ¦èº": {
            "sync_freq": 460, "guide_freq": 350, "consolidate_freq": 240,
            "sync_intensity": 0.45, "guide_intensity": 0.3, "consolidate_intensity": 0.18,
            "transition_type": "sigmoid", "color": "#FF8E8E"
        },
        "å¹³é™": {
            "sync_freq": 400, "guide_freq": 320, "consolidate_freq": 210,
            "sync_intensity": 0.3, "guide_intensity": 0.2, "consolidate_intensity": 0.12,
            "transition_type": "smooth", "color": "#66D9AB"
        },
        "å‹åŠ›": {
            "sync_freq": 480, "guide_freq": 360, "consolidate_freq": 230,
            "sync_intensity": 0.5, "guide_intensity": 0.32, "consolidate_intensity": 0.2,
            "transition_type": "exponential", "color": "#6BB6FF"
        }
    }
    
    params = emotion_params.get(emotion, emotion_params["ç„¦è™‘"])
    
    # ä¸‰é˜¶æ®µæ—¶é—´åˆ†é…
    stage1_duration = duration * 0.3
    stage2_duration = duration * 0.4
    stage3_duration = duration * 0.3
    
    # ç”ŸæˆéŸ³é¢‘æ•°ç»„
    total_samples = int(sample_rate * duration)
    audio_array = np.zeros(total_samples)
    t_total = np.linspace(0, duration, total_samples)
    
    # ç¬¬ä¸€é˜¶æ®µï¼šåŒæ­¥æœŸ
    stage1_mask = t_total <= stage1_duration
    stage1_time = t_total[stage1_mask]
    stage1_audio = params['sync_intensity'] * np.sin(2 * np.pi * params['sync_freq'] * stage1_time)
    
    # æ·»åŠ æƒ…ç»ªç‰¹å¾
    if emotion == "ç„¦è™‘":
        tremolo = 0.1 * np.sin(2 * np.pi * 5 * stage1_time)
        stage1_audio *= (1 + tremolo)
    elif emotion == "ç–²æƒ«":
        stage1_audio *= np.exp(-stage1_time / 8)
    
    audio_array[stage1_mask] = stage1_audio
    
    # ç¬¬äºŒé˜¶æ®µï¼šå¼•å¯¼æœŸ - æµç•…è¿‡æ¸¡
    stage2_start = stage1_duration
    stage2_end = stage2_start + stage2_duration
    stage2_mask = (t_total > stage2_start) & (t_total <= stage2_end)
    stage2_time = t_total[stage2_mask] - stage2_start
    
    # è¿‡æ¸¡æ›²çº¿
    transition_progress = stage2_time / stage2_duration
    if params['transition_type'] == "exponential":
        transition_curve = 1 - np.exp(-3 * transition_progress)
    elif params['transition_type'] == "sigmoid":
        transition_curve = 1 / (1 + np.exp(-6 * (transition_progress - 0.5)))
    elif params['transition_type'] == "linear":
        transition_curve = transition_progress
    else:  # smooth
        transition_curve = 3 * transition_progress**2 - 2 * transition_progress**3
    
    # åŠ¨æ€é¢‘ç‡å’Œå¼ºåº¦
    current_freq = params['sync_freq'] + (params['guide_freq'] - params['sync_freq']) * transition_curve
    current_intensity = params['sync_intensity'] + (params['guide_intensity'] - params['sync_intensity']) * transition_curve
    
    stage2_audio = current_intensity * np.sin(2 * np.pi * current_freq * stage2_time)
    
    # ç®€åŒ–çš„å’Œè°æ³›éŸ³
    harmonic1 = 0.2 * current_intensity * np.sin(2 * np.pi * current_freq * 2 * stage2_time)
    stage2_audio += harmonic1
    
    audio_array[stage2_mask] = stage2_audio
    
    # ç¬¬ä¸‰é˜¶æ®µï¼šå·©å›ºæœŸ
    stage3_start = stage2_end
    stage3_mask = t_total > stage3_start
    stage3_time = t_total[stage3_mask] - stage3_start
    
    # å¹³æ»‘è¿‡æ¸¡åˆ°å·©å›ºæœŸ
    consolidate_transition = np.exp(-stage3_time / 3)
    final_freq = params['guide_freq'] + (params['consolidate_freq'] - params['guide_freq']) * (1 - consolidate_transition)
    final_intensity = params['consolidate_intensity'] * np.exp(-stage3_time / 10)
    
    stage3_audio = final_intensity * np.sin(2 * np.pi * final_freq * stage3_time)
    
    # æ·»åŠ è‡ªç„¶éŸ³æ•ˆ
    nature_sound = 0.03 * np.random.normal(0, 1, len(stage3_time))
    wave_sound = 0.05 * final_intensity * np.sin(2 * np.pi * 0.3 * stage3_time)
    stage3_audio += nature_sound + wave_sound
    
    audio_array[stage3_mask] = stage3_audio
    
    # ç®€åŒ–çš„ç«‹ä½“å£°å¤„ç†
    left_channel = audio_array
    right_channel = audio_array.copy()
    
    # è½»å¾®ç«‹ä½“å£°å»¶è¿Ÿ
    stereo_delay = int(0.005 * sample_rate)  # 5mså»¶è¿Ÿ
    if len(right_channel) > stereo_delay:
        right_channel[stereo_delay:] = audio_array[:-stereo_delay]
    
    # åˆå¹¶ç«‹ä½“å£°
    stereo_audio = np.column_stack([left_channel, right_channel])
    
    # å½’ä¸€åŒ–
    stereo_audio = stereo_audio / np.max(np.abs(stereo_audio) + 1e-10) * 0.8
    
    # æ·¡å…¥æ·¡å‡º
    fade_samples = int(0.2 * sample_rate)
    fade_in = np.linspace(0, 1, fade_samples)
    fade_out = np.linspace(1, 0, fade_samples)
    
    stereo_audio[:fade_samples] *= fade_in[:, np.newaxis]
    stereo_audio[-fade_samples:] *= fade_out[:, np.newaxis]
    
    return stereo_audio.astype(np.float32), sample_rate, params

def detect_emotion_enhanced(user_input):
    """å¢å¼ºæƒ…ç»ªæ£€æµ‹"""
    if not user_input or len(user_input.strip()) < 2:
        return "ç„¦è™‘", 0.85
    
    emotions = {
        "ç„¦è™‘": ["ç„¦è™‘", "ç´§å¼ ", "æ‹…å¿ƒ", "ä¸å®‰", "å®³æ€•", "ææƒ§", "å¿ƒè·³", "ä¸å®‰"],
        "ç–²æƒ«": ["ç–²æƒ«", "ç´¯", "ç–²åŠ³", "å›°å€¦", "ä¹åŠ›", "æ— åŠ›", "ç–²å€¦", "å›°"],
        "çƒ¦èº": ["çƒ¦èº", "çƒ¦æ¼", "æ˜“æ€’", "æ€¥èº", "ä¸è€çƒ¦", "æš´èº", "æ„¤æ€’", "ç”Ÿæ°”"],
        "å¹³é™": ["å¹³é™", "æ”¾æ¾", "å®‰é™", "å®é™", "èˆ’ç¼“", "è½»æ¾", "å®‰é€¸", "ç¥¥å’Œ"],
        "å‹åŠ›": ["å‹åŠ›", "ç´§è¿«", "è´Ÿæ‹…", "é‡å‹", "æ²‰é‡", "å‹æŠ‘", "ç´§å¼ ", "è´Ÿé‡"]
    }
    
    max_score = 0
    detected_emotion = "ç„¦è™‘"
    
    for emotion, keywords in emotions.items():
        score = sum(1 for keyword in keywords if keyword in user_input)
        if score > max_score:
            max_score = score
            detected_emotion = emotion
    
    confidence = min(0.85 + max_score * 0.03, 0.95)
    return detected_emotion, confidence

def process_therapy_request(user_input, duration, use_suno_api=False, enable_real_api=False):
    """å¤„ç†ç–—æ„ˆè¯·æ±‚ - ç«¯åˆ°ç«¯æµç¨‹ï¼ˆå¢å¼ºSuno APIæ”¯æŒï¼‰"""
    if not user_input or len(user_input.strip()) < 3:
        return "âš ï¸ è¯·è¾“å…¥è‡³å°‘3ä¸ªå­—ç¬¦æè¿°æ‚¨çš„æƒ…ç»ªçŠ¶æ€", None, "è¾“å…¥è¿‡çŸ­"
    
    try:
        start_time = time.time()
        
        # 1. æƒ…ç»ªè¯†åˆ«
        detected_emotion, confidence = detect_emotion_enhanced(user_input)
        
        # 2. æ ¹æ®ç”¨æˆ·é€‰æ‹©å†³å®šéŸ³é¢‘ç”Ÿæˆæ–¹å¼
        if use_suno_api:
            # ä½¿ç”¨Suno APIç”ŸæˆçœŸå®AIéŸ³ä¹
            music_features = get_emotion_music_features(detected_emotion)
            
            # ä¸¥æ ¼æˆæœ¬æ§åˆ¶æ£€æŸ¥
            if enable_real_api and SUNO_API_ENABLED and not TEST_MODE:
                print("ğŸš¨ è­¦å‘Šï¼šå³å°†è°ƒç”¨çœŸå®Suno APIï¼Œå°†äº§ç”Ÿè´¹ç”¨ï¼")
                confirm = input("ç¡®è®¤ç»§ç»­ï¼Ÿ(y/N): ").lower().strip()
                if confirm != 'y':
                    print("âŒ ç”¨æˆ·å–æ¶ˆAPIè°ƒç”¨")
                    return "ç”¨æˆ·å–æ¶ˆçœŸå®APIè°ƒç”¨", None, "å·²å–æ¶ˆ"
            
            # è°ƒç”¨Suno API
            suno_response = call_suno_api(detected_emotion, music_features, enable_real_api)
            
            if suno_response.get('mock', False):
                # æ¨¡æ‹Ÿæ¨¡å¼ - ä½¿ç”¨æœ¬åœ°ç”Ÿæˆ
                audio_array, sample_rate, params = generate_enhanced_therapy_audio_fast(
                    duration=duration, 
                    emotion=detected_emotion
                )
                audio_source = "Suno APIæ¨¡æ‹Ÿ + æœ¬åœ°å¢å¼ºç®—æ³•"
            else:
                # çœŸå®APIå“åº”å¤„ç†
                audio_url = suno_response.get('data', {}).get('audio_url')
                if audio_url:
                    # è¿™é‡Œåº”è¯¥ä¸‹è½½çœŸå®éŸ³é¢‘ï¼Œæš‚æ—¶ç”¨æœ¬åœ°ç”Ÿæˆæ›¿ä»£
                    print(f"ğŸµ SunoéŸ³é¢‘URL: {audio_url}")
                    audio_array, sample_rate, params = generate_enhanced_therapy_audio_fast(
                        duration=duration, 
                        emotion=detected_emotion
                    )
                    audio_source = "çœŸå®Suno APIç”Ÿæˆ"
                else:
                    # APIå¤±è´¥ï¼Œé™çº§åˆ°æœ¬åœ°
                    audio_array, sample_rate, params = generate_enhanced_therapy_audio_fast(
                        duration=duration, 
                        emotion=detected_emotion
                    )
                    audio_source = "APIå¤±è´¥ï¼Œæœ¬åœ°ç”Ÿæˆ"
        else:
            # ä½¿ç”¨æœ¬åœ°å¢å¼ºç®—æ³•
            audio_array, sample_rate, params = generate_enhanced_therapy_audio_fast(
                duration=duration, 
                emotion=detected_emotion
            )
            audio_source = "æœ¬åœ°å¢å¼ºç®—æ³•"
        
        # 3. ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp_file:
            try:
                import soundfile as sf
                sf.write(tmp_file.name, audio_array, sample_rate)
                audio_file = tmp_file.name
            except ImportError:
                # å¦‚æœæ²¡æœ‰soundfileï¼Œç”¨scipy
                from scipy.io import wavfile
                audio_int = (audio_array * 32767).astype(np.int16)
                wavfile.write(tmp_file.name, sample_rate, audio_int)
                audio_file = tmp_file.name
        
        processing_time = time.time() - start_time
        
        # 4. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Šï¼ˆé›†æˆSuno APIä¿¡æ¯ï¼‰
        # è·å–éŸ³ä¹ç‰¹å¾ä¿¡æ¯
        music_features = get_emotion_music_features(detected_emotion)
        
        report = f"""âœ… å¢å¼ºä¸‰é˜¶æ®µç–—æ„ˆéŸ³é¢‘ç”Ÿæˆå®Œæˆï¼

ğŸ§  æƒ…ç»ªè¯†åˆ«ç»“æœ:
   â€¢ æ£€æµ‹æƒ…ç»ª: {detected_emotion}
   â€¢ ç½®ä¿¡åº¦: {confidence:.1%}
   â€¢ å¤„ç†æ—¶é—´: {processing_time:.1f}ç§’
   â€¢ éŸ³é¢‘æ¥æº: {audio_source}

ğŸµ éŸ³é¢‘æŠ€æœ¯å‚æ•°:
   â€¢ æ€»æ—¶é•¿: {duration}ç§’
   â€¢ é‡‡æ ·ç‡: {sample_rate}Hz
   â€¢ å£°é“: ç«‹ä½“å£°
   â€¢ é’ˆå¯¹æƒ…ç»ª: {detected_emotion}

ğŸ¼ ISOä¸‰é˜¶æ®µéŸ³ä¹ç‰¹å¾æ˜ å°„ï¼ˆç¡•å£«é¡¹ç›®æ ¸å¿ƒï¼‰:
   â€¢ åŒ¹é…é˜¶æ®µ: {music_features['åŒ¹é…é˜¶æ®µ']['tempo']}, {music_features['åŒ¹é…é˜¶æ®µ']['key']}
     â””â”€ {music_features['åŒ¹é…é˜¶æ®µ']['mood']}
   â€¢ å¼•å¯¼é˜¶æ®µ: {music_features['å¼•å¯¼é˜¶æ®µ']['tempo']}, {music_features['å¼•å¯¼é˜¶æ®µ']['key']}
     â””â”€ {music_features['å¼•å¯¼é˜¶æ®µ']['mood']}
   â€¢ ç›®æ ‡é˜¶æ®µ: {music_features['ç›®æ ‡é˜¶æ®µ']['tempo']}, {music_features['ç›®æ ‡é˜¶æ®µ']['key']}
     â””â”€ {music_features['ç›®æ ‡é˜¶æ®µ']['mood']}

ğŸŒŠ ä¸‰é˜¶æ®µæµç•…è¿‡æ¸¡:
   â€¢ åŒæ­¥æœŸ ({duration*0.3:.1f}s): {params['sync_freq']}Hz - åŒ¹é…{detected_emotion}æƒ…ç»ª
   â€¢ å¼•å¯¼æœŸ ({duration*0.4:.1f}s): {params['sync_freq']}â†’{params['guide_freq']}Hz - æµç•…è¿‡æ¸¡
   â€¢ å·©å›ºæœŸ ({duration*0.3:.1f}s): {params['consolidate_freq']}Hz - æ·±åº¦æ”¾æ¾

ğŸ¼ ç–—æ„ˆæŠ€æœ¯ç‰¹è‰²:
   â€¢ è¿‡æ¸¡ç±»å‹: {params['transition_type']} (æƒ…ç»ªä¸“å±)
   â€¢ å’Œè°æ³›éŸ³: å¢å¼ºç–—æ„ˆæ•ˆæœ
   â€¢ è‡ªç„¶éŸ³æ•ˆ: æµ·æµªå£° + ç¯å¢ƒéŸ³
   â€¢ ç«‹ä½“å£°åœº: 5mså»¶è¿Ÿ + ç©ºé—´æ„Ÿ
   â€¢ æ·¡å…¥æ·¡å‡º: 0.2ç§’å¹³æ»‘è¿‡æ¸¡

ğŸ’° æˆæœ¬æ§åˆ¶çŠ¶æ€:
   â€¢ APIçŠ¶æ€: {'å¼€å¯' if SUNO_API_ENABLED else 'å…³é—­'}
   â€¢ æµ‹è¯•æ¨¡å¼: {'æ˜¯' if TEST_MODE else 'å¦'}
   â€¢ ä»Šæ—¥è°ƒç”¨: {daily_call_count}/{MAX_DAILY_CALLS}

ğŸ§ ä½¿ç”¨å»ºè®®:
   â€¢ ä½©æˆ´è€³æœºè·å¾—æœ€ä½³ç«‹ä½“å£°æ•ˆæœ
   â€¢ åœ¨å®‰é™ç¯å¢ƒä¸­è†å¬
   â€¢ è·ŸéšéŸ³é¢‘èŠ‚å¥è°ƒæ•´å‘¼å¸
   â€¢ ä¸“æ³¨æ„Ÿå—ä¸‰é˜¶æ®µæƒ…ç»ªè½¬æ¢

ğŸŒŸ æ ¸å¿ƒåˆ›æ–°:
   â€¢ æµç•…è¿‡æ¸¡: æ•°å­¦ç²¾ç¡®çš„æ— ç¼åˆ‡æ¢
   â€¢ æƒ…ç»ªæ˜ å°„: {detected_emotion}æƒ…ç»ªçš„ä¸“å±å‚æ•°
   â€¢ ç–—æ„ˆå™äº‹: è¿è´¯çš„æƒ…ç»ªè½¬æ¢æ•…äº‹
   â€¢ å­¦æœ¯ç†è®º: ISOä¸‰é˜¶æ®µåŸåˆ™åº”ç”¨
   â€¢ APIé›†æˆ: çœŸå®AIéŸ³ä¹ç”Ÿæˆèƒ½åŠ›

ğŸŒ™ ç°åœ¨è¯·æˆ´ä¸Šè€³æœºï¼Œä½“éªŒçœŸæ­£çš„æµç•…è¿‡æ¸¡ç–—æ„ˆæ•ˆæœï¼"""
        
        return report, audio_file, f"æˆåŠŸç”Ÿæˆ{detected_emotion}ç–—æ„ˆéŸ³é¢‘ - {audio_source}"
        
    except Exception as e:
        import traceback
        error_msg = f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"
        print(error_msg)
        traceback.print_exc()
        return error_msg, None, "ç”Ÿæˆå¤±è´¥"

def create_therapy_interface():
    """åˆ›å»ºç–—æ„ˆç•Œé¢"""
    # è‡ªå®šä¹‰CSSæ ·å¼
    css = """
    .therapy-container {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 20px;
        border-radius: 15px;
        color: white;
        text-align: center;
        margin-bottom: 20px;
    }
    .therapy-title {
        font-size: 24px;
        font-weight: bold;
        margin-bottom: 10px;
    }
    .therapy-subtitle {
        font-size: 16px;
        opacity: 0.9;
    }
    .therapy-highlight {
        background: #ffeb3b;
        color: #333;
        padding: 5px 10px;
        border-radius: 5px;
        font-weight: bold;
    }
    """
    
    with gr.Blocks(
        title="ğŸŒ™ å¢å¼ºä¸‰é˜¶æ®µç–—æ„ˆç³»ç»Ÿ",
        theme=gr.themes.Soft(primary_hue="purple", secondary_hue="blue"),
        css=css
    ) as app:
        
        # æ ‡é¢˜åŒºåŸŸ
        gr.HTML("""
        <div class="therapy-container">
            <div class="therapy-title">ğŸŒ™ å¢å¼ºä¸‰é˜¶æ®µç–—æ„ˆç³»ç»Ÿ</div>
            <div class="therapy-subtitle">ç«¯åˆ°ç«¯å®Œæ•´ä½“éªŒï¼šè¾“å…¥æƒ…ç»ª â†’ æ™ºèƒ½ç”Ÿæˆ â†’ å³æ—¶æ’­æ”¾</div>
            <div style="margin-top: 10px;">
                <span class="therapy-highlight">âœ¨ çœŸæ­£çš„æµç•…è¿‡æ¸¡ + å®Œç¾éŸ³ç”»åŒæ­¥</span>
            </div>
        </div>
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ’­ æƒ…ç»ªè¾“å…¥")
                
                # å¿«é€Ÿæƒ…ç»ªé€‰æ‹©
                emotion_examples = gr.Dropdown(
                    choices=[
                        "ğŸ˜° æˆ‘æ„Ÿåˆ°å¾ˆç„¦è™‘ï¼Œå¿ƒè·³åŠ é€Ÿï¼Œéš¾ä»¥å…¥ç¡",
                        "ğŸ˜´ æˆ‘å¾ˆç–²æƒ«ï¼Œä½†å¤§è„‘è¿˜åœ¨æ´»è·ƒï¼Œæ— æ³•æ”¾æ¾",
                        "ğŸ˜¤ æˆ‘æ„Ÿåˆ°çƒ¦èºä¸å®‰ï¼Œå®¹æ˜“è¢«å°äº‹å½±å“",
                        "ğŸ˜Œ æˆ‘æ¯”è¾ƒå¹³é™ï¼Œä½†å¸Œæœ›æ›´æ·±å±‚çš„æ”¾æ¾",
                        "ğŸ¤¯ æœ€è¿‘å‹åŠ›å¾ˆå¤§ï¼Œæ€»æ˜¯æ„Ÿåˆ°ç´§å¼ "
                    ],
                    label="ğŸ­ å¿«é€Ÿé€‰æ‹©æƒ…ç»ª",
                    value="ğŸ˜° æˆ‘æ„Ÿåˆ°å¾ˆç„¦è™‘ï¼Œå¿ƒè·³åŠ é€Ÿï¼Œéš¾ä»¥å…¥ç¡"
                )
                
                # è¯¦ç»†æƒ…ç»ªæè¿°
                emotion_input = gr.Textbox(
                    label="âœï¸ è¯¦ç»†æè¿°æ‚¨çš„æƒ…ç»ªçŠ¶æ€",
                    placeholder="è¯·è¯¦ç»†æè¿°æ‚¨å½“å‰çš„æƒ…ç»ªæ„Ÿå—...",
                    lines=3,
                    value="æˆ‘æ„Ÿåˆ°å¾ˆç„¦è™‘ï¼Œå¿ƒè·³åŠ é€Ÿï¼Œéš¾ä»¥å…¥ç¡"
                )
                
                # ç–—æ„ˆæ—¶é•¿
                duration_slider = gr.Slider(
                    minimum=5, 
                    maximum=20, 
                    value=12, 
                    step=1,
                    label="â±ï¸ ç–—æ„ˆæ—¶é•¿ï¼ˆç§’ï¼‰",
                    info="æ¨è12-15ç§’è·å¾—æœ€ä½³ä½“éªŒ"
                )
                
                # Suno APIé€‰é¡¹
                with gr.Row():
                    use_suno = gr.Checkbox(
                        label="ğŸµ ä½¿ç”¨Suno AIéŸ³ä¹ç”Ÿæˆ",
                        value=False,
                        info="å¯ç”¨çœŸå®AIéŸ³ä¹ï¼ˆæµ‹è¯•æ¨¡å¼ä¸‹å®‰å…¨ï¼‰"
                    )
                    enable_real_api = gr.Checkbox(
                        label="ğŸ’° å¯ç”¨çœŸå®APIè°ƒç”¨",
                        value=False,
                        info="âš ï¸ éœ€è¦æ¶ˆè€—APIè´¹ç”¨ï¼"
                    )
                
                # ç”ŸæˆæŒ‰é’®
                generate_btn = gr.Button(
                    "ğŸŒŠ å¼€å§‹å¢å¼ºä¸‰é˜¶æ®µç–—æ„ˆ",
                    variant="primary",
                    size="lg"
                )
                
                # ç³»ç»Ÿè¯´æ˜
                gr.HTML("""
                <div style="margin-top: 20px; padding: 15px; background: rgba(255,255,255,0.1); border-radius: 10px;">
                    <strong>ğŸŒŠ å¢å¼ºä¸‰é˜¶æ®µç–—æ„ˆåŸç†ï¼š</strong><br>
                    <div style="margin-top: 10px; text-align: left;">
                        <div><strong>ğŸ¯ åŒæ­¥æœŸ (30%)</strong>: åŒ¹é…æ‚¨çš„æƒ…ç»ªé¢‘ç‡</div>
                        <div><strong>ğŸŒ€ å¼•å¯¼æœŸ (40%)</strong>: æµç•…è¿‡æ¸¡åˆ°æ”¾æ¾çŠ¶æ€</div>
                        <div><strong>ğŸ’¤ å·©å›ºæœŸ (30%)</strong>: æ·±åº¦æ”¾æ¾ï¼Œå‡†å¤‡å…¥ç¡</div>
                    </div>
                    <div style="margin-top: 10px; font-size: 14px; opacity: 0.8;">
                        âœ¨ ç‰¹è‰²ï¼šæ•°å­¦ç²¾ç¡®çš„æ— ç¼è¿‡æ¸¡ + æƒ…ç»ªä¸“å±å‚æ•°
                    </div>
                    <div style="margin-top: 10px; padding: 10px; background: rgba(255,215,0,0.2); border-radius: 5px;">
                        <strong>ğŸµ Suno AIé›†æˆï¼š</strong><br>
                        <div style="font-size: 12px; margin-top: 5px;">
                            â€¢ <strong>æµ‹è¯•æ¨¡å¼</strong>ï¼šå®‰å…¨æ¨¡æ‹Ÿï¼Œæ— è´¹ç”¨<br>
                            â€¢ <strong>çœŸå®æ¨¡å¼</strong>ï¼šæ¶ˆè€—APIè´¹ç”¨ï¼Œéœ€è°¨æ…<br>
                            â€¢ <strong>æˆæœ¬æ§åˆ¶</strong>ï¼šæ¯æ—¥æœ€å¤š3æ¬¡è°ƒç”¨
                        </div>
                    </div>
                </div>
                """)
            
            with gr.Column(scale=2):
                gr.Markdown("### ğŸ¬ ç–—æ„ˆä½“éªŒ")
                
                # è¯¦ç»†ä¿¡æ¯æ˜¾ç¤º
                info_output = gr.Textbox(
                    label="ğŸ“Š ç–—æ„ˆç”ŸæˆæŠ¥å‘Š",
                    lines=25,
                    interactive=False,
                    value="ç­‰å¾…æ‚¨çš„æƒ…ç»ªè¾“å…¥ï¼Œå¼€å§‹ä¸ªæ€§åŒ–ç–—æ„ˆä½“éªŒ..."
                )
                
                # éŸ³é¢‘æ’­æ”¾å™¨
                audio_output = gr.Audio(
                    label="ğŸµ ä¸‰é˜¶æ®µç–—æ„ˆéŸ³é¢‘",
                    type="filepath",
                    interactive=True
                )
                
                # çŠ¶æ€æ˜¾ç¤º
                status_output = gr.Textbox(
                    label="ğŸ”„ å¤„ç†çŠ¶æ€",
                    interactive=False,
                    value="å°±ç»ª"
                )
        
        # ä½¿ç”¨æŒ‡å—
        gr.HTML("""
        <div style="margin-top: 20px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <h3 style="color: #333;">ğŸ¯ å®Œæ•´ä½¿ç”¨æŒ‡å—</h3>
            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin-top: 15px;">
                <div>
                    <h4 style="color: #555;">ğŸµ éŸ³é¢‘ä½“éªŒ</h4>
                    <ul style="color: #666; text-align: left;">
                        <li>ä½©æˆ´è€³æœºè·å¾—æœ€ä½³ç«‹ä½“å£°æ•ˆæœ</li>
                        <li>åœ¨å®‰é™ç¯å¢ƒä¸­è†å¬</li>
                        <li>éŸ³é‡è°ƒè‡³èˆ’é€‚æ°´å¹³</li>
                        <li>ä¸“æ³¨æ„Ÿå—ä¸‰é˜¶æ®µè½¬æ¢</li>
                    </ul>
                </div>
                <div>
                    <h4 style="color: #555;">ğŸ§˜ ç–—æ„ˆè¿‡ç¨‹</h4>
                    <ul style="color: #666; text-align: left;">
                        <li>è·ŸéšéŸ³é¢‘èŠ‚å¥è°ƒæ•´å‘¼å¸</li>
                        <li>è®©éŸ³ä¹å¼•å¯¼æ‚¨çš„æƒ…ç»ª</li>
                        <li>æ„Ÿå—ä»ç´§å¼ åˆ°æ”¾æ¾çš„è¿‡æ¸¡</li>
                        <li>äº«å—æœ€ç»ˆçš„æ·±åº¦å¹³é™</li>
                    </ul>
                </div>
                <div>
                    <h4 style="color: #555;">ğŸŒŸ æŠ€æœ¯ç‰¹è‰²</h4>
                    <ul style="color: #666; text-align: left;">
                        <li>5ç§æƒ…ç»ªçš„ä¸“å±å‚æ•°è®¾è®¡</li>
                        <li>4ç§æ•°å­¦è¿‡æ¸¡å‡½æ•°</li>
                        <li>ç«‹ä½“å£°ç©ºé—´åŒ–å¤„ç†</li>
                        <li>è‡ªç„¶éŸ³æ•ˆèåˆ</li>
                    </ul>
                </div>
            </div>
        </div>
        """)
        
        # äº‹ä»¶ç»‘å®š
        def update_input_from_dropdown(selected):
            if " " in selected:
                return selected.split(" ", 1)[1]
            return selected
        
        emotion_examples.change(
            update_input_from_dropdown,
            inputs=emotion_examples,
            outputs=emotion_input
        )
        
        generate_btn.click(
            process_therapy_request,
            inputs=[emotion_input, duration_slider, use_suno, enable_real_api],
            outputs=[info_output, audio_output, status_output]
        )
    
    return app

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨å¢å¼ºä¸‰é˜¶æ®µç–—æ„ˆç³»ç»Ÿ - å®Œæ•´Webç•Œé¢")
    print("ğŸŒŠ ç«¯åˆ°ç«¯ä½“éªŒï¼šè¾“å…¥æƒ…ç»ª â†’ æ™ºèƒ½ç”Ÿæˆ â†’ å³æ—¶æ’­æ”¾")
    print("âœ¨ ç‰¹è‰²ï¼šæµç•…è¿‡æ¸¡ + å®Œç¾éŸ³ç”»åŒæ­¥")
    print("ğŸ¯ è®¿é—®åœ°å€å³å°†æ˜¾ç¤º...")
    
    app = create_therapy_interface()
    app.launch(
        server_name="0.0.0.0",
        server_port=7869,
        share=True,
        debug=False,
        show_error=True
    )

if __name__ == "__main__":
    main()