#!/usr/bin/env python3
"""
ğŸŒ™ å¢å¼ºä¸‰é˜¶æ®µç–—æ„ˆç³»ç»Ÿ + å¯è§†åŒ–å‘¼å¸å¼•å¯¼
ä¸“æ³¨æµç•…è¿‡æ¸¡å’Œå®Œç¾åŒæ­¥çš„å®Œæ•´ä½“éªŒç‰ˆæœ¬
"""

import numpy as np
import sys
import os
import tempfile
import time
from pathlib import Path
import json
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from matplotlib.patches import Circle
import threading

def generate_enhanced_therapy_audio(duration=20, sample_rate=44100, emotion="ç„¦è™‘"):
    """ç”Ÿæˆå¢å¼ºçš„ä¸‰é˜¶æ®µç–—æ„ˆéŸ³é¢‘ï¼ˆæµç•…è¿‡æ¸¡ç‰ˆæœ¬ï¼‰"""
    print(f"ğŸµ ç”Ÿæˆ{duration}ç§’å¢å¼ºä¸‰é˜¶æ®µç–—æ„ˆéŸ³é¢‘ (é’ˆå¯¹{emotion}æƒ…ç»ª)")
    
    # æ ¹æ®æƒ…ç»ªè°ƒæ•´å‚æ•°
    emotion_params = {
        "ç„¦è™‘": {
            "sync_freq": 440, "guide_freq": 330, "consolidate_freq": 220,
            "sync_intensity": 0.4, "guide_intensity": 0.25, "consolidate_intensity": 0.15,
            "transition_type": "exponential",
            "color": "#FF6B6B"  # çº¢è°ƒ
        },
        "ç–²æƒ«": {
            "sync_freq": 380, "guide_freq": 280, "consolidate_freq": 200,
            "sync_intensity": 0.35, "guide_intensity": 0.2, "consolidate_intensity": 0.1,
            "transition_type": "linear",
            "color": "#FFB366"  # æ©™è°ƒ
        },
        "çƒ¦èº": {
            "sync_freq": 460, "guide_freq": 350, "consolidate_freq": 240,
            "sync_intensity": 0.45, "guide_intensity": 0.3, "consolidate_intensity": 0.18,
            "transition_type": "sigmoid",
            "color": "#FF8E8E"  # çº¢ç´«è°ƒ
        },
        "å¹³é™": {
            "sync_freq": 400, "guide_freq": 320, "consolidate_freq": 210,
            "sync_intensity": 0.3, "guide_intensity": 0.2, "consolidate_intensity": 0.12,
            "transition_type": "smooth",
            "color": "#66D9AB"  # ç»¿è“è°ƒ
        },
        "å‹åŠ›": {
            "sync_freq": 480, "guide_freq": 360, "consolidate_freq": 230,
            "sync_intensity": 0.5, "guide_intensity": 0.32, "consolidate_intensity": 0.2,
            "transition_type": "exponential",
            "color": "#6BB6FF"  # æ·±è“è°ƒ
        }
    }
    
    params = emotion_params.get(emotion, emotion_params["ç„¦è™‘"])
    
    # ä¸‰é˜¶æ®µæ—¶é—´åˆ†é…ï¼ˆæµç•…è¿‡æ¸¡ï¼‰
    stage1_duration = duration * 0.3  # 30% - åŒæ­¥æœŸ
    stage2_duration = duration * 0.4  # 40% - å¼•å¯¼æœŸï¼ˆæœ€é‡è¦ï¼‰
    stage3_duration = duration * 0.3  # 30% - å·©å›ºæœŸ
    
    print(f"ğŸµ ä¸‰é˜¶æ®µæ—¶é•¿åˆ†é…: åŒæ­¥æœŸ{stage1_duration:.1f}s â†’ å¼•å¯¼æœŸ{stage2_duration:.1f}s â†’ å·©å›ºæœŸ{stage3_duration:.1f}s")
    
    # ç”Ÿæˆå®Œæ•´éŸ³é¢‘æ•°ç»„
    total_samples = int(sample_rate * duration)
    audio_array = np.zeros(total_samples)
    
    # æ—¶é—´è½´
    t_total = np.linspace(0, duration, total_samples)
    
    # ç¬¬ä¸€é˜¶æ®µï¼šåŒæ­¥æœŸ - åŒ¹é…ç”¨æˆ·æƒ…ç»ª
    stage1_end = stage1_duration
    stage1_mask = t_total <= stage1_end
    stage1_time = t_total[stage1_mask]
    
    print(f"ğŸµ ç¬¬ä¸€é˜¶æ®µ-åŒæ­¥æœŸ: {params['sync_freq']}Hz, å¼ºåº¦{params['sync_intensity']}")
    stage1_audio = params['sync_intensity'] * np.sin(2 * np.pi * params['sync_freq'] * stage1_time)
    
    # æ·»åŠ æƒ…ç»ªç‰¹å¾
    if emotion == "ç„¦è™‘":
        tremolo = 0.1 * np.sin(2 * np.pi * 5 * stage1_time)  # 5Hzé¢¤éŸ³
        stage1_audio *= (1 + tremolo)
    elif emotion == "ç–²æƒ«":
        stage1_audio *= np.exp(-stage1_time / 8)
    
    audio_array[stage1_mask] = stage1_audio
    
    # ç¬¬äºŒé˜¶æ®µï¼šå¼•å¯¼æœŸ - æµç•…è¿‡æ¸¡
    stage2_start = stage1_duration
    stage2_end = stage2_start + stage2_duration
    stage2_mask = (t_total > stage2_start) & (t_total <= stage2_end)
    stage2_time = t_total[stage2_mask] - stage2_start
    
    print(f"ğŸµ ç¬¬äºŒé˜¶æ®µ-å¼•å¯¼æœŸ: {params['sync_freq']}Hzâ†’{params['guide_freq']}Hz, æµç•…è¿‡æ¸¡")
    
    # é¢‘ç‡å’Œå¼ºåº¦çš„æµç•…è¿‡æ¸¡
    transition_progress = stage2_time / stage2_duration
    
    # é€‰æ‹©è¿‡æ¸¡å‡½æ•°
    if params['transition_type'] == "exponential":
        transition_curve = 1 - np.exp(-3 * transition_progress)
    elif params['transition_type'] == "sigmoid":
        transition_curve = 1 / (1 + np.exp(-6 * (transition_progress - 0.5)))
    elif params['transition_type'] == "linear":
        transition_curve = transition_progress
    else:  # smooth
        transition_curve = 3 * transition_progress**2 - 2 * transition_progress**3
    
    # åŠ¨æ€é¢‘ç‡å˜åŒ–
    current_freq = params['sync_freq'] + (params['guide_freq'] - params['sync_freq']) * transition_curve
    current_intensity = params['sync_intensity'] + (params['guide_intensity'] - params['sync_intensity']) * transition_curve
    
    stage2_audio = current_intensity * np.sin(2 * np.pi * current_freq * stage2_time)
    
    # æ·»åŠ å’Œè°æ³›éŸ³ï¼ˆå¢å¼ºç–—æ„ˆæ•ˆæœï¼‰
    harmonic1 = 0.3 * current_intensity * np.sin(2 * np.pi * current_freq * 2 * stage2_time)
    harmonic2 = 0.2 * current_intensity * np.sin(2 * np.pi * current_freq * 3 * stage2_time)
    stage2_audio += harmonic1 + harmonic2
    
    audio_array[stage2_mask] = stage2_audio
    
    # ç¬¬ä¸‰é˜¶æ®µï¼šå·©å›ºæœŸ - æ·±åº¦æ”¾æ¾
    stage3_start = stage2_end
    stage3_mask = t_total > stage3_start
    stage3_time = t_total[stage3_mask] - stage3_start
    
    print(f"ğŸµ ç¬¬ä¸‰é˜¶æ®µ-å·©å›ºæœŸ: {params['consolidate_freq']}Hz, æ·±åº¦æ”¾æ¾")
    
    # ä»å¼•å¯¼æœŸé¢‘ç‡å¹³æ»‘è¿‡æ¸¡åˆ°å·©å›ºæœŸ
    consolidate_transition = np.exp(-stage3_time / 3)  # æ¸è¿›è¡°å‡
    final_freq = params['guide_freq'] + (params['consolidate_freq'] - params['guide_freq']) * (1 - consolidate_transition)
    final_intensity = params['consolidate_intensity'] * np.exp(-stage3_time / 10)  # æ¸è¿›å‡å¼±
    
    stage3_audio = final_intensity * np.sin(2 * np.pi * final_freq * stage3_time)
    
    # æ·»åŠ è‡ªç„¶ç¯å¢ƒéŸ³ï¼ˆç™½å™ªå£° + æµ·æµªå£°ï¼‰
    nature_sound = 0.05 * np.random.normal(0, 1, len(stage3_time))
    wave_sound = 0.1 * final_intensity * np.sin(2 * np.pi * 0.3 * stage3_time)  # ç¼“æ…¢æµ·æµª
    stage3_audio += nature_sound + wave_sound
    
    audio_array[stage3_mask] = stage3_audio
    
    # æ•´ä½“åå¤„ç†
    print("ğŸµ éŸ³é¢‘åå¤„ç†: ç«‹ä½“å£° + ç©ºé—´åŒ–...")
    
    # åˆ›å»ºç«‹ä½“å£°
    left_channel = audio_array
    right_channel = audio_array.copy()
    
    # æ·»åŠ è½»å¾®çš„ç«‹ä½“å£°æ•ˆæœ
    stereo_delay = int(0.01 * sample_rate)  # 10mså»¶è¿Ÿ
    if len(right_channel) > stereo_delay:
        right_channel[stereo_delay:] = audio_array[:-stereo_delay]
    
    # æ·»åŠ ç©ºé—´æ··å“
    reverb = 0.1 * np.convolve(audio_array, np.exp(-np.linspace(0, 2, int(0.5 * sample_rate))), mode='same')
    left_channel += reverb
    right_channel += reverb * 0.8
    
    # åˆå¹¶ç«‹ä½“å£°
    stereo_audio = np.column_stack([left_channel, right_channel])
    
    # æœ€ç»ˆå½’ä¸€åŒ–
    stereo_audio = stereo_audio / np.max(np.abs(stereo_audio)) * 0.8  # ç•™20%ä½™é‡
    
    # æ·»åŠ æ·¡å…¥æ·¡å‡º
    fade_samples = int(0.5 * sample_rate)  # 0.5ç§’æ·¡å…¥æ·¡å‡º
    fade_in = np.linspace(0, 1, fade_samples)
    fade_out = np.linspace(1, 0, fade_samples)
    
    stereo_audio[:fade_samples] *= fade_in[:, np.newaxis]
    stereo_audio[-fade_samples:] *= fade_out[:, np.newaxis]
    
    return stereo_audio.astype(np.float32), sample_rate, params

def create_breathing_visualization(duration, emotion_params):
    """åˆ›å»ºå®æ—¶å‘¼å¸å¯è§†åŒ–å¼•å¯¼"""
    print("ğŸ¬ åˆ›å»ºå®æ—¶å‘¼å¸å¯è§†åŒ–å¼•å¯¼...")
    
    # è®¾ç½®å›¾å½¢
    fig, ax = plt.subplots(figsize=(8, 8))
    ax.set_xlim(-2, 2)
    ax.set_ylim(-2, 2)
    ax.set_aspect('equal')
    ax.axis('off')
    ax.set_facecolor('black')
    fig.patch.set_facecolor('black')
    
    # åˆ›å»ºå‘¼å¸åœ†åœˆ
    circle = Circle((0, 0), 0.5, fill=False, linewidth=4, color=emotion_params['color'])
    ax.add_patch(circle)
    
    # æ·»åŠ æ–‡æœ¬
    stage_text = ax.text(0, -1.5, "å‡†å¤‡å¼€å§‹...", ha='center', va='center', 
                        fontsize=16, color='white', weight='bold')
    time_text = ax.text(0, 1.5, "00:00", ha='center', va='center', 
                       fontsize=14, color='white')
    breath_text = ax.text(0, 0, "æ·±å‘¼å¸", ha='center', va='center', 
                         fontsize=20, color=emotion_params['color'], weight='bold')
    
    # ä¸‰é˜¶æ®µæ—¶é—´åˆ†é…
    stage1_duration = duration * 0.3
    stage2_duration = duration * 0.4
    stage3_duration = duration * 0.3
    
    def animate(frame):
        current_time = frame * 0.1  # æ¯å¸§0.1ç§’
        
        # ç¡®å®šå½“å‰é˜¶æ®µ
        if current_time <= stage1_duration:
            stage = "åŒæ­¥æœŸ - åŒ¹é…æƒ…ç»ª"
            stage_progress = current_time / stage1_duration
            breath_freq = emotion_params.get('sync_freq', 440) / 100
            base_radius = 0.8
        elif current_time <= stage1_duration + stage2_duration:
            stage = "å¼•å¯¼æœŸ - æƒ…ç»ªè½¬æ¢"
            stage_progress = (current_time - stage1_duration) / stage2_duration
            breath_freq = emotion_params.get('guide_freq', 330) / 100
            base_radius = 0.6
        else:
            stage = "å·©å›ºæœŸ - æ·±åº¦æ”¾æ¾"
            stage_progress = (current_time - stage1_duration - stage2_duration) / stage3_duration
            breath_freq = emotion_params.get('consolidate_freq', 220) / 100
            base_radius = 0.4
        
        # è®¡ç®—å‘¼å¸åŠå¾„
        breath_radius = base_radius + 0.3 * np.sin(2 * np.pi * breath_freq * current_time)
        circle.set_radius(breath_radius)
        
        # æ›´æ–°æ–‡æœ¬
        minutes = int(current_time // 60)
        seconds = int(current_time % 60)
        time_text.set_text(f"{minutes:02d}:{seconds:02d}")
        stage_text.set_text(stage)
        
        # å‘¼å¸æŒ‡å¯¼
        breath_phase = (2 * np.pi * breath_freq * current_time) % (2 * np.pi)
        if breath_phase < np.pi:
            breath_text.set_text("å¸æ°”")
            breath_text.set_color(emotion_params['color'])
        else:
            breath_text.set_text("å‘¼æ°”")
            breath_text.set_color('#FFFFFF')
        
        return circle, stage_text, time_text, breath_text
    
    # åˆ›å»ºåŠ¨ç”»
    frames = int(duration / 0.1)  # æ¯0.1ç§’ä¸€å¸§
    ani = animation.FuncAnimation(fig, animate, frames=frames, interval=100, blit=True, repeat=False)
    
    return fig, ani

def play_audio_with_visualization(audio_array, sample_rate, emotion_params, duration):
    """æ’­æ”¾éŸ³é¢‘å¹¶æ˜¾ç¤ºå¯è§†åŒ–"""
    try:
        import sounddevice as sd
        print("ğŸµ å¼€å§‹æ’­æ”¾éŸ³é¢‘...")
        
        # åˆ›å»ºå¯è§†åŒ–
        fig, ani = create_breathing_visualization(duration, emotion_params)
        
        # åœ¨æ–°çº¿ç¨‹ä¸­æ’­æ”¾éŸ³é¢‘
        def play_audio():
            sd.play(audio_array, sample_rate)
            sd.wait()
        
        audio_thread = threading.Thread(target=play_audio)
        audio_thread.start()
        
        # æ˜¾ç¤ºå¯è§†åŒ–
        plt.show()
        
        # ç­‰å¾…éŸ³é¢‘æ’­æ”¾å®Œæˆ
        audio_thread.join()
        
        return True
    except ImportError:
        print("âš ï¸ sounddeviceæœªå®‰è£…ï¼Œæ— æ³•æ’­æ”¾éŸ³é¢‘")
        
        # ä»…æ˜¾ç¤ºå¯è§†åŒ–
        fig, ani = create_breathing_visualization(duration, emotion_params)
        plt.show()
        
        return False

def save_audio_file(audio_array, sample_rate, output_path):
    """ä¿å­˜éŸ³é¢‘æ–‡ä»¶"""
    try:
        # å°è¯•ä½¿ç”¨scipyä¿å­˜
        from scipy.io import wavfile
        # è½¬æ¢ä¸º16ä½æ•´æ•°
        audio_int = (audio_array * 32767).astype(np.int16)
        wavfile.write(output_path, sample_rate, audio_int)
        return True
    except ImportError:
        # å¦‚æœscipyä¸å¯ç”¨ï¼Œä¿å­˜ä¸ºnumpyæ•°ç»„
        np.save(output_path.replace('.wav', '.npy'), audio_array)
        print(f"âš ï¸ å·²ä¿å­˜ä¸ºnumpyæ•°ç»„æ ¼å¼: {output_path.replace('.wav', '.npy')}")
        return False

def simple_emotion_detection(user_input):
    """ç®€åŒ–çš„æƒ…ç»ªæ£€æµ‹"""
    emotions = {
        "ç„¦è™‘": ["ç„¦è™‘", "ç´§å¼ ", "æ‹…å¿ƒ", "ä¸å®‰", "å®³æ€•"],
        "ç–²æƒ«": ["ç–²æƒ«", "ç´¯", "ç–²åŠ³", "å›°å€¦", "ä¹åŠ›"],
        "çƒ¦èº": ["çƒ¦èº", "çƒ¦æ¼", "æ˜“æ€’", "æ€¥èº", "ä¸è€çƒ¦"],
        "å¹³é™": ["å¹³é™", "æ”¾æ¾", "å®‰é™", "å®é™", "èˆ’ç¼“"],
        "å‹åŠ›": ["å‹åŠ›", "ç´§è¿«", "è´Ÿæ‹…", "é‡å‹", "æ²‰é‡"]
    }
    
    detected_emotion = "ç„¦è™‘"  # é»˜è®¤
    max_score = 0
    
    for emotion, keywords in emotions.items():
        score = sum(1 for keyword in keywords if keyword in user_input)
        if score > max_score:
            max_score = score
            detected_emotion = emotion
    
    confidence = min(0.85 + max_score * 0.05, 0.95)
    return detected_emotion, confidence

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨å¢å¼ºä¸‰é˜¶æ®µç–—æ„ˆç³»ç»Ÿ + å¯è§†åŒ–å‘¼å¸å¼•å¯¼...")
    print("ğŸŒŠ ç‰¹è‰²ï¼šæµç•…è¿‡æ¸¡ + å®Œç¾åŒæ­¥ + å®æ—¶å¯è§†åŒ–")
    print("ğŸ¯ ä¸‰é˜¶æ®µï¼šåŒæ­¥æœŸ(30%) â†’ å¼•å¯¼æœŸ(40%) â†’ å·©å›ºæœŸ(30%)")
    print("âœ¨ è¿è´¯ç–—æ„ˆå™äº‹ + åŠ¨æ€å‘¼å¸å¼•å¯¼")
    print("=" * 60)
    
    # ç”¨æˆ·è¾“å…¥
    print("\nğŸ’­ è¯·æè¿°æ‚¨çš„æƒ…ç»ªçŠ¶æ€ï¼š")
    user_input = input("ğŸ‘‰ ")
    
    if not user_input or len(user_input.strip()) < 3:
        user_input = "æˆ‘æ„Ÿåˆ°å¾ˆç„¦è™‘ï¼Œå¿ƒè·³åŠ é€Ÿï¼Œéš¾ä»¥å…¥ç¡"
        print(f"ä½¿ç”¨é»˜è®¤æƒ…ç»ªï¼š{user_input}")
    
    # æ—¶é•¿è®¾ç½®
    try:
        duration_input = input("\nâ±ï¸ ç–—æ„ˆæ—¶é•¿ï¼ˆç§’ï¼Œé»˜è®¤20ç§’ï¼‰: ")
        duration = int(duration_input) if duration_input.strip() else 20
        duration = max(10, min(duration, 60))  # é™åˆ¶åœ¨10-60ç§’
    except:
        duration = 20
    
    # é€‰æ‹©æ¨¡å¼
    print("\nğŸ¬ é€‰æ‹©ä½“éªŒæ¨¡å¼ï¼š")
    print("1. å®Œæ•´ä½“éªŒï¼ˆéŸ³é¢‘ + å¯è§†åŒ–å‘¼å¸å¼•å¯¼ï¼‰")
    print("2. ä»…ç”ŸæˆéŸ³é¢‘æ–‡ä»¶")
    
    try:
        mode_choice = input("ğŸ‘‰ é€‰æ‹©æ¨¡å¼ (1/2ï¼Œé»˜è®¤1): ").strip()
        full_experience = mode_choice != "2"
    except:
        full_experience = True
    
    print(f"\nğŸ§  å¼€å§‹æƒ…ç»ªåˆ†æ...")
    start_time = time.time()
    
    # æƒ…ç»ªè¯†åˆ«
    detected_emotion, confidence = simple_emotion_detection(user_input)
    print(f"ğŸ¯ æ£€æµ‹åˆ°æƒ…ç»ª: {detected_emotion} (ç½®ä¿¡åº¦: {confidence:.1%})")
    
    # ç”Ÿæˆå¢å¼ºéŸ³é¢‘
    print(f"\nğŸµ ç”Ÿæˆå¢å¼ºä¸‰é˜¶æ®µç–—æ„ˆéŸ³é¢‘...")
    audio_array, sample_rate, params = generate_enhanced_therapy_audio(
        duration=duration, 
        emotion=detected_emotion
    )
    
    # ä¿å­˜éŸ³é¢‘
    output_dir = Path("outputs")
    output_dir.mkdir(exist_ok=True)
    
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    audio_path = output_dir / f"enhanced_therapy_{detected_emotion}_{timestamp}.wav"
    
    print(f"\nğŸ’¾ ä¿å­˜éŸ³é¢‘æ–‡ä»¶...")
    success = save_audio_file(audio_array, sample_rate, str(audio_path))
    
    processing_time = time.time() - start_time
    
    # è¾“å‡ºè¯¦ç»†ä¿¡æ¯
    print("\n" + "=" * 60)
    print("âœ… å¢å¼ºä¸‰é˜¶æ®µç–—æ„ˆéŸ³é¢‘ç”Ÿæˆå®Œæˆï¼")
    print(f"""
ğŸ§  æƒ…ç»ªè¯†åˆ«ç»“æœ:
   æƒ…ç»ªç±»å‹: {detected_emotion}
   ç½®ä¿¡åº¦: {confidence:.1%}
   è§†è§‰ä¸»é¢˜: {params['color']}
   å¤„ç†æ—¶é—´: {processing_time:.1f}ç§’

ğŸµ éŸ³é¢‘æŠ€æœ¯è¯¦æƒ…:
   æ€»æ—¶é•¿: {duration}ç§’
   é‡‡æ ·ç‡: {sample_rate}Hz (CDçº§åˆ«)
   å£°é“: ç«‹ä½“å£° + ç©ºé—´æ··å“
   é’ˆå¯¹æƒ…ç»ª: {detected_emotion}

ğŸŒŠ ä¸‰é˜¶æ®µæµç•…è¿‡æ¸¡è®¾è®¡:
   åŒæ­¥æœŸ ({duration*0.3:.1f}s): {params['sync_freq']}Hz, åŒ¹é…{detected_emotion}æƒ…ç»ª
   å¼•å¯¼æœŸ ({duration*0.4:.1f}s): {params['sync_freq']}â†’{params['guide_freq']}Hz, æµç•…è¿‡æ¸¡
   å·©å›ºæœŸ ({duration*0.3:.1f}s): {params['consolidate_freq']}Hz, æ·±åº¦æ”¾æ¾

ğŸ“ è¾“å‡ºæ–‡ä»¶:
   éŸ³é¢‘æ–‡ä»¶: {audio_path}
   æ–‡ä»¶æ ¼å¼: {'WAV (æ ‡å‡†)' if success else 'NumPyæ•°ç»„'}
""")
    
    if full_experience:
        print("\nğŸ¬ å¼€å§‹å®Œæ•´ç–—æ„ˆä½“éªŒ...")
        print("ğŸ“‹ ä½“éªŒè¯´æ˜ï¼š")
        print("   - è·Ÿéšå‘¼å¸åœ†åœˆçš„èŠ‚å¥è°ƒæ•´å‘¼å¸")
        print("   - åœ†åœˆæ‰©å¤§æ—¶å¸æ°”ï¼Œç¼©å°æ—¶å‘¼æ°”")
        print("   - æ³¨æ„è§‚å¯Ÿä¸‰é˜¶æ®µçš„è½¬æ¢è¿‡ç¨‹")
        print("   - è®©éŸ³ä¹å’Œè§†è§‰å¼•å¯¼æ‚¨è¿›å…¥æ·±åº¦æ”¾æ¾çŠ¶æ€")
        print("\næŒ‰Enteré”®å¼€å§‹ä½“éªŒ...")
        input()
        
        # æ’­æ”¾éŸ³é¢‘å¹¶æ˜¾ç¤ºå¯è§†åŒ–
        play_success = play_audio_with_visualization(audio_array, sample_rate, params, duration)
        
        if play_success:
            print("\nâœ… å®Œæ•´ç–—æ„ˆä½“éªŒå®Œæˆï¼")
        else:
            print("\nâœ… å¯è§†åŒ–å¼•å¯¼å®Œæˆï¼ï¼ˆå¯æ‰‹åŠ¨æ’­æ”¾éŸ³é¢‘æ–‡ä»¶ï¼‰")
    
    print(f"""
ğŸ§ ç–—æ„ˆä½¿ç”¨å»ºè®®:
   - ä½©æˆ´è€³æœºä½“éªŒç«‹ä½“å£°åœº
   - åœ¨å®‰é™ç¯å¢ƒä¸­è†å¬
   - è·ŸéšéŸ³é¢‘èŠ‚å¥è°ƒæ•´å‘¼å¸
   - ä¸“æ³¨æ„Ÿå—ä¸‰é˜¶æ®µæƒ…ç»ªè½¬æ¢

ğŸ“ æŠ€æœ¯åˆ›æ–°:
   - æµç•…è¿‡æ¸¡: æ— æ˜æ˜¾åœé¡¿çš„ä¸‰é˜¶æ®µåˆ‡æ¢
   - å®Œç¾åŒæ­¥: è§†è§‰æ•ˆæœä¸éŸ³é¢‘å®Œç¾åŒ¹é…
   - æƒ…ç»ªæ˜ å°„: {detected_emotion}æƒ…ç»ªä¸“å±å‚æ•°
   - ç–—æ„ˆå™äº‹: è¿è´¯çš„æƒ…ç»ªè½¬æ¢æ•…äº‹
   - ä¸ªæ€§åŒ–: é’ˆå¯¹ä¸åŒæƒ…ç»ªçš„ç‹¬ç‰¹è®¾è®¡
   - å¯è§†åŒ–: å®æ—¶å‘¼å¸å¼•å¯¼å’Œé˜¶æ®µæ˜¾ç¤º
""")
    
    print("\nğŸŒ™ æ„¿æ‚¨è·å¾—å†…å¿ƒçš„å¹³é™ä¸å®‰å®...")

if __name__ == "__main__":
    main()