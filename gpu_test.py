#!/usr/bin/env python3
"""
GPUåŠ é€Ÿæµ‹è¯•è„šæœ¬
æ£€æµ‹GPUå¯ç”¨æ€§å¹¶ä¼˜åŒ–ç³»ç»Ÿé…ç½®ä»¥å……åˆ†åˆ©ç”¨GPUç®—åŠ›
"""

import torch
import sys
import time
import numpy as np
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

def test_gpu_availability():
    """æµ‹è¯•GPUå¯ç”¨æ€§"""
    print("ğŸš€ GPUå¯ç”¨æ€§æµ‹è¯•")
    print("=" * 50)
    
    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    cuda_available = torch.cuda.is_available()
    print(f"CUDAå¯ç”¨: {cuda_available}")
    
    if cuda_available:
        # GPUä¿¡æ¯
        gpu_count = torch.cuda.device_count()
        print(f"GPUæ•°é‡: {gpu_count}")
        
        for i in range(gpu_count):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
        
        # è®¾ç½®é»˜è®¤GPU
        if gpu_count > 0:
            torch.cuda.set_device(0)
            print(f"é»˜è®¤GPU: {torch.cuda.current_device()}")
    
    return cuda_available

def test_tensor_operations():
    """æµ‹è¯•GPUå¼ é‡æ“ä½œæ€§èƒ½"""
    print("\nğŸ§® GPUå¼ é‡æ“ä½œæ€§èƒ½æµ‹è¯•")
    print("=" * 50)
    
    # æµ‹è¯•æ•°æ®
    size = 1000
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºå¼ é‡
    start_time = time.time()
    a = torch.randn(size, size, device=device)
    b = torch.randn(size, size, device=device)
    creation_time = time.time() - start_time
    print(f"å¼ é‡åˆ›å»ºæ—¶é—´: {creation_time*1000:.2f}ms")
    
    # çŸ©é˜µä¹˜æ³•
    start_time = time.time()
    c = torch.matmul(a, b)
    matmul_time = time.time() - start_time
    print(f"çŸ©é˜µä¹˜æ³•æ—¶é—´: {matmul_time*1000:.2f}ms")
    
    # ç¥ç»ç½‘ç»œå±‚
    start_time = time.time()
    linear = torch.nn.Linear(size, size).to(device)
    output = linear(a)
    nn_time = time.time() - start_time
    print(f"ç¥ç»ç½‘ç»œå±‚æ—¶é—´: {nn_time*1000:.2f}ms")
    
    # å†…å­˜ä½¿ç”¨
    if device == 'cuda':
        memory_allocated = torch.cuda.memory_allocated() / 1024**2
        memory_cached = torch.cuda.memory_reserved() / 1024**2
        print(f"GPUå†…å­˜ä½¿ç”¨: {memory_allocated:.1f}MB / {memory_cached:.1f}MB")
    
    return {
        'device': device,
        'creation_time_ms': creation_time * 1000,
        'matmul_time_ms': matmul_time * 1000,
        'nn_time_ms': nn_time * 1000
    }

def test_emotion_classifier_performance():
    """æµ‹è¯•27ç»´æƒ…ç»ªåˆ†ç±»å™¨çš„GPUæ€§èƒ½"""
    print("\nğŸ­ 27ç»´æƒ…ç»ªåˆ†ç±»å™¨æ€§èƒ½æµ‹è¯•")
    print("=" * 50)
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # æ¨¡æ‹Ÿæƒ…ç»ªåˆ†ç±»å™¨
    class EmotionClassifier(torch.nn.Module):
        def __init__(self):
            super().__init__()
            self.features = torch.nn.Sequential(
                torch.nn.Linear(768, 512),
                torch.nn.ReLU(),
                torch.nn.Dropout(0.2),
                torch.nn.Linear(512, 256),
                torch.nn.ReLU(),
                torch.nn.Dropout(0.2),
                torch.nn.Linear(256, 128),
                torch.nn.ReLU()
            )
            self.emotion_head = torch.nn.Linear(128, 27)
            self.confidence_head = torch.nn.Linear(128, 1)
            self.intensity_head = torch.nn.Linear(128, 27)
        
        def forward(self, x):
            features = self.features(x)
            emotions = torch.softmax(self.emotion_head(features), dim=-1)
            confidence = torch.sigmoid(self.confidence_head(features))
            intensity = torch.sigmoid(self.intensity_head(features))
            return emotions, confidence, intensity
    
    # åˆ›å»ºæ¨¡å‹
    model = EmotionClassifier().to(device)
    model.eval()
    
    # æµ‹è¯•æ•°æ®
    batch_sizes = [1, 4, 8, 16]
    
    for batch_size in batch_sizes:
        # è¾“å…¥æ•°æ® (batch_size, 768) - æ¨¡æ‹ŸBERTç‰¹å¾
        input_data = torch.randn(batch_size, 768, device=device)
        
        # é¢„çƒ­
        with torch.no_grad():
            for _ in range(10):
                _ = model(input_data)
        
        # æ€§èƒ½æµ‹è¯•
        start_time = time.time()
        with torch.no_grad():
            for _ in range(100):
                emotions, confidence, intensity = model(input_data)
        end_time = time.time()
        
        avg_time = (end_time - start_time) / 100 * 1000  # ms
        throughput = batch_size * 100 / (end_time - start_time)  # samples/sec
        
        print(f"æ‰¹å¤§å° {batch_size:2d}: {avg_time:.2f}ms/batch, {throughput:.1f} samples/sec")

def optimize_gpu_settings():
    """ä¼˜åŒ–GPUè®¾ç½®"""
    print("\nâš™ï¸ GPUä¼˜åŒ–è®¾ç½®")
    print("=" * 50)
    
    if torch.cuda.is_available():
        # å¯ç”¨CUDNNåŸºå‡†æµ‹è¯•
        torch.backends.cudnn.benchmark = True
        print("âœ… å¯ç”¨CUDNNåŸºå‡†æµ‹è¯•")
        
        # è®¾ç½®å†…å­˜åˆ†é…ç­–ç•¥
        torch.cuda.empty_cache()
        print("âœ… æ¸…ç©ºGPUç¼“å­˜")
        
        # æ˜¾ç¤ºä¼˜åŒ–åçš„GPUçŠ¶æ€
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            print(f"GPU {i}: {props.name}")
            print(f"  è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
            print(f"  å¤šå¤„ç†å™¨: {props.multi_processor_count}")
            print(f"  æœ€å¤§çº¿ç¨‹/å—: {props.max_threads_per_block}")
            print(f"  æœ€å¤§å—ç»´åº¦: {props.max_block_dims}")
            print(f"  æœ€å¤§ç½‘æ ¼ç»´åº¦: {props.max_grid_dims}")
    else:
        print("âŒ æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPU")
        print("å»ºè®®:")
        print("- æ£€æŸ¥CUDAå®‰è£…")
        print("- æ£€æŸ¥PyTorch GPUç‰ˆæœ¬")
        print("- æ£€æŸ¥GPUé©±åŠ¨")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ qm_final3 GPUåŠ é€Ÿæµ‹è¯•")
    print("æ—¶é—´:", time.strftime("%Y-%m-%d %H:%M:%S"))
    print("=" * 60)
    
    # 1. æµ‹è¯•GPUå¯ç”¨æ€§
    gpu_available = test_gpu_availability()
    
    # 2. æµ‹è¯•å¼ é‡æ“ä½œæ€§èƒ½
    tensor_performance = test_tensor_operations()
    
    # 3. æµ‹è¯•æƒ…ç»ªåˆ†ç±»å™¨æ€§èƒ½
    test_emotion_classifier_performance()
    
    # 4. ä¼˜åŒ–GPUè®¾ç½®
    optimize_gpu_settings()
    
    # 5. æ€§èƒ½å»ºè®®
    print("\nğŸ’¡ æ€§èƒ½å»ºè®®")
    print("=" * 50)
    
    if gpu_available:
        print("âœ… GPUå¯ç”¨ï¼Œå»ºè®®:")
        print("- ä½¿ç”¨æ‰¹å¤„ç† (batch_size >= 4)")
        print("- å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (fp16)")
        print("- ä½¿ç”¨GPUåŠ é€Ÿçš„è§†é¢‘å¤„ç†")
        print("- ä¼˜åŒ–æ•°æ®ä¼ è¾“ (pin_memory=True)")
        
        if tensor_performance['matmul_time_ms'] < 10:
            print("- GPUæ€§èƒ½ä¼˜ç§€ï¼Œå¯ä»¥å¤„ç†å®æ—¶ä»»åŠ¡")
        elif tensor_performance['matmul_time_ms'] < 50:
            print("- GPUæ€§èƒ½è‰¯å¥½ï¼Œé€‚åˆæ‰¹å¤„ç†ä»»åŠ¡")
        else:
            print("- GPUæ€§èƒ½ä¸€èˆ¬ï¼Œè€ƒè™‘é™ä½æ¨¡å‹å¤æ‚åº¦")
    else:
        print("âŒ GPUä¸å¯ç”¨ï¼Œå»ºè®®:")
        print("- æ£€æŸ¥CUDAç¯å¢ƒ")
        print("- ä½¿ç”¨CPUä¼˜åŒ–ç‰ˆæœ¬")
        print("- é™ä½æ‰¹å¤§å°å’Œæ¨¡å‹å¤æ‚åº¦")
        print("- å¯ç”¨å¤šçº¿ç¨‹å¤„ç†")
    
    print("\nğŸ‰ æµ‹è¯•å®Œæˆ!")

if __name__ == "__main__":
    main()