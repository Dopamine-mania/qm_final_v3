#!/usr/bin/env python3
"""
æµ‹è¯•æ ‡å‡†åŒ–èåˆå±‚æ¥å£
éªŒè¯ç”¨æˆ·è§„èŒƒè¦æ±‚çš„äº”ä¸ªæ ¸å¿ƒå‡½æ•°
"""

import sys
import os
import numpy as np
import torch
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from layers.fusion_layer import FusionLayer, FusionLayerConfig
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def test_standard_fusion_interface():
    """æµ‹è¯•æ ‡å‡†åŒ–èåˆæ¥å£"""
    
    print("ğŸ§  æ ‡å‡†åŒ–èåˆå±‚æ¥å£æµ‹è¯•")
    print("=" * 60)
    
    # åˆå§‹åŒ–èåˆå±‚
    config = FusionLayerConfig(layer_name="test_fusion_layer")
    fusion_layer = FusionLayer(config)
    
    # æµ‹è¯•åœºæ™¯ï¼šæ¨¡æ‹Ÿä¸åŒæ¨¡æ€çš„è¾“å…¥æ•°æ®
    test_scenarios = [
        {
            "name": "å®Œæ•´å¤šæ¨¡æ€æ•°æ®",
            "face_data": {
                'au_01': 0.2,  # å†…çœ‰ä¸Šæ‰¬
                'au_04': 0.8,  # çœ‰æ¯›ä¸‹å‹ - æ„¤æ€’
                'au_06': 0.1,  # é¢é¢Šæå‡
                'au_12': 0.0,  # å˜´è§’ä¸Šæ‰¬
                'au_15': 0.7,  # å˜´è§’ä¸‹æ‹‰ - æ‚²ä¼¤
                'au_17': 0.5,  # ä¸‹å·´ä¸ŠæŠ¬
                'au_43': 0.3   # é—­çœ¼ - ç–²åŠ³
            },
            "audio_data": {
                'pitch_mean': 180.0,
                'pitch_std': 25.0,
                'loudness_mean': 0.3,
                'loudness_std': 0.15,
                'speech_rate': 3.5,
                'pause_ratio': 0.3,
                'jitter': 0.02,
                'shimmer': 0.08,
                'harmonics_noise_ratio': 15.0
            },
            "text_input": "æˆ‘ä»Šå¤©æ„Ÿåˆ°å¾ˆç„¦è™‘ï¼Œèººåœ¨åºŠä¸Šç¡ä¸ç€ï¼Œå¿ƒæƒ…ç‰¹åˆ«æ²®ä¸§"
        },
        {
            "name": "é¢éƒ¨å…³é”®ç‚¹æ•°æ®",
            "face_data": [
                # æ¨¡æ‹Ÿ68ä¸ªå…³é”®ç‚¹çš„x,yåæ ‡
                *[np.random.uniform(50, 590) for _ in range(136)]  # 68ç‚¹ * 2åæ ‡
            ],
            "audio_data": None,
            "text_input": "æ„Ÿè§‰å¾ˆç–²æƒ«ä½†æ˜¯å¤§è„‘è¿˜åœ¨æ´»è·ƒï¼Œéš¾ä»¥å…¥ç¡"
        },
        {
            "name": "ä»…æ–‡æœ¬è¾“å…¥",
            "face_data": None,
            "audio_data": None,
            "text_input": "å¿ƒæƒ…å¹³é™ï¼Œå‡†å¤‡è¿›å…¥ç¡çœ çŠ¶æ€ï¼Œæ„Ÿè§‰å¾ˆæ”¾æ¾"
        },
        {
            "name": "éŸ³é¢‘ç‰¹å¾åˆ—è¡¨",
            "face_data": None,
            "audio_data": [0.2, 0.8, 0.1, 0.9, 0.3, 0.7, 0.4, 0.6, 0.5, 0.8, 0.2, 0.1, 0.9, 0.3, 0.7],
            "text_input": None
        }
    ]
    
    print(f"\nğŸ“Š æµ‹è¯• {len(test_scenarios)} ç§è¾“å…¥åœºæ™¯...\n")
    
    for i, scenario in enumerate(test_scenarios, 1):
        print(f"ğŸ”„ === åœºæ™¯ {i}: {scenario['name']} ===")
        
        # æµ‹è¯•1: é¢éƒ¨æƒ…ç»ªç‰¹å¾æå–
        print(f"\nğŸ˜Š 1. é¢éƒ¨æƒ…ç»ªç‰¹å¾æå–:")
        face_features = fusion_layer.extract_face_emotion_features(scenario['face_data'])
        print(f"   è¾“å…¥ç±»å‹: {type(scenario['face_data']).__name__ if scenario['face_data'] else 'None'}")
        print(f"   è¾“å‡ºç»´åº¦: {len(face_features)}ç»´")
        print(f"   ç‰¹å¾èŒƒå›´: [{min(face_features):.3f}, {max(face_features):.3f}]")
        print(f"   ç‰¹å¾å‡å€¼: {np.mean(face_features):.3f}")
        
        # æ˜¾ç¤ºå…³é”®ç‰¹å¾
        if scenario['face_data'] and isinstance(scenario['face_data'], dict):
            print(f"   ä¸»è¦AUç‰¹å¾:")
            au_keys = ['au_04', 'au_06', 'au_12', 'au_15', 'au_43']  # é‡è¦çš„AU
            for j, au_key in enumerate(au_keys):
                if j < len(face_features) and au_key in scenario['face_data']:
                    original_value = scenario['face_data'][au_key]
                    feature_value = face_features[j] if j < len(face_features) else 0.0
                    print(f"     {au_key}: {original_value:.2f} â†’ {feature_value:.3f}")
        
        # æµ‹è¯•2: éŸ³é¢‘æƒ…ç»ªç‰¹å¾æå–
        print(f"\nğŸµ 2. éŸ³é¢‘æƒ…ç»ªç‰¹å¾æå–:")
        audio_features = fusion_layer.extract_audio_emotion_features(scenario['audio_data'])
        print(f"   è¾“å…¥ç±»å‹: {type(scenario['audio_data']).__name__ if scenario['audio_data'] else 'None'}")
        print(f"   è¾“å‡ºç»´åº¦: {len(audio_features)}ç»´")
        print(f"   ç‰¹å¾èŒƒå›´: [{min(audio_features):.3f}, {max(audio_features):.3f}]")
        print(f"   ç‰¹å¾å‡å€¼: {np.mean(audio_features):.3f}")
        
        # æ˜¾ç¤ºéŸ³é¢‘ç‰¹å¾è§£é‡Š
        if scenario['audio_data'] and isinstance(scenario['audio_data'], dict):
            print(f"   éŸ³é¢‘æƒ…ç»ªç‰¹å¾:")
            feature_names = ['éŸ³è°ƒå‡å€¼', 'éŸ³è°ƒå˜åŒ–', 'å“åº¦å‡å€¼', 'å“åº¦å˜åŒ–', 'è¯­é€Ÿ', 
                           'åœé¡¿æ¯”ä¾‹', 'åŸºé¢‘æŠ–åŠ¨', 'æŒ¯å¹…æŠ–åŠ¨', 'è°å™ªæ¯”', 'ç´§å¼ åº¦', 
                           'æ´»åŠ›åº¦', 'å¹³é™åº¦', 'ç–²åŠ³åº¦', 'ç„¦è™‘åº¦']
            for j, name in enumerate(feature_names):
                if j < len(audio_features):
                    value = audio_features[j]
                    percentage = value * 100
                    print(f"     {name}: {percentage:.1f}%")
        
        # æµ‹è¯•3: æ–‡æœ¬æƒ…ç»ªç‰¹å¾æå–
        print(f"\nğŸ“ 3. æ–‡æœ¬æƒ…ç»ªç‰¹å¾æå–:")
        text_features = fusion_layer.extract_text_emotion_features(scenario['text_input'])
        text_display = f'"{scenario["text_input"]}"' if scenario['text_input'] else 'None'
        print(f"   è¾“å…¥æ–‡æœ¬: {text_display}")
        print(f"   è¾“å‡ºç»´åº¦: {len(text_features)}ç»´")
        print(f"   ç‰¹å¾èŒƒå›´: [{min(text_features):.3f}, {max(text_features):.3f}]")
        print(f"   ç‰¹å¾å‡å€¼: {np.mean(text_features):.3f}")
        
        # æ˜¾ç¤ºæ–‡æœ¬æƒ…ç»ªæ£€æµ‹ç»“æœ
        if scenario['text_input']:
            emotion_names = ['æ–‡æœ¬é•¿åº¦', 'è¯æ•°', 'å¥æ•°', 'å¿«ä¹', 'æ‚²ä¼¤', 'æ„¤æ€’', 'ææƒ§', 
                           'æƒŠè®¶', 'åŒæ¶', 'ç„¦è™‘', 'ç–²åŠ³', 'å¹³é™']
            print(f"   æ–‡æœ¬æƒ…ç»ªæ£€æµ‹:")
            for j, name in enumerate(emotion_names):
                if j < len(text_features):
                    value = text_features[j]
                    if name in ['æ–‡æœ¬é•¿åº¦', 'è¯æ•°', 'å¥æ•°']:
                        print(f"     {name}: {value:.2f}")
                    else:
                        percentage = value * 100
                        print(f"     {name}: {percentage:.1f}%")
        
        # æµ‹è¯•4: å¤šæ¨¡æ€ç‰¹å¾èåˆ
        print(f"\nğŸ”— 4. å¤šæ¨¡æ€ç‰¹å¾èåˆ:")
        fused_features = fusion_layer.fuse_multimodal_features(face_features, audio_features, text_features)
        print(f"   è¾“å…¥: é¢éƒ¨{len(face_features)}ç»´ + éŸ³é¢‘{len(audio_features)}ç»´ + æ–‡æœ¬{len(text_features)}ç»´")
        print(f"   è¾“å‡ºç»´åº¦: {len(fused_features)}ç»´")
        print(f"   ç‰¹å¾èŒƒå›´: [{min(fused_features):.3f}, {max(fused_features):.3f}]")
        print(f"   ç‰¹å¾å‡å€¼: {np.mean(fused_features):.3f}")
        
        # æ˜¾ç¤ºèåˆç‰¹å¾ç»“æ„
        print(f"   èåˆç‰¹å¾ç»“æ„:")
        print(f"     â€¢ é¢éƒ¨ç‰¹å¾: 0-19 ({np.mean(fused_features[0:20]):.3f})")
        print(f"     â€¢ éŸ³é¢‘ç‰¹å¾: 20-34 ({np.mean(fused_features[20:35]):.3f})")
        print(f"     â€¢ æ–‡æœ¬ç‰¹å¾: 35-59 ({np.mean(fused_features[35:60]):.3f})")
        print(f"     â€¢ äº¤äº’ç‰¹å¾: 60-63 ({np.mean(fused_features[60:64]):.3f})")
        
        # æµ‹è¯•5: ä¸»å‡½æ•° - æƒ…æ„ŸçŠ¶æ€æ¨æ–­
        print(f"\nğŸ¯ 5. æƒ…æ„ŸçŠ¶æ€æ¨æ–­ (ä¸»å‡½æ•°):")
        emotion_vector = fusion_layer.infer_affective_state(
            raw_face_data=scenario['face_data'],
            raw_audio_data=scenario['audio_data'],
            text_input=scenario['text_input']
        )
        print(f"   è¾“å‡ºç»´åº¦: {len(emotion_vector)}ç»´æƒ…ç»ªå‘é‡")
        print(f"   å‘é‡å’Œ: {sum(emotion_vector):.6f} (åº”æ¥è¿‘1.0)")
        print(f"   å‘é‡èŒƒå›´: [{min(emotion_vector):.3f}, {max(emotion_vector):.3f}]")
        
        # æ‰¾å‡ºä¸»å¯¼æƒ…ç»ª
        max_emotion_idx = np.argmax(emotion_vector)
        max_emotion_prob = emotion_vector[max_emotion_idx]
        
        # æƒ…ç»ªåç§°æ˜ å°„ (ç®€åŒ–ç‰ˆ27ç»´æƒ…ç»ªç©ºé—´)
        emotion_names = [
            'anger', 'fear_anxiety', 'disgust', 'sadness', 'amusement', 'joy', 
            'inspiration', 'tenderness', 'neutral', 'rumination', 'sleep_anxiety',
            'physical_fatigue', 'mental_fatigue', 'hyperarousal', 'bedtime_worry', 
            'sleep_dread', 'restless_sleep', 'sleep_guilt', 'dawn_anxiety', 'peaceful',
            'relaxed', 'drowsy', 'tired_content', 'pre_sleep_calm', 'deep_relaxation',
            'sleep_readiness', 'meditative'
        ]
        
        main_emotion = emotion_names[max_emotion_idx] if max_emotion_idx < len(emotion_names) else f"emotion_{max_emotion_idx}"
        print(f"   ä¸»å¯¼æƒ…ç»ª: {main_emotion} ({max_emotion_prob:.3f})")
        
        # æ˜¾ç¤ºå‰5ä¸ªæœ€é«˜æ¦‚ç‡çš„æƒ…ç»ª
        emotion_probs = [(emotion_names[i] if i < len(emotion_names) else f"emotion_{i}", prob) 
                        for i, prob in enumerate(emotion_vector)]
        emotion_probs.sort(key=lambda x: x[1], reverse=True)
        print(f"   æƒ…ç»ªæ’åº (å‰5):")
        for j, (emotion, prob) in enumerate(emotion_probs[:5]):
            percentage = prob * 100
            print(f"     {j+1}. {emotion}: {percentage:.1f}%")
        
        print(f"\n" + "â”€" * 50)
    
    print(f"\nğŸ‰ æ ‡å‡†åŒ–èåˆå±‚æ¥å£æµ‹è¯•å®Œæˆï¼")
    print(f"\nğŸ“‹ æµ‹è¯•æ€»ç»“:")
    print(f"   âœ… extract_face_emotion_features() - é¢éƒ¨AU/å…³é”®ç‚¹ç‰¹å¾æå–æ­£å¸¸")
    print(f"   âœ… extract_audio_emotion_features() - éŸ³é¢‘éŸµå¾‹ç‰¹å¾æå–æ­£å¸¸")
    print(f"   âœ… extract_text_emotion_features() - æ–‡æœ¬æƒ…ç»ªå…³é”®è¯æ£€æµ‹æ­£å¸¸")
    print(f"   âœ… fuse_multimodal_features() - ç‰¹å¾çº§èåˆæ­£å¸¸")
    print(f"   âœ… infer_affective_state() - ä¸»å‡½æ•°æƒ…æ„Ÿæ¨æ–­æ­£å¸¸")
    print(f"\nğŸ’¡ ç‰¹ç‚¹:")
    print(f"   â€¢ æ”¯æŒå¤šç§è¾“å…¥æ ¼å¼ (AUå­—å…¸ã€å…³é”®ç‚¹åæ ‡ã€éŸ³é¢‘ç‰¹å¾ã€æ–‡æœ¬)")
    print(f"   â€¢ ç‰¹å¾çº§èåˆç­–ç•¥ + è·¨æ¨¡æ€äº¤äº’")
    print(f"   â€¢ è¾“å‡º27ç»´æ ‡å‡†åŒ–æƒ…ç»ªå‘é‡")
    print(f"   â€¢ å¼ºåˆ¶å†³ç­–é€»è¾‘ç¡®ä¿å‡†ç¡®åˆ†ç±»")
    print(f"   â€¢ ä¸ç°æœ‰ç³»ç»Ÿå®Œå…¨å…¼å®¹")

def test_edge_cases():
    """æµ‹è¯•è¾¹ç•Œæƒ…å†µå’Œå¼‚å¸¸å¤„ç†"""
    
    print(f"\nğŸ§ª è¾¹ç•Œæƒ…å†µæµ‹è¯•")
    print("=" * 40)
    
    config = FusionLayerConfig(layer_name="test_fusion_layer")
    fusion_layer = FusionLayer(config)
    
    # æµ‹è¯•ç©ºè¾“å…¥
    print("1. ç©ºè¾“å…¥æµ‹è¯•:")
    emotion_vector = fusion_layer.infer_affective_state(None, None, None)
    print(f"   ç©ºè¾“å…¥ç»“æœ: ä¸»å¯¼æƒ…ç»ªç´¢å¼• {np.argmax(emotion_vector)} (æ¦‚ç‡: {max(emotion_vector):.3f})")
    
    # æµ‹è¯•æ— æ•ˆæ•°æ®
    print("\n2. æ— æ•ˆæ•°æ®æµ‹è¯•:")
    emotion_vector = fusion_layer.infer_affective_state({}, [], "")
    print(f"   æ— æ•ˆæ•°æ®ç»“æœ: ä¸»å¯¼æƒ…ç»ªç´¢å¼• {np.argmax(emotion_vector)} (æ¦‚ç‡: {max(emotion_vector):.3f})")
    
    # æµ‹è¯•æå€¼æ•°æ®
    print("\n3. æå€¼æ•°æ®æµ‹è¯•:")
    extreme_face = {f'au_{i:02d}': 1.0 for i in range(1, 46)}  # æ‰€æœ‰AUæœ€å¤§å€¼
    extreme_audio = {
        'pitch_mean': 500.0,  # æé«˜éŸ³è°ƒ
        'loudness_mean': 1.0,  # æœ€å¤§å“åº¦
        'speech_rate': 15.0   # æå¿«è¯­é€Ÿ
    }
    emotion_vector = fusion_layer.infer_affective_state(extreme_face, extreme_audio, "éå¸¸éå¸¸éå¸¸ç„¦è™‘æ„¤æ€’")
    print(f"   æå€¼æ•°æ®ç»“æœ: ä¸»å¯¼æƒ…ç»ªç´¢å¼• {np.argmax(emotion_vector)} (æ¦‚ç‡: {max(emotion_vector):.3f})")
    
    print("\nâœ… è¾¹ç•Œæƒ…å†µæµ‹è¯•é€šè¿‡")

if __name__ == "__main__":
    test_standard_fusion_interface()
    test_edge_cases()