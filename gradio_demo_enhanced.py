#!/usr/bin/env python3
"""
ğŸŒ™ ç¡çœ ç–—æ„ˆAI - å¢å¼ºæ¼”ç¤ºæ¨¡å¼
å¸¦è¿›åº¦æ¡å’Œå®æ—¶çŠ¶æ€æ›´æ–°çš„å®Œæ•´æ¼”ç¤º
"""

import gradio as gr
import numpy as np
import cv2
import os
import tempfile
import time
import threading
from pathlib import Path
from datetime import datetime
import json

# å…¨å±€çŠ¶æ€å˜é‡
processing_status = {
    "current_step": "",
    "progress": 0,
    "total_steps": 7,
    "details": "",
    "error": None
}

def update_progress(step_name, progress, details=""):
    """æ›´æ–°å¤„ç†è¿›åº¦"""
    global processing_status
    processing_status["current_step"] = step_name
    processing_status["progress"] = progress
    processing_status["details"] = details
    print(f"ğŸ“Š è¿›åº¦æ›´æ–°: {step_name} ({progress}/{processing_status['total_steps']}) - {details}")

def generate_mock_audio_with_progress(duration=15, sample_rate=44100):
    """ç”Ÿæˆæ¨¡æ‹ŸéŸ³é¢‘å¹¶æ˜¾ç¤ºè¿›åº¦"""
    update_progress("ğŸµ ç”Ÿæˆä¸‰é˜¶æ®µéŸ³é¢‘", 3, f"éŸ³é¢‘æ—¶é•¿: {duration}ç§’, é‡‡æ ·ç‡: {sample_rate}Hz")
    
    # æ¨¡æ‹ŸéŸ³é¢‘ç”Ÿæˆè¿‡ç¨‹
    stage_duration = duration // 3
    t = np.linspace(0, stage_duration, int(sample_rate * stage_duration))
    
    # ç¬¬ä¸€é˜¶æ®µï¼šåŒæ­¥æœŸ
    update_progress("ğŸµ ç¬¬ä¸€é˜¶æ®µ - åŒæ­¥æœŸ", 3, "ç”Ÿæˆé«˜é¢‘åŒ¹é…éŸ³é¢‘...")
    time.sleep(0.5)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
    stage1_freq = 440  # A4
    stage1 = 0.3 * np.sin(2 * np.pi * stage1_freq * t) * np.exp(-t/5)
    
    # ç¬¬äºŒé˜¶æ®µï¼šå¼•å¯¼æœŸ
    update_progress("ğŸµ ç¬¬äºŒé˜¶æ®µ - å¼•å¯¼æœŸ", 3, "ç”Ÿæˆè¿‡æ¸¡å¼•å¯¼éŸ³é¢‘...")
    time.sleep(0.5)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
    stage2_freq = 330  # E4
    stage2 = 0.2 * np.sin(2 * np.pi * stage2_freq * t) * np.exp(-t/8)
    
    # ç¬¬ä¸‰é˜¶æ®µï¼šå·©å›ºæœŸ
    update_progress("ğŸµ ç¬¬ä¸‰é˜¶æ®µ - å·©å›ºæœŸ", 3, "ç”Ÿæˆä½é¢‘æ”¾æ¾éŸ³é¢‘...")
    time.sleep(0.5)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
    stage3_freq = 220  # A3
    stage3 = 0.1 * np.sin(2 * np.pi * stage3_freq * t) * np.exp(-t/12)
    
    # åˆå¹¶å’Œåå¤„ç†
    update_progress("ğŸµ éŸ³é¢‘åå¤„ç†", 3, "åˆå¹¶ä¸‰é˜¶æ®µéŸ³é¢‘...")
    time.sleep(0.3)
    audio_array = np.concatenate([stage1, stage2, stage3])
    
    # æ·»åŠ ç¯å¢ƒéŸ³æ•ˆ
    noise = 0.05 * np.random.normal(0, 1, len(audio_array))
    audio_array = audio_array + noise
    
    # å½’ä¸€åŒ–
    audio_array = audio_array / np.max(np.abs(audio_array))
    
    return audio_array.astype(np.float32), sample_rate

def generate_mock_video_with_progress(duration=15, fps=30):
    """ç”Ÿæˆæ¨¡æ‹Ÿè§†é¢‘å¹¶æ˜¾ç¤ºè¿›åº¦"""
    update_progress("ğŸ¬ ç”Ÿæˆç–—æ„ˆè§†é¢‘", 4, f"è§†é¢‘æ—¶é•¿: {duration}ç§’, å¸§ç‡: {fps}fps")
    
    frame_count = int(duration * fps)
    frames = []
    width, height = 640, 480
    
    for i in range(frame_count):
        # æ›´æ–°è¿›åº¦
        if i % 30 == 0:  # æ¯ç§’æ›´æ–°ä¸€æ¬¡
            current_second = i // fps
            update_progress("ğŸ¬ æ¸²æŸ“è§†é¢‘å¸§", 4, f"æ­£åœ¨æ¸²æŸ“ç¬¬{current_second+1}ç§’ ({i+1}/{frame_count}å¸§)")
        
        # åˆ›å»ºæ¸å˜èƒŒæ™¯
        frame = np.zeros((height, width, 3), dtype=np.uint8)
        
        # æ—¶é—´è¿›åº¦
        progress = i / frame_count
        
        # ä¸‰é˜¶æ®µé¢œè‰²å˜åŒ–
        if progress < 0.33:  # ç¬¬ä¸€é˜¶æ®µï¼šåŒæ­¥æœŸ
            blue = int(255 * (1 - progress * 3))
            green = int(100 * progress * 3)
            red = int(50 * progress * 3)
            stage_name = "åŒæ­¥æœŸ"
        elif progress < 0.66:  # ç¬¬äºŒé˜¶æ®µï¼šå¼•å¯¼æœŸ
            stage_progress = (progress - 0.33) / 0.33
            blue = int(100 + 155 * (1 - stage_progress))
            green = int(100 * (1 - stage_progress))
            red = int(50 + 100 * stage_progress)
            stage_name = "å¼•å¯¼æœŸ"
        else:  # ç¬¬ä¸‰é˜¶æ®µï¼šå·©å›ºæœŸ
            stage_progress = (progress - 0.66) / 0.34
            blue = int(50 + 50 * (1 - stage_progress))
            green = int(20 * (1 - stage_progress))
            red = int(20 + 30 * (1 - stage_progress))
            stage_name = "å·©å›ºæœŸ"
        
        # å¡«å……æ¸å˜èƒŒæ™¯
        for y in range(height):
            for x in range(width):
                center_x, center_y = width // 2, height // 2
                distance = np.sqrt((x - center_x)**2 + (y - center_y)**2)
                max_distance = np.sqrt(center_x**2 + center_y**2)
                gradient = 1 - (distance / max_distance)
                
                frame[y, x] = [
                    int(blue * gradient),
                    int(green * gradient),
                    int(red * gradient)
                ]
        
        # æ·»åŠ å‘¼å¸æ•ˆæœåœ†åœˆ
        breathing_radius = 50 + 30 * np.sin(progress * 4 * np.pi)
        cv2.circle(frame, (width//2, height//2), int(breathing_radius), (255, 255, 255), 2)
        
        # æ·»åŠ é˜¶æ®µæ–‡å­—
        cv2.putText(frame, stage_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
        
        frames.append(frame)
    
    return frames, fps

def create_demo_video_with_progress(audio_array, sample_rate, video_frames, fps):
    """åˆ›å»ºæ¼”ç¤ºè§†é¢‘å¹¶æ˜¾ç¤ºè¿›åº¦"""
    try:
        update_progress("ğŸ¬ åˆæˆéŸ³ç”»åŒæ­¥è§†é¢‘", 5, "åˆ›å»ºä¸´æ—¶æ–‡ä»¶...")
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        temp_dir = tempfile.mkdtemp()
        audio_path = os.path.join(temp_dir, "demo_audio.wav")
        video_path = os.path.join(temp_dir, "demo_video.mp4")
        output_path = os.path.join(temp_dir, "demo_synchronized.mp4")
        
        # ä¿å­˜éŸ³é¢‘
        update_progress("ğŸ¬ ä¿å­˜éŸ³é¢‘æ–‡ä»¶", 5, f"ä¿å­˜åˆ°: {audio_path}")
        import soundfile as sf
        sf.write(audio_path, audio_array, sample_rate)
        
        # åˆ›å»ºè§†é¢‘
        update_progress("ğŸ¬ åˆ›å»ºè§†é¢‘æ–‡ä»¶", 5, f"å¤„ç†{len(video_frames)}å¸§...")
        if len(video_frames) > 0:
            frame_height, frame_width = video_frames[0].shape[:2]
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(video_path, fourcc, fps, (frame_width, frame_height))
            
            for i, frame in enumerate(video_frames):
                if i % 90 == 0:  # æ¯3ç§’æ›´æ–°ä¸€æ¬¡
                    update_progress("ğŸ¬ å†™å…¥è§†é¢‘å¸§", 5, f"å·²å†™å…¥{i+1}/{len(video_frames)}å¸§")
                out.write(frame)
            
            out.release()
            
            # ä½¿ç”¨ffmpegåˆæˆ
            update_progress("ğŸ¬ FFmpegéŸ³ç”»åŒæ­¥", 6, "æ­£åœ¨åˆæˆæœ€ç»ˆè§†é¢‘...")
            try:
                import subprocess
                ffmpeg_cmd = [
                    'ffmpeg', '-y', '-loglevel', 'error',
                    '-i', video_path,
                    '-i', audio_path,
                    '-c:v', 'libx264',
                    '-c:a', 'aac',
                    '-strict', 'experimental',
                    '-shortest',
                    output_path
                ]
                
                result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True)
                if result.returncode == 0:
                    update_progress("âœ… è§†é¢‘ç”Ÿæˆå®Œæˆ", 7, f"è¾“å‡ºæ–‡ä»¶: {output_path}")
                    return output_path
                else:
                    update_progress("âš ï¸ FFmpegä¸å¯ç”¨", 6, "è¿”å›çº¯è§†é¢‘æ–‡ä»¶")
                    return video_path
            except Exception as e:
                update_progress("âš ï¸ FFmpegé”™è¯¯", 6, f"é”™è¯¯: {str(e)}")
                return video_path
        else:
            update_progress("âŒ è§†é¢‘ç”Ÿæˆå¤±è´¥", 6, "æ²¡æœ‰è§†é¢‘å¸§")
            return None
            
    except Exception as e:
        update_progress("âŒ åˆ›å»ºè§†é¢‘å¤±è´¥", 6, f"é”™è¯¯: {str(e)}")
        return None

def get_status_info():
    """è·å–å½“å‰çŠ¶æ€ä¿¡æ¯"""
    global processing_status
    if processing_status["progress"] == 0:
        return "ğŸ¯ ç­‰å¾…å¼€å§‹å¤„ç†..."
    elif processing_status["progress"] >= processing_status["total_steps"]:
        return "âœ… å¤„ç†å®Œæˆï¼"
    else:
        progress_bar = "â–ˆ" * processing_status["progress"] + "â–‘" * (processing_status["total_steps"] - processing_status["progress"])
        return f"{processing_status['current_step']}\n[{progress_bar}] {processing_status['progress']}/{processing_status['total_steps']}\n{processing_status['details']}"

def enhanced_demo_processing(user_input):
    """å¢å¼ºçš„æ¼”ç¤ºæ¨¡å¼å¤„ç†"""
    global processing_status
    
    if not user_input or len(user_input.strip()) < 5:
        return "âš ï¸ è¯·è¾“å…¥è‡³å°‘5ä¸ªå­—ç¬¦çš„æƒ…ç»ªæè¿°", None, None, "è¾“å…¥å¤ªçŸ­"
    
    try:
        # é‡ç½®çŠ¶æ€
        processing_status = {
            "current_step": "",
            "progress": 0,
            "total_steps": 7,
            "details": "",
            "error": None
        }
        
        # æ­¥éª¤1: æƒ…ç»ªè¯†åˆ«
        update_progress("ğŸ§  æƒ…ç»ªè¯†åˆ«åˆ†æ", 1, "åˆ†æç”¨æˆ·è¾“å…¥çš„æƒ…ç»ªçŠ¶æ€...")
        time.sleep(0.5)
        
        # æ¨¡æ‹Ÿæƒ…ç»ªè¯†åˆ«
        emotions = {
            "ç„¦è™‘": {"confidence": 0.85, "type": "ç¡å‰ç„¦è™‘çŠ¶æ€"},
            "ç–²æƒ«": {"confidence": 0.82, "type": "èº«ä½“ç–²æƒ«çŠ¶æ€"},
            "çƒ¦èº": {"confidence": 0.88, "type": "æƒ…ç»ªçƒ¦èºçŠ¶æ€"},
            "å¹³é™": {"confidence": 0.75, "type": "ç›¸å¯¹å¹³é™çŠ¶æ€"},
            "å‹åŠ›": {"confidence": 0.90, "type": "å¿ƒç†å‹åŠ›çŠ¶æ€"}
        }
        
        detected_emotion = "ç„¦è™‘"
        for emotion_key in emotions.keys():
            if emotion_key in user_input:
                detected_emotion = emotion_key
                break
        
        emotion_info = emotions[detected_emotion]
        
        # æ­¥éª¤2: æ²»ç–—æ–¹æ¡ˆåˆ¶å®š
        update_progress("ğŸ“‹ åˆ¶å®šæ²»ç–—æ–¹æ¡ˆ", 2, f"åŸºäº{detected_emotion}æƒ…ç»ªåˆ¶å®šä¸‰é˜¶æ®µæ²»ç–—æ–¹æ¡ˆ...")
        time.sleep(0.5)
        
        # æ­¥éª¤3-5: éŸ³é¢‘ç”Ÿæˆ
        audio_array, sample_rate = generate_mock_audio_with_progress(duration=15)
        
        # æ­¥éª¤4: è§†é¢‘ç”Ÿæˆ
        video_frames, fps = generate_mock_video_with_progress(duration=15, fps=30)
        
        # æ­¥éª¤5-6: è§†é¢‘åˆæˆ
        video_path = create_demo_video_with_progress(audio_array, sample_rate, video_frames, fps)
        
        # æ­¥éª¤7: å®Œæˆ
        update_progress("ğŸ‰ ç”Ÿæˆå®Œæˆ", 7, "éŸ³ç”»åŒæ­¥ç–—æ„ˆè§†é¢‘å·²å‡†å¤‡å°±ç»ªï¼")
        
        # ç»„ç»‡è¿”å›ä¿¡æ¯
        emotion_result = f"""ğŸ§  æƒ…ç»ªè¯†åˆ«ç»“æœ:
æƒ…ç»ªç±»å‹: {detected_emotion}
ç½®ä¿¡åº¦: {emotion_info['confidence']:.1%}
çŠ¶æ€æè¿°: {emotion_info['type']}
è¯†åˆ«æ—¶é—´: {datetime.now().strftime('%H:%M:%S')}"""
        
        if video_path:
            info_text = f"""âœ… æ¼”ç¤ºæ¨¡å¼ç”Ÿæˆå®Œæˆï¼

ğŸµ éŸ³é¢‘ä¿¡æ¯:
  - æ—¶é•¿: 15ç§’ (3ä¸ªé˜¶æ®µå„5ç§’)
  - é‡‡æ ·ç‡: {sample_rate}Hz
  - å£°é“: ç«‹ä½“å£°
  - ä¸‰é˜¶æ®µè®¾è®¡: åŒæ­¥(440Hz) â†’ å¼•å¯¼(330Hz) â†’ å·©å›º(220Hz)

ğŸ¬ è§†é¢‘ä¿¡æ¯:
  - æ€»å¸§æ•°: {len(video_frames)}å¸§
  - å¸§ç‡: {fps}fps
  - åˆ†è¾¨ç‡: 640x480
  - è§†è§‰æ•ˆæœ: æ¸å˜é¢œè‰² + å‘¼å¸åœ†åœˆ + é˜¶æ®µæ ‡è¯†

ğŸ”§ æŠ€æœ¯ä¿¡æ¯:
  - å¤„ç†æ—¶é—´: {datetime.now().strftime('%H:%M:%S')}
  - æ–‡ä»¶è·¯å¾„: {video_path}
  - åˆæˆæ–¹å¼: OpenCV + FFmpeg

ğŸ“ æ³¨æ„äº‹é¡¹:
  - è¿™æ˜¯æ¼”ç¤ºæ¨¡å¼ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
  - çœŸå®ç‰ˆæœ¬éœ€è¦é…ç½®å•†ä¸šAPI (Suno, Runwayç­‰)
  - æ¼”ç¤ºéŸ³é¢‘é‡‡ç”¨æ•°å­¦åˆæˆï¼ŒçœŸå®ç‰ˆæœ¬ä½¿ç”¨AIç”Ÿæˆ"""
            
            return emotion_result, video_path, (sample_rate, audio_array), info_text
        else:
            return emotion_result, None, (sample_rate, audio_array), "âŒ è§†é¢‘ç”Ÿæˆå¤±è´¥"
    
    except Exception as e:
        error_msg = f"âŒ å¤„ç†é”™è¯¯: {str(e)}"
        print(error_msg)
        import traceback
        traceback.print_exc()
        return error_msg, None, None, f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}"

def create_enhanced_interface():
    """åˆ›å»ºå¢å¼ºçš„æ¼”ç¤ºç•Œé¢"""
    with gr.Blocks(
        title="ğŸŒ™ ç¡çœ ç–—æ„ˆAI - å¢å¼ºæ¼”ç¤ºæ¨¡å¼",
        theme=gr.themes.Soft(primary_hue="purple", secondary_hue="blue")
    ) as app:
        
        gr.HTML("""
        <div style="text-align: center; padding: 20px;">
            <h1>ğŸŒ™ ç¡çœ ç–—æ„ˆAI - å¢å¼ºæ¼”ç¤ºæ¨¡å¼</h1>
            <p style="color: #666;">å®Œæ•´çš„ä¸‰é˜¶æ®µéŸ³ä¹å™äº‹ç–—æ„ˆç³»ç»Ÿæ¼”ç¤º</p>
            <p style="color: orange; font-weight: bold;">âš ï¸ æ¼”ç¤ºæ¨¡å¼ï¼šä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼Œå±•ç¤ºå®Œæ•´åŠŸèƒ½æµç¨‹</p>
        </div>
        """)
        
        with gr.Row():
            with gr.Column(scale=2):
                gr.Markdown("### ğŸ’­ æƒ…ç»ªè¾“å…¥")
                
                # å¿«é€Ÿé€‰æ‹©
                emotion_examples = gr.Radio(
                    choices=[
                        "ğŸ˜° æˆ‘æ„Ÿåˆ°å¾ˆç„¦è™‘ï¼Œå¿ƒè·³åŠ é€Ÿï¼Œéš¾ä»¥å…¥ç¡",
                        "ğŸ˜´ æˆ‘å¾ˆç–²æƒ«ï¼Œä½†å¤§è„‘è¿˜åœ¨æ´»è·ƒï¼Œæ— æ³•æ”¾æ¾",
                        "ğŸ˜¤ æˆ‘æ„Ÿåˆ°çƒ¦èºä¸å®‰ï¼Œå®¹æ˜“è¢«å°äº‹å½±å“",
                        "ğŸ˜Œ æˆ‘æ¯”è¾ƒå¹³é™ï¼Œä½†å¸Œæœ›æ›´æ·±å±‚çš„æ”¾æ¾",
                        "ğŸ¤¯ æœ€è¿‘å‹åŠ›å¾ˆå¤§ï¼Œæ€»æ˜¯æ„Ÿåˆ°ç´§å¼ "
                    ],
                    label="ğŸ­ å¿«é€Ÿé€‰æ‹©å¸¸è§æƒ…ç»ª",
                    value="ğŸ˜° æˆ‘æ„Ÿåˆ°å¾ˆç„¦è™‘ï¼Œå¿ƒè·³åŠ é€Ÿï¼Œéš¾ä»¥å…¥ç¡"
                )
                
                emotion_input = gr.Textbox(
                    label="âœï¸ è¯¦ç»†æè¿°æ‚¨çš„æ„Ÿå—",
                    placeholder="æè¿°æ‚¨ç°åœ¨çš„æƒ…ç»ªçŠ¶æ€ã€èº«ä½“æ„Ÿå—ã€æ€ç»´çŠ¶æ€ç­‰...",
                    lines=4,
                    value="æˆ‘æ„Ÿåˆ°å¾ˆç„¦è™‘ï¼Œå¿ƒè·³åŠ é€Ÿï¼Œéš¾ä»¥å…¥ç¡"
                )
                
                with gr.Row():
                    process_btn = gr.Button("ğŸ¬ å¼€å§‹ç”ŸæˆéŸ³ç”»åŒæ­¥ç–—æ„ˆè§†é¢‘", variant="primary", size="lg")
                    clear_btn = gr.Button("ğŸ”„ æ¸…é™¤", variant="secondary")
                
                # å®æ—¶çŠ¶æ€æ˜¾ç¤º
                status_display = gr.Textbox(
                    label="ğŸ“Š å®æ—¶å¤„ç†çŠ¶æ€",
                    value="ğŸ¯ ç­‰å¾…å¼€å§‹å¤„ç†...",
                    lines=3,
                    interactive=False
                )
            
            with gr.Column(scale=3):
                gr.Markdown("### ğŸ¬ ç”Ÿæˆç»“æœ")
                
                # æƒ…ç»ªè¯†åˆ«ç»“æœ
                emotion_result = gr.Textbox(
                    label="ğŸ§  æƒ…ç»ªè¯†åˆ«ç»“æœ",
                    lines=4,
                    interactive=False
                )
                
                # ä¸»è¦è¾“å‡ºï¼šéŸ³ç”»åŒæ­¥è§†é¢‘
                video_output = gr.Video(
                    label="ğŸ¬ ä¸‰é˜¶æ®µéŸ³ç”»åŒæ­¥ç–—æ„ˆè§†é¢‘",
                    height=400
                )
                
                # éŸ³é¢‘è¾“å‡º
                audio_output = gr.Audio(
                    label="ğŸµ ä¸‰é˜¶æ®µç–—æ„ˆéŸ³é¢‘",
                    type="numpy"
                )
                
                # è¯¦ç»†ä¿¡æ¯
                info_output = gr.Textbox(
                    label="ğŸ“Š è¯¦ç»†æŠ€æœ¯ä¿¡æ¯",
                    lines=12,
                    interactive=False
                )
        
        # ç³»ç»Ÿè¯´æ˜
        gr.HTML("""
        <div style="margin-top: 30px; padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; color: white;">
            <h3>ğŸ¯ ç³»ç»Ÿç‰¹è‰²</h3>
            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin-top: 15px;">
                <div>
                    <h4>ğŸ§  æƒ…ç»ªè¯†åˆ«</h4>
                    <p>â€¢ 27ç»´ç»†ç²’åº¦æƒ…ç»ªåˆ†ç±»<br>â€¢ ç¡å‰ä¸“ç”¨æƒ…ç»ªè¯†åˆ«<br>â€¢ å®æ—¶ç½®ä¿¡åº¦è¯„ä¼°</p>
                </div>
                <div>
                    <h4>ğŸµ ä¸‰é˜¶æ®µéŸ³ä¹</h4>
                    <p>â€¢ åŒæ­¥æœŸï¼šåŒ¹é…ç”¨æˆ·æƒ…ç»ª<br>â€¢ å¼•å¯¼æœŸï¼šé€æ­¥æƒ…ç»ªè½¬æ¢<br>â€¢ å·©å›ºæœŸï¼šæ·±åº¦æ”¾æ¾çŠ¶æ€</p>
                </div>
                <div>
                    <h4>ğŸ¬ è§†è§‰ç–—æ„ˆ</h4>
                    <p>â€¢ é¢œè‰²æ¸å˜ç–—æ„ˆ<br>â€¢ å‘¼å¸å¼•å¯¼åŠ¨ç”»<br>â€¢ éŸ³ç”»å®Œç¾åŒæ­¥</p>
                </div>
            </div>
        </div>
        """)
        
        # äº‹ä»¶ç»‘å®š
        def update_input_from_example(example):
            return example.split(" ", 1)[1] if " " in example else example
        
        emotion_examples.change(
            update_input_from_example,
            inputs=emotion_examples,
            outputs=emotion_input
        )
        
        # å¼‚æ­¥å¤„ç†å‡½æ•°
        def process_with_status_updates(user_input):
            # å¤„ç†å‡½æ•°
            result = enhanced_demo_processing(user_input)
            return result
        
        process_btn.click(
            process_with_status_updates,
            inputs=emotion_input,
            outputs=[emotion_result, video_output, audio_output, info_output]
        )
        
        clear_btn.click(
            lambda: ("", None, None, "", "ğŸ¯ ç­‰å¾…å¼€å§‹å¤„ç†..."),
            outputs=[emotion_input, video_output, audio_output, info_output, status_display]
        )
        
        # çŠ¶æ€æ›´æ–°ï¼ˆæ¯ç§’æ›´æ–°ä¸€æ¬¡ï¼‰
        def update_status():
            return get_status_info()
        
        # å®šæ—¶æ›´æ–°çŠ¶æ€
        app.load(lambda: gr.update(value=get_status_info()), outputs=status_display, every=1)
    
    return app

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨å¢å¼ºæ¼”ç¤ºæ¨¡å¼...")
    print("ğŸ“Š åŒ…å«è¯¦ç»†è¿›åº¦æ˜¾ç¤ºå’ŒçŠ¶æ€æ›´æ–°")
    
    app = create_enhanced_interface()
    app.launch(
        server_name="0.0.0.0",
        server_port=7864,
        share=True,
        debug=True
    )

if __name__ == "__main__":
    main()