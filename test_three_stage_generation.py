#!/usr/bin/env python3
"""
æµ‹è¯•ISOä¸‰é˜¶æ®µéŸ³ç”»åŒæ­¥ç”Ÿæˆ
éªŒè¯ç”Ÿæˆå±‚çš„ä¸‰é˜¶æ®µå†…å®¹ç”ŸæˆåŠŸèƒ½
"""

import sys
import os
import asyncio
import json
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from layers.input_layer import InputLayer, InputLayerConfig
from layers.fusion_layer import FusionLayer, FusionLayerConfig  
from layers.mapping_layer import MappingLayer, MappingLayerConfig
from layers.generation_layer import GenerationLayer, GenerationLayerConfig
from layers.base_layer import LayerData
from datetime import datetime
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def test_three_stage_generation():
    """æµ‹è¯•ISOä¸‰é˜¶æ®µéŸ³ç”»åŒæ­¥ç”Ÿæˆ"""
    
    print("ğŸ­ ISOä¸‰é˜¶æ®µéŸ³ç”»åŒæ­¥ç”Ÿæˆæµ‹è¯•")
    print("=" * 60)
    
    # åˆå§‹åŒ–æ‰€æœ‰å±‚
    input_config = InputLayerConfig(layer_name="input_layer")
    fusion_config = FusionLayerConfig(layer_name="fusion_layer")
    mapping_config = MappingLayerConfig(layer_name="mapping_layer")
    generation_config = GenerationLayerConfig(
        layer_name="generation_layer",
        audio_enabled=True,
        video_enabled=True,
        audio_duration=30.0,  # ç¼©çŸ­æµ‹è¯•æ—¶é—´
        video_duration=30.0,
        video_fps=5  # é™ä½å¸§ç‡åŠ å¿«æµ‹è¯•
    )
    
    input_layer = InputLayer(input_config)
    fusion_layer = FusionLayer(fusion_config)
    mapping_layer = MappingLayer(mapping_config)
    generation_layer = GenerationLayer(generation_config)
    
    # æµ‹è¯•åœºæ™¯ï¼šç¡çœ ç„¦è™‘æƒ…å¢ƒ
    test_scenario = {
        "text": "æˆ‘ä»Šå¤©æ„Ÿåˆ°å¾ˆç„¦è™‘ï¼Œèººåœ¨åºŠä¸Šç¡ä¸ç€ï¼Œå¿ƒæƒ…ç‰¹åˆ«æ²®ä¸§",
        "expected_emotion": "sleep_anxiety"
    }
    
    print(f"\nğŸ¯ æµ‹è¯•åœºæ™¯: {test_scenario['text']}")
    print("=" * 50)
    
    # é˜¶æ®µ1ï¼šè¾“å…¥å±‚å¤„ç†
    print("\nğŸ“¥ é˜¶æ®µ1: è¾“å…¥å±‚å¤„ç†")
    input_data = LayerData(
        layer_name="test_input",
        timestamp=datetime.now(),
        data={
            "multimodal_data": {
                "text": {
                    "text": test_scenario["text"],
                    "features": {
                        "emotion_keywords": [],
                        "semantic_features": {
                            "sentiment_polarity": -0.3,
                            "subjectivity": 0.8,
                            "complexity": 0.05
                        },
                        "text_length": len(test_scenario["text"]),
                        "sentence_count": 1
                    }
                }
            },
            "mode": "text_only"
        }
    )
    
    input_result = await input_layer._process_impl(input_data)
    print(f"âœ… è¾“å…¥å±‚å®Œæˆ - æ•°æ®æ¨¡å¼: {input_result.data.get('mode', 'unknown')}")
    
    # é˜¶æ®µ2ï¼šèåˆå±‚å¤„ç†
    print("\nğŸ§  é˜¶æ®µ2: èåˆå±‚å¤„ç†")
    fusion_result = await fusion_layer._process_impl(input_result)
    emotion_analysis = fusion_result.data.get('emotion_analysis', {})
    primary_emotion = emotion_analysis.get('primary_emotion', {})
    print(f"âœ… èåˆå±‚å®Œæˆ - ä¸»è¦æƒ…ç»ª: {primary_emotion.get('name', 'unknown')} "
          f"(ç½®ä¿¡åº¦: {primary_emotion.get('probability', 0):.3f})")
    
    # é˜¶æ®µ3ï¼šæ˜ å°„å±‚å¤„ç†ï¼ˆç”ŸæˆISOä¸‰é˜¶æ®µå‚æ•°ï¼‰
    print("\nğŸ—ºï¸ é˜¶æ®µ3: æ˜ å°„å±‚å¤„ç†")
    mapping_result = await mapping_layer._process_impl(fusion_result)
    print(f"âœ… æ˜ å°„å±‚å®Œæˆ - ç½®ä¿¡åº¦: {mapping_result.confidence:.3f}")
    
    # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†ISOä¸‰é˜¶æ®µå‚æ•°
    iso_params = mapping_result.data.get('iso_three_stage_params')
    if iso_params:
        print(f"\nğŸ¼ ISOä¸‰é˜¶æ®µå‚æ•°ç”ŸæˆæˆåŠŸ:")
        stages = ['match_stage', 'guide_stage', 'target_stage']
        total_duration = 0
        
        for stage_name in stages:
            stage_data = iso_params[stage_name]
            stage_duration = stage_data['stage_duration']
            total_duration += stage_duration
            
            print(f"  ğŸ“ {stage_name}:")
            print(f"     èŠ‚æ‹: {stage_data['tempo_bpm']:.1f} BPM")
            print(f"     è°ƒæ€§: {stage_data['key_signature']}")
            print(f"     åŠ›åº¦: {stage_data['dynamics']}")
            print(f"     æŒç»­: {stage_duration:.1f}åˆ†é’Ÿ")
            print(f"     æ²»ç–—å¼ºåº¦: {stage_data.get('therapy_intensity', 0.0):.2f}")
        
        print(f"  ğŸ•’ æ€»æ²»ç–—æ—¶é•¿: {total_duration:.1f}åˆ†é’Ÿ")
    else:
        print("âŒ æœªç”ŸæˆISOä¸‰é˜¶æ®µå‚æ•°ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•")
        return
    
    # é˜¶æ®µ4ï¼šç”Ÿæˆå±‚å¤„ç†ï¼ˆä¸‰é˜¶æ®µéŸ³ç”»åŒæ­¥ç”Ÿæˆï¼‰
    print(f"\nğŸ¬ é˜¶æ®µ4: ä¸‰é˜¶æ®µéŸ³ç”»åŒæ­¥ç”Ÿæˆ")
    print("=" * 40)
    
    generation_start_time = datetime.now()
    generation_result = await generation_layer._process_impl(mapping_result)
    generation_end_time = datetime.now()
    generation_time = (generation_end_time - generation_start_time).total_seconds()
    
    print(f"âœ… ç”Ÿæˆå±‚å®Œæˆ - è€—æ—¶: {generation_time:.1f}ç§’")
    
    # åˆ†æç”Ÿæˆç»“æœ
    generated_content = generation_result.data.get('generated_content', {})
    
    if 'stages' in generated_content:
        print(f"\nğŸ­ ä¸‰é˜¶æ®µå†…å®¹ç”Ÿæˆåˆ†æ:")
        print(f"   â€¢ è¿è´¯å™äº‹: {generated_content.get('continuous_narrative', False)}")
        print(f"   â€¢ æ€»æ—¶é•¿: {generated_content.get('total_duration', 0):.1f}åˆ†é’Ÿ")
        
        # åˆ†ææ¯ä¸ªé˜¶æ®µçš„ç”Ÿæˆç»“æœ
        stages_data = generated_content['stages']
        for stage_idx, stage_name in enumerate(['match_stage', 'guide_stage', 'target_stage'], 1):
            if stage_name in stages_data:
                stage_content = stages_data[stage_name]
                stage_info = stage_content.get('stage_info', {})
                
                print(f"\n   ğŸ¼ é˜¶æ®µ{stage_idx}: {stage_name}")
                print(f"      ç›®æ ‡: {stage_info.get('emotional_target', 'neutral')}")
                print(f"      æ—¶é•¿: {stage_info.get('stage_duration', 0):.1f}åˆ†é’Ÿ")
                print(f"      èŠ‚æ‹: {stage_info.get('tempo_bpm', 0):.1f} BPM")
                
                # éŸ³é¢‘åˆ†æ
                if 'audio' in stage_content:
                    audio_data = stage_content['audio']
                    if 'error' not in audio_data:
                        print(f"      ğŸµ éŸ³é¢‘: âœ… {audio_data.get('duration', 0):.1f}s, "
                              f"{audio_data.get('sample_rate', 0)}Hz, "
                              f"{audio_data.get('format', 'Unknown')}")
                        
                        # éŸ³é¢‘æ•°ç»„åˆ†æ
                        audio_array = audio_data.get('audio_array')
                        if audio_array is not None:
                            import numpy as np
                            rms = np.sqrt(np.mean(audio_array**2))
                            peak = np.max(np.abs(audio_array))
                            print(f"               RMS: {rms:.3f}, Peak: {peak:.3f}")
                    else:
                        print(f"      ğŸµ éŸ³é¢‘: âŒ {audio_data.get('error', 'æœªçŸ¥é”™è¯¯')}")
                
                # è§†é¢‘åˆ†æ
                if 'video' in stage_content:
                    video_data = stage_content['video']
                    if 'error' not in video_data:
                        frames = video_data.get('frames', [])
                        print(f"      ğŸ¬ è§†é¢‘: âœ… {len(frames)}å¸§, "
                              f"{video_data.get('fps', 0)}fps, "
                              f"{video_data.get('resolution', (0, 0))}")
                        
                        # è§†é¢‘å¸§åˆ†æ
                        if frames:
                            import numpy as np
                            first_frame = frames[0]
                            last_frame = frames[-1]
                            first_brightness = np.mean(first_frame)
                            last_brightness = np.mean(last_frame)
                            brightness_change = last_brightness - first_brightness
                            print(f"               äº®åº¦å˜åŒ–: {first_brightness:.1f} â†’ {last_brightness:.1f} "
                                  f"({brightness_change:+.1f})")
                    else:
                        print(f"      ğŸ¬ è§†é¢‘: âŒ {video_data.get('error', 'æœªçŸ¥é”™è¯¯')}")
                
                # åŒæ­¥åˆ†æ
                if 'sync_metadata' in stage_content:
                    sync_info = stage_content['sync_metadata']
                    sync_accuracy = sync_info.get('sync_accuracy', 0)
                    print(f"      ğŸ”— åŒæ­¥: {sync_accuracy:.1%} å‡†ç¡®åº¦")
        
        # å™äº‹è¿è´¯æ€§åˆ†æ
        narrative_quality = generated_content.get('narrative_quality', {})
        if narrative_quality:
            print(f"\n   ğŸ“– å™äº‹è¿è´¯æ€§åˆ†æ:")
            print(f"      â€¢ èŠ‚æ‹è¿è´¯æ€§: {'âœ…' if narrative_quality.get('tempo_coherence', False) else 'âŒ'}")
            print(f"      â€¢ æ²»ç–—è¿è´¯æ€§: {'âœ…' if narrative_quality.get('therapy_coherence', False) else 'âŒ'}")
            print(f"      â€¢ æ€»ä½“è¯„åˆ†: {narrative_quality.get('overall_coherence_score', 0):.2f}")
            print(f"      â€¢ å™äº‹ç±»å‹: {narrative_quality.get('narrative_type', 'æœªçŸ¥')}")
        
        # é˜¶æ®µè½¬æ¢åˆ†æ
        sync_metadata = generated_content.get('sync_metadata', {})
        transitions = sync_metadata.get('stage_transitions', [])
        if transitions:
            print(f"\n   ğŸ”„ é˜¶æ®µè½¬æ¢åˆ†æ:")
            for transition in transitions:
                print(f"      â€¢ {transition['from_stage']} â†’ {transition['to_stage']}")
                print(f"        è½¬æ¢ç‚¹: {transition['transition_point']:.1f}åˆ†é’Ÿ")
                print(f"        æ–¹æ³•: {transition['transition_method']}")
                print(f"        è¿è´¯æ€§: {transition['continuity_score']:.2f}")
    
    else:
        print("âŒ æœªç”Ÿæˆä¸‰é˜¶æ®µå†…å®¹")
    
    print(f"\nğŸ‰ ISOä¸‰é˜¶æ®µéŸ³ç”»åŒæ­¥ç”Ÿæˆæµ‹è¯•å®Œæˆï¼")
    print(f"\nğŸ“Š æµ‹è¯•æ€»ç»“:")
    print(f"   âœ… è¾“å…¥å±‚æ–‡æœ¬å¤„ç† - æ­£å¸¸")
    print(f"   âœ… èåˆå±‚æƒ…ç»ªè¯†åˆ« - æ­£å¸¸") 
    print(f"   âœ… æ˜ å°„å±‚ISOå‚æ•°ç”Ÿæˆ - æ­£å¸¸")
    print(f"   âœ… ç”Ÿæˆå±‚ä¸‰é˜¶æ®µå†…å®¹ç”Ÿæˆ - æ­£å¸¸")
    print(f"   â±ï¸ æ€»å¤„ç†æ—¶é—´: {generation_time:.1f}ç§’")
    print(f"\nğŸ’¡ ç‰¹è‰²åŠŸèƒ½:")
    print(f"   â€¢ ä¸‰é˜¶æ®µè¿è´¯éŸ³ä¹å™äº‹ (match â†’ guide â†’ target)")
    print(f"   â€¢ é˜¶æ®µç‰¹å®šçš„éŸ³é¢‘æ•ˆæœå¤„ç†")
    print(f"   â€¢ é˜¶æ®µç‰¹å®šçš„è§†è§‰æ•ˆæœå¤„ç†") 
    print(f"   â€¢ è·¨é˜¶æ®µéŸ³ç”»åŒæ­¥ä¼˜åŒ–")
    print(f"   â€¢ å™äº‹è¿è´¯æ€§è‡ªåŠ¨éªŒè¯")

if __name__ == "__main__":
    asyncio.run(test_three_stage_generation())