#!/usr/bin/env python3
"""
ğŸŒ™ å¢å¼ºä¸‰é˜¶æ®µç–—æ„ˆç³»ç»Ÿæ¼”ç¤ºç‰ˆæœ¬
ä¸“æ³¨æµç•…è¿‡æ¸¡å’Œå®Œç¾åŒæ­¥ï¼Œè‡ªåŠ¨è¿è¡Œæ¼”ç¤º
"""

import numpy as np
import sys
import os
import tempfile
import time
from pathlib import Path
import json

def generate_enhanced_therapy_audio(duration=20, sample_rate=44100, emotion="ç„¦è™‘"):
    """ç”Ÿæˆå¢å¼ºçš„ä¸‰é˜¶æ®µç–—æ„ˆéŸ³é¢‘ï¼ˆæµç•…è¿‡æ¸¡ç‰ˆæœ¬ï¼‰"""
    print(f"ğŸµ ç”Ÿæˆ{duration}ç§’å¢å¼ºä¸‰é˜¶æ®µç–—æ„ˆéŸ³é¢‘ (é’ˆå¯¹{emotion}æƒ…ç»ª)")
    
    # æ ¹æ®æƒ…ç»ªè°ƒæ•´å‚æ•°
    emotion_params = {
        "ç„¦è™‘": {
            "sync_freq": 440, "guide_freq": 330, "consolidate_freq": 220,
            "sync_intensity": 0.4, "guide_intensity": 0.25, "consolidate_intensity": 0.15,
            "transition_type": "exponential"
        },
        "ç–²æƒ«": {
            "sync_freq": 380, "guide_freq": 280, "consolidate_freq": 200,
            "sync_intensity": 0.35, "guide_intensity": 0.2, "consolidate_intensity": 0.1,
            "transition_type": "linear"
        },
        "çƒ¦èº": {
            "sync_freq": 460, "guide_freq": 350, "consolidate_freq": 240,
            "sync_intensity": 0.45, "guide_intensity": 0.3, "consolidate_intensity": 0.18,
            "transition_type": "sigmoid"
        },
        "å¹³é™": {
            "sync_freq": 400, "guide_freq": 320, "consolidate_freq": 210,
            "sync_intensity": 0.3, "guide_intensity": 0.2, "consolidate_intensity": 0.12,
            "transition_type": "smooth"
        },
        "å‹åŠ›": {
            "sync_freq": 480, "guide_freq": 360, "consolidate_freq": 230,
            "sync_intensity": 0.5, "guide_intensity": 0.32, "consolidate_intensity": 0.2,
            "transition_type": "exponential"
        }
    }
    
    params = emotion_params.get(emotion, emotion_params["ç„¦è™‘"])
    
    # ä¸‰é˜¶æ®µæ—¶é—´åˆ†é…ï¼ˆæµç•…è¿‡æ¸¡ï¼‰
    stage1_duration = duration * 0.3  # 30% - åŒæ­¥æœŸ
    stage2_duration = duration * 0.4  # 40% - å¼•å¯¼æœŸï¼ˆæœ€é‡è¦ï¼‰
    stage3_duration = duration * 0.3  # 30% - å·©å›ºæœŸ
    
    print(f"ğŸµ ä¸‰é˜¶æ®µæ—¶é•¿åˆ†é…: åŒæ­¥æœŸ{stage1_duration:.1f}s â†’ å¼•å¯¼æœŸ{stage2_duration:.1f}s â†’ å·©å›ºæœŸ{stage3_duration:.1f}s")
    
    # ç”Ÿæˆå®Œæ•´éŸ³é¢‘æ•°ç»„
    total_samples = int(sample_rate * duration)
    audio_array = np.zeros(total_samples)
    
    # æ—¶é—´è½´
    t_total = np.linspace(0, duration, total_samples)
    
    # ç¬¬ä¸€é˜¶æ®µï¼šåŒæ­¥æœŸ - åŒ¹é…ç”¨æˆ·æƒ…ç»ª
    stage1_end = stage1_duration
    stage1_mask = t_total <= stage1_end
    stage1_time = t_total[stage1_mask]
    
    print(f"ğŸµ ç¬¬ä¸€é˜¶æ®µ-åŒæ­¥æœŸ: {params['sync_freq']}Hz, å¼ºåº¦{params['sync_intensity']}")
    stage1_audio = params['sync_intensity'] * np.sin(2 * np.pi * params['sync_freq'] * stage1_time)
    
    # æ·»åŠ æƒ…ç»ªç‰¹å¾ï¼ˆé’ˆå¯¹ç„¦è™‘æ·»åŠ è½»å¾®é¢¤æŠ–ï¼‰
    if emotion == "ç„¦è™‘":
        tremolo = 0.1 * np.sin(2 * np.pi * 5 * stage1_time)  # 5Hzé¢¤éŸ³
        stage1_audio *= (1 + tremolo)
    elif emotion == "ç–²æƒ«":
        # æ·»åŠ è¡°å‡æ•ˆæœ
        stage1_audio *= np.exp(-stage1_time / 8)
    
    audio_array[stage1_mask] = stage1_audio
    
    # ç¬¬äºŒé˜¶æ®µï¼šå¼•å¯¼æœŸ - æµç•…è¿‡æ¸¡
    stage2_start = stage1_duration
    stage2_end = stage2_start + stage2_duration
    stage2_mask = (t_total > stage2_start) & (t_total <= stage2_end)
    stage2_time = t_total[stage2_mask] - stage2_start
    
    print(f"ğŸµ ç¬¬äºŒé˜¶æ®µ-å¼•å¯¼æœŸ: {params['sync_freq']}Hzâ†’{params['guide_freq']}Hz, æµç•…è¿‡æ¸¡")
    
    # é¢‘ç‡å’Œå¼ºåº¦çš„æµç•…è¿‡æ¸¡
    transition_progress = stage2_time / stage2_duration
    
    # é€‰æ‹©è¿‡æ¸¡å‡½æ•°
    if params['transition_type'] == "exponential":
        transition_curve = 1 - np.exp(-3 * transition_progress)
    elif params['transition_type'] == "sigmoid":
        transition_curve = 1 / (1 + np.exp(-6 * (transition_progress - 0.5)))
    elif params['transition_type'] == "linear":
        transition_curve = transition_progress
    else:  # smooth
        transition_curve = 3 * transition_progress**2 - 2 * transition_progress**3
    
    # åŠ¨æ€é¢‘ç‡å˜åŒ–
    current_freq = params['sync_freq'] + (params['guide_freq'] - params['sync_freq']) * transition_curve
    current_intensity = params['sync_intensity'] + (params['guide_intensity'] - params['sync_intensity']) * transition_curve
    
    stage2_audio = current_intensity * np.sin(2 * np.pi * current_freq * stage2_time)
    
    # æ·»åŠ å’Œè°æ³›éŸ³ï¼ˆå¢å¼ºç–—æ„ˆæ•ˆæœï¼‰
    harmonic1 = 0.3 * current_intensity * np.sin(2 * np.pi * current_freq * 2 * stage2_time)
    harmonic2 = 0.2 * current_intensity * np.sin(2 * np.pi * current_freq * 3 * stage2_time)
    stage2_audio += harmonic1 + harmonic2
    
    audio_array[stage2_mask] = stage2_audio
    
    # ç¬¬ä¸‰é˜¶æ®µï¼šå·©å›ºæœŸ - æ·±åº¦æ”¾æ¾
    stage3_start = stage2_end
    stage3_mask = t_total > stage3_start
    stage3_time = t_total[stage3_mask] - stage3_start
    
    print(f"ğŸµ ç¬¬ä¸‰é˜¶æ®µ-å·©å›ºæœŸ: {params['consolidate_freq']}Hz, æ·±åº¦æ”¾æ¾")
    
    # ä»å¼•å¯¼æœŸé¢‘ç‡å¹³æ»‘è¿‡æ¸¡åˆ°å·©å›ºæœŸ
    consolidate_transition = np.exp(-stage3_time / 3)  # æ¸è¿›è¡°å‡
    final_freq = params['guide_freq'] + (params['consolidate_freq'] - params['guide_freq']) * (1 - consolidate_transition)
    final_intensity = params['consolidate_intensity'] * np.exp(-stage3_time / 10)  # æ¸è¿›å‡å¼±
    
    stage3_audio = final_intensity * np.sin(2 * np.pi * final_freq * stage3_time)
    
    # æ·»åŠ è‡ªç„¶ç¯å¢ƒéŸ³ï¼ˆç™½å™ªå£° + æµ·æµªå£°ï¼‰
    nature_sound = 0.05 * np.random.normal(0, 1, len(stage3_time))
    wave_sound = 0.1 * final_intensity * np.sin(2 * np.pi * 0.3 * stage3_time)  # ç¼“æ…¢æµ·æµª
    stage3_audio += nature_sound + wave_sound
    
    audio_array[stage3_mask] = stage3_audio
    
    # æ•´ä½“åå¤„ç†
    print("ğŸµ éŸ³é¢‘åå¤„ç†: ç«‹ä½“å£° + ç©ºé—´åŒ–...")
    
    # åˆ›å»ºç«‹ä½“å£°
    left_channel = audio_array
    right_channel = audio_array.copy()
    
    # æ·»åŠ è½»å¾®çš„ç«‹ä½“å£°æ•ˆæœ
    stereo_delay = int(0.01 * sample_rate)  # 10mså»¶è¿Ÿ
    if len(right_channel) > stereo_delay:
        right_channel[stereo_delay:] = audio_array[:-stereo_delay]
    
    # æ·»åŠ ç©ºé—´æ··å“
    reverb = 0.1 * np.convolve(audio_array, np.exp(-np.linspace(0, 2, int(0.5 * sample_rate))), mode='same')
    left_channel += reverb
    right_channel += reverb * 0.8
    
    # åˆå¹¶ç«‹ä½“å£°
    stereo_audio = np.column_stack([left_channel, right_channel])
    
    # æœ€ç»ˆå½’ä¸€åŒ–
    stereo_audio = stereo_audio / np.max(np.abs(stereo_audio)) * 0.8  # ç•™20%ä½™é‡
    
    # æ·»åŠ æ·¡å…¥æ·¡å‡º
    fade_samples = int(0.5 * sample_rate)  # 0.5ç§’æ·¡å…¥æ·¡å‡º
    fade_in = np.linspace(0, 1, fade_samples)
    fade_out = np.linspace(1, 0, fade_samples)
    
    stereo_audio[:fade_samples] *= fade_in[:, np.newaxis]
    stereo_audio[-fade_samples:] *= fade_out[:, np.newaxis]
    
    return stereo_audio.astype(np.float32), sample_rate, params

def save_audio_file(audio_array, sample_rate, output_path):
    """ä¿å­˜éŸ³é¢‘æ–‡ä»¶ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    try:
        # å°è¯•ä½¿ç”¨scipyä¿å­˜
        from scipy.io import wavfile
        # è½¬æ¢ä¸º16ä½æ•´æ•°
        audio_int = (audio_array * 32767).astype(np.int16)
        wavfile.write(output_path, sample_rate, audio_int)
        return True
    except ImportError:
        # å¦‚æœscipyä¸å¯ç”¨ï¼Œä¿å­˜ä¸ºnumpyæ•°ç»„
        np.save(output_path.replace('.wav', '.npy'), audio_array)
        print(f"âš ï¸ å·²ä¿å­˜ä¸ºnumpyæ•°ç»„æ ¼å¼: {output_path.replace('.wav', '.npy')}")
        return False

def simple_emotion_detection(user_input):
    """ç®€åŒ–çš„æƒ…ç»ªæ£€æµ‹"""
    emotions = {
        "ç„¦è™‘": ["ç„¦è™‘", "ç´§å¼ ", "æ‹…å¿ƒ", "ä¸å®‰", "å®³æ€•"],
        "ç–²æƒ«": ["ç–²æƒ«", "ç´¯", "ç–²åŠ³", "å›°å€¦", "ä¹åŠ›"],
        "çƒ¦èº": ["çƒ¦èº", "çƒ¦æ¼", "æ˜“æ€’", "æ€¥èº", "ä¸è€çƒ¦"],
        "å¹³é™": ["å¹³é™", "æ”¾æ¾", "å®‰é™", "å®é™", "èˆ’ç¼“"],
        "å‹åŠ›": ["å‹åŠ›", "ç´§è¿«", "è´Ÿæ‹…", "é‡å‹", "æ²‰é‡"]
    }
    
    detected_emotion = "ç„¦è™‘"  # é»˜è®¤
    max_score = 0
    
    for emotion, keywords in emotions.items():
        score = sum(1 for keyword in keywords if keyword in user_input)
        if score > max_score:
            max_score = score
            detected_emotion = emotion
    
    confidence = min(0.85 + max_score * 0.05, 0.95)
    return detected_emotion, confidence

def run_demo_scenarios():
    """è¿è¡Œå¤šç§æƒ…ç»ªåœºæ™¯çš„æ¼”ç¤º"""
    print("ğŸš€ å¯åŠ¨å¢å¼ºä¸‰é˜¶æ®µç–—æ„ˆç³»ç»Ÿæ¼”ç¤º...")
    print("ğŸŒŠ ç‰¹è‰²ï¼šæµç•…è¿‡æ¸¡ + æ•°å­¦ç²¾ç¡®åŒæ­¥")
    print("ğŸ¯ ä¸‰é˜¶æ®µï¼šåŒæ­¥æœŸ(30%) â†’ å¼•å¯¼æœŸ(40%) â†’ å·©å›ºæœŸ(30%)")
    print("âœ¨ è¿è´¯ç–—æ„ˆå™äº‹ï¼ŒçœŸæ­£çš„æƒ…ç»ªè½¬æ¢ä½“éªŒ")
    print("=" * 60)
    
    # æ¼”ç¤ºåœºæ™¯
    demo_scenarios = [
        {
            "user_input": "æˆ‘æ„Ÿåˆ°å¾ˆç„¦è™‘ï¼Œå¿ƒè·³åŠ é€Ÿï¼Œéš¾ä»¥å…¥ç¡",
            "duration": 20,
            "description": "ç„¦è™‘æƒ…ç»ªåœºæ™¯"
        },
        {
            "user_input": "æˆ‘å¾ˆç–²æƒ«ï¼Œä½†å¤§è„‘è¿˜åœ¨æ´»è·ƒï¼Œæ— æ³•æ”¾æ¾",
            "duration": 15,
            "description": "ç–²æƒ«æƒ…ç»ªåœºæ™¯"
        },
        {
            "user_input": "æˆ‘æ„Ÿåˆ°çƒ¦èºä¸å®‰ï¼Œå®¹æ˜“è¢«å°äº‹å½±å“",
            "duration": 18,
            "description": "çƒ¦èºæƒ…ç»ªåœºæ™¯"
        },
        {
            "user_input": "æœ€è¿‘å‹åŠ›å¾ˆå¤§ï¼Œæ€»æ˜¯æ„Ÿåˆ°ç´§å¼ ",
            "duration": 25,
            "description": "å‹åŠ›æƒ…ç»ªåœºæ™¯"
        },
        {
            "user_input": "æˆ‘æ¯”è¾ƒå¹³é™ï¼Œä½†å¸Œæœ›æ›´æ·±å±‚çš„æ”¾æ¾",
            "duration": 12,
            "description": "å¹³é™æƒ…ç»ªåœºæ™¯"
        }
    ]
    
    output_dir = Path("outputs")
    output_dir.mkdir(exist_ok=True)
    
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    
    for i, scenario in enumerate(demo_scenarios, 1):
        print(f"\n{'='*60}")
        print(f"ğŸ¬ æ¼”ç¤ºåœºæ™¯ {i}/5: {scenario['description']}")
        print(f"ğŸ’­ ç”¨æˆ·è¾“å…¥: {scenario['user_input']}")
        print(f"â±ï¸ ç–—æ„ˆæ—¶é•¿: {scenario['duration']}ç§’")
        print(f"{'='*60}")
        
        start_time = time.time()
        
        # æƒ…ç»ªè¯†åˆ«
        print(f"\nğŸ§  å¼€å§‹æƒ…ç»ªåˆ†æ...")
        detected_emotion, confidence = simple_emotion_detection(scenario['user_input'])
        print(f"ğŸ¯ æ£€æµ‹åˆ°æƒ…ç»ª: {detected_emotion} (ç½®ä¿¡åº¦: {confidence:.1%})")
        
        # ç”Ÿæˆå¢å¼ºéŸ³é¢‘
        print(f"\nğŸµ ç”Ÿæˆå¢å¼ºä¸‰é˜¶æ®µç–—æ„ˆéŸ³é¢‘...")
        audio_array, sample_rate, params = generate_enhanced_therapy_audio(
            duration=scenario['duration'], 
            emotion=detected_emotion
        )
        
        # ä¿å­˜éŸ³é¢‘
        audio_path = output_dir / f"enhanced_therapy_{detected_emotion}_{i}_{timestamp}.wav"
        print(f"\nğŸ’¾ ä¿å­˜éŸ³é¢‘æ–‡ä»¶...")
        success = save_audio_file(audio_array, sample_rate, str(audio_path))
        
        processing_time = time.time() - start_time
        
        # è¾“å‡ºè¯¦ç»†ä¿¡æ¯
        print(f"\nâœ… åœºæ™¯ {i} å¢å¼ºä¸‰é˜¶æ®µç–—æ„ˆéŸ³é¢‘ç”Ÿæˆå®Œæˆï¼")
        print(f"""
ğŸ§  æƒ…ç»ªè¯†åˆ«ç»“æœ:
   æƒ…ç»ªç±»å‹: {detected_emotion}
   ç½®ä¿¡åº¦: {confidence:.1%}
   å¤„ç†æ—¶é—´: {processing_time:.1f}ç§’

ğŸµ éŸ³é¢‘æŠ€æœ¯è¯¦æƒ…:
   æ€»æ—¶é•¿: {scenario['duration']}ç§’
   é‡‡æ ·ç‡: {sample_rate}Hz (CDçº§åˆ«)
   å£°é“: ç«‹ä½“å£° + ç©ºé—´æ··å“
   é’ˆå¯¹æƒ…ç»ª: {detected_emotion}

ğŸŒŠ ä¸‰é˜¶æ®µæµç•…è¿‡æ¸¡è®¾è®¡:
   åŒæ­¥æœŸ ({scenario['duration']*0.3:.1f}s): {params['sync_freq']}Hz, åŒ¹é…{detected_emotion}æƒ…ç»ª
   å¼•å¯¼æœŸ ({scenario['duration']*0.4:.1f}s): {params['sync_freq']}â†’{params['guide_freq']}Hz, æµç•…è¿‡æ¸¡
   å·©å›ºæœŸ ({scenario['duration']*0.3:.1f}s): {params['consolidate_freq']}Hz, æ·±åº¦æ”¾æ¾

ğŸ¼ ç–—æ„ˆæŠ€æœ¯ç‰¹è‰²:
   è¿‡æ¸¡ç±»å‹: {params['transition_type']} (ä¸ªæ€§åŒ–)
   å’Œè°æ³›éŸ³: å¢å¼ºç–—æ„ˆæ•ˆæœ
   è‡ªç„¶éŸ³æ•ˆ: æµ·æµªå£° + ç¯å¢ƒéŸ³
   ç«‹ä½“å£°åœº: 10mså»¶è¿Ÿ + ç©ºé—´æ··å“
   æ·¡å…¥æ·¡å‡º: 0.5ç§’å¹³æ»‘è¿‡æ¸¡

ğŸ“ è¾“å‡ºæ–‡ä»¶:
   éŸ³é¢‘æ–‡ä»¶: {audio_path}
   æ–‡ä»¶æ ¼å¼: {'WAV (æ ‡å‡†)' if success else 'NumPyæ•°ç»„'}
""")
        
        # çŸ­æš‚æš‚åœ
        time.sleep(1)
    
    print(f"\n{'='*60}")
    print("ğŸ‰ æ‰€æœ‰æ¼”ç¤ºåœºæ™¯å®Œæˆï¼")
    print(f"""
ğŸ“Š æ¼”ç¤ºæ€»ç»“:
   åœºæ™¯æ•°é‡: {len(demo_scenarios)}ä¸ª
   æƒ…ç»ªç±»å‹: ç„¦è™‘ã€ç–²æƒ«ã€çƒ¦èºã€å‹åŠ›ã€å¹³é™
   æ€»éŸ³é¢‘æ—¶é•¿: {sum(s['duration'] for s in demo_scenarios)}ç§’
   è¾“å‡ºç›®å½•: {output_dir}

ğŸ§ ç–—æ„ˆä½¿ç”¨å»ºè®®:
   - ä½©æˆ´è€³æœºä½“éªŒç«‹ä½“å£°åœº
   - åœ¨å®‰é™ç¯å¢ƒä¸­è†å¬
   - è·ŸéšéŸ³é¢‘èŠ‚å¥è°ƒæ•´å‘¼å¸
   - ä¸“æ³¨æ„Ÿå—ä¸‰é˜¶æ®µæƒ…ç»ªè½¬æ¢

ğŸ“ æŠ€æœ¯åˆ›æ–°äº®ç‚¹:
   - æµç•…è¿‡æ¸¡: æ— æ˜æ˜¾åœé¡¿çš„ä¸‰é˜¶æ®µåˆ‡æ¢
   - æ•°å­¦ç²¾ç¡®: åŸºäºæ•°å­¦å‡½æ•°çš„å¹³æ»‘è¿‡æ¸¡æ›²çº¿
   - æƒ…ç»ªæ˜ å°„: æ¯ç§æƒ…ç»ªçš„ä¸“å±å‚æ•°è®¾è®¡
   - ç–—æ„ˆå™äº‹: è¿è´¯çš„æƒ…ç»ªè½¬æ¢æ•…äº‹
   - ä¸ªæ€§åŒ–: é’ˆå¯¹ä¸åŒæƒ…ç»ªçš„ç‹¬ç‰¹ç®—æ³•
   - ç«‹ä½“å£°åœº: ä¸“ä¸šçº§éŸ³é¢‘ç©ºé—´åŒ–å¤„ç†

ğŸŒ™ å¢å¼ºä¸‰é˜¶æ®µç–—æ„ˆç³»ç»Ÿæ¼”ç¤ºæˆåŠŸå®Œæˆï¼
   æ„¿æ¯ä¸ªäººéƒ½èƒ½æ‰¾åˆ°å†…å¿ƒçš„å¹³é™ä¸å®‰å®...
""")

def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1 and sys.argv[1] == "--demo":
        run_demo_scenarios()
    else:
        # å•ä¸€åœºæ™¯æ¼”ç¤º
        user_input = "æˆ‘æ„Ÿåˆ°å¾ˆç„¦è™‘ï¼Œå¿ƒè·³åŠ é€Ÿï¼Œéš¾ä»¥å…¥ç¡"
        duration = 20
        
        print("ğŸš€ å¯åŠ¨å¢å¼ºä¸‰é˜¶æ®µç–—æ„ˆç³»ç»Ÿ...")
        print("ğŸŒŠ ç‰¹è‰²ï¼šæµç•…è¿‡æ¸¡ + æ•°å­¦ç²¾ç¡®åŒæ­¥")
        print("ğŸ¯ ä¸‰é˜¶æ®µï¼šåŒæ­¥æœŸ(30%) â†’ å¼•å¯¼æœŸ(40%) â†’ å·©å›ºæœŸ(30%)")
        print("âœ¨ è¿è´¯ç–—æ„ˆå™äº‹ï¼ŒçœŸæ­£çš„æƒ…ç»ªè½¬æ¢ä½“éªŒ")
        print("=" * 60)
        
        print(f"\nğŸ’­ ç”¨æˆ·è¾“å…¥: {user_input}")
        print(f"â±ï¸ ç–—æ„ˆæ—¶é•¿: {duration}ç§’")
        
        start_time = time.time()
        
        # æƒ…ç»ªè¯†åˆ«
        print(f"\nğŸ§  å¼€å§‹æƒ…ç»ªåˆ†æ...")
        detected_emotion, confidence = simple_emotion_detection(user_input)
        print(f"ğŸ¯ æ£€æµ‹åˆ°æƒ…ç»ª: {detected_emotion} (ç½®ä¿¡åº¦: {confidence:.1%})")
        
        # ç”Ÿæˆå¢å¼ºéŸ³é¢‘
        print(f"\nğŸµ ç”Ÿæˆå¢å¼ºä¸‰é˜¶æ®µç–—æ„ˆéŸ³é¢‘...")
        audio_array, sample_rate, params = generate_enhanced_therapy_audio(
            duration=duration, 
            emotion=detected_emotion
        )
        
        # ä¿å­˜éŸ³é¢‘
        output_dir = Path("outputs")
        output_dir.mkdir(exist_ok=True)
        
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        audio_path = output_dir / f"enhanced_therapy_{detected_emotion}_{timestamp}.wav"
        
        print(f"\nğŸ’¾ ä¿å­˜éŸ³é¢‘æ–‡ä»¶...")
        success = save_audio_file(audio_array, sample_rate, str(audio_path))
        
        processing_time = time.time() - start_time
        
        # è¾“å‡ºè¯¦ç»†ä¿¡æ¯
        print("\n" + "=" * 60)
        print("âœ… å¢å¼ºä¸‰é˜¶æ®µç–—æ„ˆéŸ³é¢‘ç”Ÿæˆå®Œæˆï¼")
        print(f"""
ğŸ§  æƒ…ç»ªè¯†åˆ«ç»“æœ:
   æƒ…ç»ªç±»å‹: {detected_emotion}
   ç½®ä¿¡åº¦: {confidence:.1%}
   å¤„ç†æ—¶é—´: {processing_time:.1f}ç§’

ğŸµ éŸ³é¢‘æŠ€æœ¯è¯¦æƒ…:
   æ€»æ—¶é•¿: {duration}ç§’
   é‡‡æ ·ç‡: {sample_rate}Hz (CDçº§åˆ«)
   å£°é“: ç«‹ä½“å£° + ç©ºé—´æ··å“
   é’ˆå¯¹æƒ…ç»ª: {detected_emotion}

ğŸŒŠ ä¸‰é˜¶æ®µæµç•…è¿‡æ¸¡è®¾è®¡:
   åŒæ­¥æœŸ ({duration*0.3:.1f}s): {params['sync_freq']}Hz, åŒ¹é…{detected_emotion}æƒ…ç»ª
   å¼•å¯¼æœŸ ({duration*0.4:.1f}s): {params['sync_freq']}â†’{params['guide_freq']}Hz, æµç•…è¿‡æ¸¡
   å·©å›ºæœŸ ({duration*0.3:.1f}s): {params['consolidate_freq']}Hz, æ·±åº¦æ”¾æ¾

ğŸ¼ ç–—æ„ˆæŠ€æœ¯ç‰¹è‰²:
   è¿‡æ¸¡ç±»å‹: {params['transition_type']} (ä¸ªæ€§åŒ–)
   å’Œè°æ³›éŸ³: å¢å¼ºç–—æ„ˆæ•ˆæœ
   è‡ªç„¶éŸ³æ•ˆ: æµ·æµªå£° + ç¯å¢ƒéŸ³
   ç«‹ä½“å£°åœº: 10mså»¶è¿Ÿ + ç©ºé—´æ··å“
   æ·¡å…¥æ·¡å‡º: 0.5ç§’å¹³æ»‘è¿‡æ¸¡

ğŸ“ è¾“å‡ºæ–‡ä»¶:
   éŸ³é¢‘æ–‡ä»¶: {audio_path}
   æ–‡ä»¶æ ¼å¼: {'WAV (æ ‡å‡†)' if success else 'NumPyæ•°ç»„'}

ğŸ§ ç–—æ„ˆä½¿ç”¨å»ºè®®:
   - ä½©æˆ´è€³æœºä½“éªŒç«‹ä½“å£°åœº
   - åœ¨å®‰é™ç¯å¢ƒä¸­è†å¬
   - è·ŸéšéŸ³é¢‘èŠ‚å¥è°ƒæ•´å‘¼å¸
   - ä¸“æ³¨æ„Ÿå—ä¸‰é˜¶æ®µæƒ…ç»ªè½¬æ¢

ğŸ“ æŠ€æœ¯åˆ›æ–°:
   - æµç•…è¿‡æ¸¡: æ— æ˜æ˜¾åœé¡¿çš„ä¸‰é˜¶æ®µåˆ‡æ¢
   - æ•°å­¦ç²¾ç¡®: åŸºäºæ•°å­¦å‡½æ•°çš„å¹³æ»‘è¿‡æ¸¡æ›²çº¿
   - æƒ…ç»ªæ˜ å°„: {detected_emotion}æƒ…ç»ªä¸“å±å‚æ•°
   - ç–—æ„ˆå™äº‹: è¿è´¯çš„æƒ…ç»ªè½¬æ¢æ•…äº‹
   - ä¸ªæ€§åŒ–: é’ˆå¯¹ä¸åŒæƒ…ç»ªçš„ç‹¬ç‰¹è®¾è®¡
""")
        
        print("\nğŸŒ™ æ„¿æ‚¨è·å¾—å†…å¿ƒçš„å¹³é™ä¸å®‰å®...")

if __name__ == "__main__":
    main()