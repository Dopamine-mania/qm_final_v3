#!/usr/bin/env python3
"""
ğŸŒ™ ç¡çœ ç–—æ„ˆAI - æ¼”ç¤ºæ¨¡å¼
ä¸éœ€è¦çœŸå®APIï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®å±•ç¤ºå®Œæ•´åŠŸèƒ½
"""

import gradio as gr
import numpy as np
import cv2
import os
import tempfile
import time
from pathlib import Path
from datetime import datetime
import json

# ç”Ÿæˆæ¨¡æ‹ŸéŸ³é¢‘æ•°æ®
def generate_mock_audio(duration=15, sample_rate=44100):
    """ç”Ÿæˆæ¨¡æ‹Ÿçš„ä¸‰é˜¶æ®µéŸ³é¢‘æ•°æ®"""
    print(f"ğŸµ ç”Ÿæˆæ¨¡æ‹ŸéŸ³é¢‘: {duration}ç§’, é‡‡æ ·ç‡: {sample_rate}Hz")
    
    # åˆ›å»ºä¸‰é˜¶æ®µéŸ³é¢‘
    stage_duration = duration // 3
    t = np.linspace(0, stage_duration, int(sample_rate * stage_duration))
    
    # ç¬¬ä¸€é˜¶æ®µï¼šåŒæ­¥æœŸ - è¾ƒé«˜é¢‘ç‡ï¼ŒåŒ¹é…ç”¨æˆ·æƒ…ç»ª
    stage1_freq = 440  # A4
    stage1 = 0.3 * np.sin(2 * np.pi * stage1_freq * t) * np.exp(-t/5)
    
    # ç¬¬äºŒé˜¶æ®µï¼šå¼•å¯¼æœŸ - é€æ¸é™ä½é¢‘ç‡
    stage2_freq = 330  # E4
    stage2 = 0.2 * np.sin(2 * np.pi * stage2_freq * t) * np.exp(-t/8)
    
    # ç¬¬ä¸‰é˜¶æ®µï¼šå·©å›ºæœŸ - ä½é¢‘æ”¾æ¾éŸ³
    stage3_freq = 220  # A3
    stage3 = 0.1 * np.sin(2 * np.pi * stage3_freq * t) * np.exp(-t/12)
    
    # åˆå¹¶ä¸‰é˜¶æ®µ
    audio_array = np.concatenate([stage1, stage2, stage3])
    
    # æ·»åŠ ç™½å™ªå£°å’Œç¯å¢ƒéŸ³
    noise = 0.05 * np.random.normal(0, 1, len(audio_array))
    audio_array = audio_array + noise
    
    # å½’ä¸€åŒ–
    audio_array = audio_array / np.max(np.abs(audio_array))
    
    return audio_array.astype(np.float32), sample_rate

# ç”Ÿæˆæ¨¡æ‹Ÿè§†é¢‘å¸§
def generate_mock_video_frames(duration=15, fps=30):
    """ç”Ÿæˆæ¨¡æ‹Ÿçš„ç–—æ„ˆè§†é¢‘å¸§"""
    print(f"ğŸ¬ ç”Ÿæˆæ¨¡æ‹Ÿè§†é¢‘: {duration}ç§’, {fps}fps")
    
    frame_count = int(duration * fps)
    frames = []
    
    width, height = 640, 480
    
    for i in range(frame_count):
        # åˆ›å»ºæ¸å˜èƒŒæ™¯
        frame = np.zeros((height, width, 3), dtype=np.uint8)
        
        # æ—¶é—´è¿›åº¦
        progress = i / frame_count
        
        # é¢œè‰²å˜åŒ–ï¼šä»è“è‰²åˆ°ç´«è‰²åˆ°æ·±è“
        if progress < 0.33:  # ç¬¬ä¸€é˜¶æ®µï¼šåŒæ­¥æœŸ
            blue = int(255 * (1 - progress * 3))
            green = int(100 * progress * 3)
            red = int(50 * progress * 3)
        elif progress < 0.66:  # ç¬¬äºŒé˜¶æ®µï¼šå¼•å¯¼æœŸ
            stage_progress = (progress - 0.33) / 0.33
            blue = int(100 + 155 * (1 - stage_progress))
            green = int(100 * (1 - stage_progress))
            red = int(50 + 100 * stage_progress)
        else:  # ç¬¬ä¸‰é˜¶æ®µï¼šå·©å›ºæœŸ
            stage_progress = (progress - 0.66) / 0.34
            blue = int(50 + 50 * (1 - stage_progress))
            green = int(20 * (1 - stage_progress))
            red = int(20 + 30 * (1 - stage_progress))
        
        # å¡«å……æ¸å˜èƒŒæ™¯
        for y in range(height):
            for x in range(width):
                # å¾„å‘æ¸å˜
                center_x, center_y = width // 2, height // 2
                distance = np.sqrt((x - center_x)**2 + (y - center_y)**2)
                max_distance = np.sqrt(center_x**2 + center_y**2)
                
                gradient = 1 - (distance / max_distance)
                
                frame[y, x] = [
                    int(blue * gradient),
                    int(green * gradient),
                    int(red * gradient)
                ]
        
        # æ·»åŠ å‘¼å¸æ•ˆæœåœ†åœˆ
        breathing_radius = 50 + 30 * np.sin(progress * 4 * np.pi)
        cv2.circle(frame, (width//2, height//2), int(breathing_radius), (255, 255, 255), 2)
        
        # æ·»åŠ æ–‡å­—æŒ‡ç¤º
        if progress < 0.33:
            stage_text = "åŒæ­¥æœŸ - åŒ¹é…æƒ…ç»ª"
        elif progress < 0.66:
            stage_text = "å¼•å¯¼æœŸ - æƒ…ç»ªè½¬æ¢"
        else:
            stage_text = "å·©å›ºæœŸ - æ·±åº¦æ”¾æ¾"
        
        cv2.putText(frame, stage_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
        
        frames.append(frame)
    
    return frames, fps

# åˆ›å»ºéŸ³ç”»åŒæ­¥è§†é¢‘
def create_demo_video(audio_array, sample_rate, video_frames, fps):
    """åˆ›å»ºæ¼”ç¤ºç”¨çš„éŸ³ç”»åŒæ­¥è§†é¢‘"""
    try:
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        temp_dir = tempfile.mkdtemp()
        audio_path = os.path.join(temp_dir, "demo_audio.wav")
        video_path = os.path.join(temp_dir, "demo_video.mp4")
        output_path = os.path.join(temp_dir, "demo_synchronized.mp4")
        
        # ä¿å­˜éŸ³é¢‘
        import soundfile as sf
        sf.write(audio_path, audio_array, sample_rate)
        print(f"âœ… æ¼”ç¤ºéŸ³é¢‘ä¿å­˜åˆ°: {audio_path}")
        
        # åˆ›å»ºè§†é¢‘
        if len(video_frames) > 0:
            frame_height, frame_width = video_frames[0].shape[:2]
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(video_path, fourcc, fps, (frame_width, frame_height))
            
            for frame in video_frames:
                out.write(frame)
            
            out.release()
            print(f"âœ… æ¼”ç¤ºè§†é¢‘ä¿å­˜åˆ°: {video_path}")
            
            # ä½¿ç”¨ffmpegåˆæˆï¼ˆå¦‚æœå¯ç”¨ï¼‰
            try:
                import subprocess
                ffmpeg_cmd = [
                    'ffmpeg', '-y',
                    '-i', video_path,
                    '-i', audio_path,
                    '-c:v', 'libx264',
                    '-c:a', 'aac',
                    '-strict', 'experimental',
                    '-shortest',
                    output_path
                ]
                
                result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True)
                if result.returncode == 0:
                    print(f"âœ… æ¼”ç¤ºéŸ³ç”»åŒæ­¥è§†é¢‘ç”ŸæˆæˆåŠŸ: {output_path}")
                    return output_path
                else:
                    print(f"âš ï¸ ffmpegä¸å¯ç”¨ï¼Œè¿”å›çº¯è§†é¢‘æ–‡ä»¶")
                    return video_path
            except:
                print(f"âš ï¸ ffmpegä¸å¯ç”¨ï¼Œè¿”å›çº¯è§†é¢‘æ–‡ä»¶")
                return video_path
        else:
            print("âŒ æ²¡æœ‰è§†é¢‘å¸§")
            return None
            
    except Exception as e:
        print(f"âŒ åˆ›å»ºæ¼”ç¤ºè§†é¢‘æ—¶å‡ºé”™: {e}")
        return None

# æ¼”ç¤ºæ¨¡å¼çš„æƒ…ç»ªå¤„ç†
def demo_emotion_processing(user_input):
    """æ¼”ç¤ºæ¨¡å¼çš„æƒ…ç»ªå¤„ç†"""
    if not user_input or len(user_input.strip()) < 5:
        return "âš ï¸ è¯·è¾“å…¥è‡³å°‘5ä¸ªå­—ç¬¦çš„æƒ…ç»ªæè¿°", None, None, "è¾“å…¥å¤ªçŸ­"
    
    print(f"ğŸ”„ æ¼”ç¤ºæ¨¡å¼å¤„ç†æƒ…ç»ªè¾“å…¥: {user_input}")
    
    # æ¨¡æ‹Ÿæƒ…ç»ªè¯†åˆ«
    emotions = ["ç„¦è™‘", "ç–²æƒ«", "çƒ¦èº", "å¹³é™", "å‹åŠ›"]
    detected_emotion = "ç„¦è™‘"  # ç®€å•èµ·è§ï¼Œå›ºå®šè¯†åˆ«ä¸ºç„¦è™‘
    
    if "ç„¦è™‘" in user_input or "ç´§å¼ " in user_input:
        detected_emotion = "ç„¦è™‘"
    elif "ç´¯" in user_input or "ç–²" in user_input:
        detected_emotion = "ç–²æƒ«"
    elif "çƒ¦" in user_input or "èº" in user_input:
        detected_emotion = "çƒ¦èº"
    elif "å¹³é™" in user_input or "è¿˜å¥½" in user_input:
        detected_emotion = "å¹³é™"
    elif "å‹åŠ›" in user_input or "å¿™" in user_input:
        detected_emotion = "å‹åŠ›"
    
    # æ¨¡æ‹Ÿç½®ä¿¡åº¦
    confidence = 0.85
    
    # ç”Ÿæˆæ¨¡æ‹ŸéŸ³é¢‘å’Œè§†é¢‘
    print("ğŸµ ç”Ÿæˆä¸‰é˜¶æ®µæ²»ç–—éŸ³é¢‘...")
    audio_array, sample_rate = generate_mock_audio(duration=15)
    
    print("ğŸ¬ ç”ŸæˆåŒæ­¥è§†è§‰å†…å®¹...")
    video_frames, fps = generate_mock_video_frames(duration=15, fps=30)
    
    # åˆ›å»ºéŸ³ç”»åŒæ­¥è§†é¢‘
    print("ğŸ¬ åˆæˆéŸ³ç”»åŒæ­¥è§†é¢‘...")
    video_path = create_demo_video(audio_array, sample_rate, video_frames, fps)
    
    # ç»„ç»‡è¿”å›ä¿¡æ¯
    emotion_info = f"ğŸ§  è¯†åˆ«æƒ…ç»ª: {detected_emotion}\nç½®ä¿¡åº¦: {confidence:.1%}\næƒ…ç»ªç±»å‹: ç¡å‰{detected_emotion}çŠ¶æ€"
    
    if video_path:
        info_text = f"""âœ… æ¼”ç¤ºæ¨¡å¼éŸ³ç”»åŒæ­¥è§†é¢‘ç”ŸæˆæˆåŠŸï¼

ğŸµ éŸ³é¢‘ä¿¡æ¯:
  - æ—¶é•¿: 15ç§’
  - é‡‡æ ·ç‡: {sample_rate}Hz
  - ä¸‰é˜¶æ®µè®¾è®¡: åŒæ­¥â†’å¼•å¯¼â†’å·©å›º

ğŸ¬ è§†é¢‘ä¿¡æ¯:
  - å¸§æ•°: {len(video_frames)}å¸§
  - å¸§ç‡: {fps}fps
  - åˆ†è¾¨ç‡: 640x480

ğŸ“ æ³¨æ„: è¿™æ˜¯æ¼”ç¤ºæ¨¡å¼ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
çœŸå®ç‰ˆæœ¬éœ€è¦é…ç½®Suno APIå’Œå…¶ä»–å•†ä¸šAPI"""
        
        return emotion_info, video_path, (sample_rate, audio_array), info_text
    else:
        return emotion_info, None, (sample_rate, audio_array), "âŒ è§†é¢‘ç”Ÿæˆå¤±è´¥"

def create_demo_interface():
    """åˆ›å»ºæ¼”ç¤ºç•Œé¢"""
    with gr.Blocks(title="ğŸŒ™ ç¡çœ ç–—æ„ˆAI - æ¼”ç¤ºæ¨¡å¼") as app:
        gr.HTML("<h1 style='text-align: center;'>ğŸŒ™ ç¡çœ ç–—æ„ˆAI - æ¼”ç¤ºæ¨¡å¼</h1>")
        gr.HTML("<p style='text-align: center; color: orange;'>âš ï¸ æ¼”ç¤ºæ¨¡å¼ï¼šä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼Œä¸éœ€è¦çœŸå®API</p>")
        
        with gr.Row():
            with gr.Column():
                emotion_input = gr.Textbox(
                    label="ğŸ’­ æè¿°æ‚¨çš„æ„Ÿå—",
                    placeholder="ä¾‹å¦‚ï¼šæˆ‘æ„Ÿåˆ°å¾ˆç„¦è™‘ï¼Œå¿ƒè·³åŠ é€Ÿï¼Œéš¾ä»¥å…¥ç¡...",
                    lines=4
                )
                
                process_btn = gr.Button("ğŸ¬ æ¼”ç¤ºï¼šç”ŸæˆéŸ³ç”»åŒæ­¥ç–—æ„ˆè§†é¢‘", variant="primary")
            
            with gr.Column():
                emotion_result = gr.Textbox(label="ğŸ§  æƒ…ç»ªè¯†åˆ«ç»“æœ", lines=3)
                
                # éŸ³ç”»åŒæ­¥è§†é¢‘è¾“å‡º
                video_output = gr.Video(label="ğŸ¬ æ¼”ç¤ºï¼šéŸ³ç”»åŒæ­¥ç–—æ„ˆè§†é¢‘")
                
                # éŸ³é¢‘è¾“å‡º
                audio_output = gr.Audio(label="ğŸµ æ¼”ç¤ºï¼šä¸‰é˜¶æ®µç–—æ„ˆéŸ³é¢‘")
                
                info_output = gr.Textbox(label="ğŸ“Š æ¼”ç¤ºä¿¡æ¯", lines=8)
        
        # ä½¿ç”¨è¯´æ˜
        gr.HTML("""
        <div style="margin-top: 20px; padding: 15px; background-color: #f0f0f0; border-radius: 10px;">
            <h3>ğŸ¯ æ¼”ç¤ºæ¨¡å¼è¯´æ˜</h3>
            <p><strong>ğŸµ éŸ³é¢‘ç”Ÿæˆï¼š</strong> æ¨¡æ‹Ÿä¸‰é˜¶æ®µéŸ³ä¹å™äº‹ (åŒæ­¥â†’å¼•å¯¼â†’å·©å›º)</p>
            <p><strong>ğŸ¬ è§†é¢‘ç”Ÿæˆï¼š</strong> æ¨¡æ‹Ÿç–—æ„ˆè§†è§‰å†…å®¹ï¼Œé¢œè‰²æ¸å˜+å‘¼å¸æ•ˆæœ</p>
            <p><strong>ğŸ§  æƒ…ç»ªè¯†åˆ«ï¼š</strong> åŸºäºå…³é”®è¯çš„ç®€å•æƒ…ç»ªåˆ†ç±»</p>
            <p><strong>âš ï¸ æ³¨æ„ï¼š</strong> çœŸå®ç‰ˆæœ¬éœ€è¦é…ç½®Suno APIç­‰å•†ä¸šæœåŠ¡</p>
        </div>
        """)
        
        # ç»‘å®šäº‹ä»¶
        process_btn.click(
            demo_emotion_processing,
            inputs=emotion_input,
            outputs=[emotion_result, video_output, audio_output, info_output]
        )
    
    return app

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨æ¼”ç¤ºæ¨¡å¼...")
    
    app = create_demo_interface()
    app.launch(
        server_name="0.0.0.0",
        server_port=7863,
        share=True,
        debug=True
    )

if __name__ == "__main__":
    main()