#!/usr/bin/env python3
"""
ğŸŒ™ ç¡çœ ç–—æ„ˆAI - Gradioç‰ˆæœ¬
åŸºäºä¸‰é˜¶æ®µéŸ³ä¹å™äº‹çš„ç¡å‰æƒ…ç»ªç–—æ„ˆWebåº”ç”¨

ç”¨æ³•: python gradio_app.py
"""

import gradio as gr
import asyncio
import sys
import time
import json
import numpy as np
from pathlib import Path
from datetime import datetime
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from main import QMFinal3System
from layers.base_layer import LayerData

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å…¨å±€ç³»ç»Ÿå®ä¾‹
system = None

def init_system():
    """åˆå§‹åŒ–ç³»ç»Ÿ"""
    global system
    if system is None:
        try:
            system = QMFinal3System()
            return "âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼"
        except Exception as e:
            return f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}"
    return "âœ… ç³»ç»Ÿå·²å°±ç»ª"

async def process_emotion_input(user_input: str):
    """å¤„ç†ç”¨æˆ·æƒ…ç»ªè¾“å…¥"""
    global system
    
    if not system:
        return "âŒ ç³»ç»Ÿæœªåˆå§‹åŒ–", None, "è¯·å…ˆåˆå§‹åŒ–ç³»ç»Ÿ"
    
    if not user_input or len(user_input.strip()) < 5:
        return "âš ï¸ è¯·è¾“å…¥è‡³å°‘5ä¸ªå­—ç¬¦çš„æƒ…ç»ªæè¿°", None, ""
    
    try:
        # åˆ›å»ºè¾“å…¥æ•°æ®
        input_data = LayerData(
            layer_name="gradio_interface",
            timestamp=datetime.now(),
            data={"test_input": user_input},
            metadata={"source": "gradio_app", "user_input": user_input}
        )
        
        # æ·»åŠ æ–‡æœ¬è¾“å…¥åˆ°è¾“å…¥å±‚
        if system.layers:
            input_layer = system.layers[0]
            if hasattr(input_layer, 'add_text_input'):
                input_layer.add_text_input(user_input)
        
        # é€šè¿‡ç®¡é“å¤„ç†
        result = await system.pipeline.process(input_data)
        
        # æå–æƒ…ç»ªè¯†åˆ«ç»“æœ
        emotion_info = "æœªçŸ¥æƒ…ç»ª"
        confidence = 0.0
        audio_info = "æš‚æ— éŸ³é¢‘ç”Ÿæˆ"
        
        # ä»ç®¡é“å†å²ä¸­è·å–æƒ…ç»ªä¿¡æ¯
        if hasattr(system.pipeline, 'layer_results'):
            for layer_result in system.pipeline.layer_results:
                if (hasattr(layer_result, 'data') and 
                    'emotion_analysis' in layer_result.data):
                    analysis = layer_result.data['emotion_analysis']
                    primary_emotion = analysis.get('primary_emotion', {})
                    emotion_info = f"ä¸»è¦æƒ…ç»ª: {primary_emotion.get('name', 'æœªçŸ¥')}"
                    confidence = layer_result.confidence
                    break
        
        # æŸ¥æ‰¾ç”Ÿæˆçš„éŸ³é¢‘å†…å®¹
        if hasattr(system.pipeline, 'layer_results'):
            for layer_result in system.pipeline.layer_results:
                if (hasattr(layer_result, 'data') and 
                    'generated_content' in layer_result.data):
                    generated_content = layer_result.data['generated_content']
                    audio_content = generated_content.get('audio', {})
                    if audio_content and 'audio_array' in audio_content:
                        duration = audio_content.get('duration', 0)
                        sample_rate = audio_content.get('sample_rate', 44100)
                        three_stage = audio_content.get('three_stage_narrative', False)
                        audio_info = f"âœ… ç”ŸæˆæˆåŠŸ!\næ—¶é•¿: {duration:.0f}ç§’\né‡‡æ ·ç‡: {sample_rate}Hz\nä¸‰é˜¶æ®µå™äº‹: {'æ˜¯' if three_stage else 'å¦'}"
                        
                        # è¿”å›éŸ³é¢‘æ•°ç»„ç”¨äºæ’­æ”¾
                        audio_array = audio_content.get('audio_array')
                        if audio_array is not None and isinstance(audio_array, np.ndarray):
                            if audio_array.dtype != np.float32:
                                audio_array = audio_array.astype(np.float32)
                            if np.max(np.abs(audio_array)) > 0:
                                audio_array = audio_array / np.max(np.abs(audio_array))
                            return f"{emotion_info}\nç½®ä¿¡åº¦: {confidence:.1%}", (sample_rate, audio_array), audio_info
                    break
        
        return f"{emotion_info}\nç½®ä¿¡åº¦: {confidence:.1%}", None, audio_info
        
    except Exception as e:
        logger.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        return f"âŒ å¤„ç†å¤±è´¥: {e}", None, "å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯"

def gradio_process_emotion(user_input):
    """GradioåŒæ­¥åŒ…è£…å‡½æ•°"""
    global system
    
    if not user_input or len(user_input.strip()) < 5:
        return "âš ï¸ è¯·è¾“å…¥è‡³å°‘5ä¸ªå­—ç¬¦çš„æƒ…ç»ªæè¿°", None, None, "è¾“å…¥å¤ªçŸ­"
    
    if not system:
        return "âŒ è¯·å…ˆç‚¹å‡»'åˆå§‹åŒ–ç³»ç»Ÿ'æŒ‰é’®", None, None, "ç³»ç»Ÿæœªåˆå§‹åŒ–"
    
    try:
        # åŒæ­¥æ–¹å¼å¤„ç†ï¼Œé¿å…asyncioé—®é¢˜
        from layers.base_layer import LayerData
        from datetime import datetime
        
        # åˆ›å»ºè¾“å…¥æ•°æ®
        input_data = LayerData(
            layer_name="gradio_interface",
            timestamp=datetime.now(),
            data={"test_input": user_input},
            metadata={"source": "gradio_app", "user_input": user_input}
        )
        
        # æ·»åŠ æ–‡æœ¬è¾“å…¥åˆ°è¾“å…¥å±‚
        if system.layers:
            input_layer = system.layers[0]
            if hasattr(input_layer, 'add_text_input'):
                input_layer.add_text_input(user_input)
        
        # ä½¿ç”¨çœŸæ­£çš„åç«¯ç³»ç»Ÿå¤„ç†
        print(f"ğŸ”„ å¼€å§‹å¤„ç†æƒ…ç»ªè¾“å…¥: {user_input[:50]}...")
        
        # é€šè¿‡çœŸæ­£çš„ç³»ç»Ÿç®¡é“å¤„ç†
        import asyncio
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        try:
            result = loop.run_until_complete(system.pipeline.process(input_data))
            
            # æå–æƒ…ç»ªè¯†åˆ«ç»“æœ
            emotion_info = "ğŸ§  æƒ…ç»ªè¯†åˆ«ä¸­..."
            confidence = 0.0
            audio_array = None
            sample_rate = 44100
            
            # ä»ç®¡é“å†å²ä¸­è·å–æƒ…ç»ªä¿¡æ¯
            if hasattr(system.pipeline, 'layer_results'):
                for layer_result in system.pipeline.layer_results:
                    if (hasattr(layer_result, 'data') and 
                        'emotion_analysis' in layer_result.data):
                        analysis = layer_result.data['emotion_analysis']
                        primary_emotion = analysis.get('primary_emotion', {})
                        emotion_name = primary_emotion.get('name', 'æœªçŸ¥')
                        confidence = layer_result.confidence
                        emotion_info = f"ğŸ§  ä¸»è¦æƒ…ç»ª: {emotion_name}\nç½®ä¿¡åº¦: {confidence:.1%}"
                        print(f"âœ… æƒ…ç»ªè¯†åˆ«å®Œæˆ: {emotion_name}, ç½®ä¿¡åº¦: {confidence:.1%}")
                        break
            
            # æŸ¥æ‰¾ç”Ÿæˆçš„éŸ³é¢‘å†…å®¹
            audio_info = "ğŸµ æ­£åœ¨ç”Ÿæˆä¸‰é˜¶æ®µéŸ³ä¹..."
            
            if hasattr(system.pipeline, 'layer_results'):
                for layer_result in system.pipeline.layer_results:
                    if (hasattr(layer_result, 'data') and 
                        'generated_content' in layer_result.data):
                        generated_content = layer_result.data['generated_content']
                        audio_content = generated_content.get('audio', {})
                        
                        if audio_content and 'audio_array' in audio_content:
                            duration = audio_content.get('duration', 0)
                            sample_rate = audio_content.get('sample_rate', 44100)
                            three_stage = audio_content.get('three_stage_narrative', False)
                            
                            # è·å–éŸ³é¢‘æ•°ç»„
                            audio_array = audio_content.get('audio_array')
                            if audio_array is not None and isinstance(audio_array, np.ndarray):
                                if audio_array.dtype != np.float32:
                                    audio_array = audio_array.astype(np.float32)
                                if np.max(np.abs(audio_array)) > 0:
                                    audio_array = audio_array / np.max(np.abs(audio_array))
                                
                                audio_info = f"ğŸµ ä¸‰é˜¶æ®µéŸ³ä¹ç”Ÿæˆå®Œæˆ!\nâ±ï¸ æ—¶é•¿: {duration:.0f}ç§’\nğŸ”Š é‡‡æ ·ç‡: {sample_rate}Hz\nğŸ“– ä¸‰é˜¶æ®µå™äº‹: {'âœ…' if three_stage else 'âŒ'}"
                                
                                # æ˜¾ç¤ºé˜¶æ®µä¿¡æ¯
                                stage_prompts = audio_content.get('stage_prompts', {})
                                if stage_prompts:
                                    audio_info += "\n\nğŸ¼ éŸ³ä¹é˜¶æ®µè®¾è®¡:"
                                    for stage, prompt in stage_prompts.items():
                                        audio_info += f"\nâ€¢ {stage}: {prompt[:100]}..."
                                
                                print(f"âœ… éŸ³é¢‘ç”Ÿæˆå®Œæˆ: {duration:.0f}ç§’, {sample_rate}Hz")
                                break
                        else:
                            # å¦‚æœæ²¡æœ‰çœŸæ­£çš„éŸ³é¢‘ï¼Œæ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
                            if 'error' in audio_content:
                                audio_info = f"âŒ éŸ³é¢‘ç”Ÿæˆå¤±è´¥: {audio_content['error']}"
                            else:
                                audio_info = "âš ï¸ éŸ³é¢‘ç”Ÿæˆä¸­ï¼Œè¯·ç¨å€™..."
                        break
            
            # è¿”å›ç»“æœ (emotion_result, audio_output, video_output, audio_info)
            if audio_array is not None:
                return emotion_info, (sample_rate, audio_array), None, audio_info
            else:
                return emotion_info, None, None, audio_info
                
        except Exception as e:
            print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return f"âŒ å¤„ç†å¤±è´¥: {str(e)}", None, None, f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}"
        finally:
            loop.close()
        
    except Exception as e:
        import traceback
        return f"âŒ å¤„ç†é”™è¯¯: {str(e)}", None, None, f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}"

# é¢„è®¾æƒ…ç»ªé€‰é¡¹
emotion_presets = {
    "ğŸ˜° ç„¦è™‘ç´§å¼ ": "æˆ‘æ„Ÿåˆ°å¾ˆç„¦è™‘ï¼Œå¿ƒè·³åŠ é€Ÿï¼Œéš¾ä»¥å¹³é™ä¸‹æ¥ï¼Œè„‘å­é‡Œæ€»æ˜¯æƒ³ç€å„ç§æ‹…å¿ƒçš„äº‹æƒ…",
    "ğŸ˜´ ç–²æƒ«å›°å€¦": "æˆ‘æ„Ÿåˆ°éå¸¸ç–²æƒ«ï¼Œèº«ä½“å¾ˆç´¯ï¼Œä½†æ˜¯å¤§è„‘è¿˜åœ¨æ´»è·ƒï¼Œéš¾ä»¥å…¥ç¡",
    "ğŸ˜¤ çƒ¦èºä¸å®‰": "æˆ‘æ„Ÿåˆ°å¾ˆçƒ¦èºï¼Œå¿ƒæƒ…ä¸å¥½ï¼Œå®¹æ˜“è¢«å°äº‹å½±å“ï¼Œæ— æ³•é›†ä¸­æ³¨æ„åŠ›",
    "ğŸ˜Œ ç›¸å¯¹å¹³é™": "æˆ‘çš„å¿ƒæƒ…æ¯”è¾ƒå¹³é™ï¼Œä½†å¸Œæœ›èƒ½è¿›å…¥æ›´æ·±å±‚çš„æ”¾æ¾çŠ¶æ€ï¼Œä¸ºç¡çœ åšå‡†å¤‡",
    "ğŸ¤¯ å‹åŠ›å±±å¤§": "æœ€è¿‘å‹åŠ›å¾ˆå¤§ï¼Œå­¦ä¹ å·¥ä½œä»»åŠ¡é‡ï¼Œæ€»æ˜¯æ„Ÿåˆ°æ—¶é—´ä¸å¤Ÿç”¨ï¼Œå†…å¿ƒå¾ˆç´§å¼ "
}

def create_interface():
    """åˆ›å»ºGradioç•Œé¢"""
    
    with gr.Blocks(
        title="ğŸŒ™ ç¡çœ ç–—æ„ˆAI",
        theme=gr.themes.Soft(
            primary_hue="purple",
            secondary_hue="blue"
        ),
        css="""
        .gradio-container {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
        }
        .main-title {
            text-align: center;
            color: white;
            font-size: 3rem;
            margin: 2rem 0;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
        }
        .subtitle {
            text-align: center;
            color: rgba(255,255,255,0.9);
            font-size: 1.2rem;
            margin-bottom: 2rem;
        }
        """
    ) as app:
        
        gr.HTML("""
        <div class="main-title">ğŸŒ™ ç¡çœ ç–—æ„ˆAI</div>
        <div class="subtitle">åŸºäºæƒ…ç»ªè¯†åˆ«çš„ä¸‰é˜¶æ®µéŸ³ä¹å™äº‹ç–—æ„ˆç³»ç»Ÿ</div>
        """)
        
        # ç³»ç»Ÿåˆå§‹åŒ–
        with gr.Row():
            init_btn = gr.Button("ğŸš€ åˆå§‹åŒ–ç³»ç»Ÿ", variant="primary")
            init_status = gr.Textbox(label="ç³»ç»ŸçŠ¶æ€", interactive=False)
        
        init_btn.click(init_system, outputs=init_status)
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ’­ æè¿°æ‚¨ç°åœ¨çš„æ„Ÿå—")
                
                # é¢„è®¾æƒ…ç»ªé€‰æ‹©
                emotion_dropdown = gr.Dropdown(
                    choices=list(emotion_presets.keys()),
                    label="ğŸ­ å¿«é€Ÿé€‰æ‹©é¢„è®¾æƒ…ç»ª",
                    value=None
                )
                
                # æƒ…ç»ªè¾“å…¥æ¡†
                emotion_input = gr.Textbox(
                    label="âœï¸ è¯¦ç»†æè¿°æ‚¨çš„æ„Ÿå—",
                    placeholder="ä¾‹å¦‚ï¼šæˆ‘ä»Šå¤©å·¥ä½œå‹åŠ›å¾ˆå¤§ï¼Œå¿ƒæƒ…æœ‰äº›ç„¦è™‘ï¼Œèººåœ¨åºŠä¸Šæ€»æ˜¯æƒ³ä¸œæƒ³è¥¿ï¼Œæ— æ³•å…¥ç¡...",
                    lines=4
                )
                
                # å¤„ç†æŒ‰é’®
                process_btn = gr.Button("ğŸ§  å¼€å§‹æƒ…ç»ªåˆ†æä¸éŸ³ä¹ç”Ÿæˆ", variant="primary", size="lg")
            
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ¬ æ‚¨çš„ç–—æ„ˆå†…å®¹")
                
                # æƒ…ç»ªè¯†åˆ«ç»“æœ
                emotion_result = gr.Textbox(
                    label="ğŸ§  æƒ…ç»ªè¯†åˆ«ç»“æœ",
                    lines=3,
                    interactive=False
                )
                
                # ç”Ÿæˆçš„éŸ³é¢‘
                audio_output = gr.Audio(
                    label="ğŸµ ä¸‰é˜¶æ®µç–—æ„ˆéŸ³ä¹",
                    type="numpy"
                )
                
                # ç”Ÿæˆçš„è§†é¢‘ï¼ˆå¦‚æœæœ‰ï¼‰
                video_output = gr.Video(
                    label="ğŸ–¼ï¸ ç–—æ„ˆè§†è§‰å†…å®¹",
                    visible=True
                )
                
                # éŸ³é¢‘ä¿¡æ¯
                audio_info = gr.Textbox(
                    label="ğŸ“Š éŸ³é¢‘/è§†é¢‘ä¿¡æ¯",
                    lines=6,
                    interactive=False
                )
        
        # ä½¿ç”¨æŒ‡å—
        with gr.Row():
            gr.Markdown("""
            ### ğŸ’¡ ä½¿ç”¨å»ºè®®
            1. **ğŸ§ ä½©æˆ´è€³æœº**ï¼šè·å¾—æœ€ä½³çš„ç«‹ä½“å£°æ•ˆæœ
            2. **ğŸŒ™ è°ƒæš—ç¯å…‰**ï¼šåˆ›é€ é€‚åˆç¡çœ çš„ç¯å¢ƒ  
            3. **ğŸ§˜â€â™€ï¸ æ”¾æ¾èº«ä½“**ï¼šæ‰¾åˆ°èˆ’é€‚çš„å§¿åŠ¿
            4. **ğŸµ ä¸“æ³¨è†å¬**ï¼šè·ŸéšéŸ³ä¹çš„ä¸‰é˜¶æ®µå¼•å¯¼
            5. **ğŸ˜´ è‡ªç„¶å…¥ç¡**ï¼šè®©éŸ³ä¹å¼•å¯¼æ‚¨è¿›å…¥æ¢¦ä¹¡
            
            ### ğŸ”® AIç–—æ„ˆåŸç†
            - **ğŸ¯ ä¸‰é˜¶æ®µéŸ³ä¹å™äº‹**: åŒ¹é…â†’å¼•å¯¼â†’å·©å›º
            - **ğŸ§  27ç»´æƒ…ç»ªè¯†åˆ«**: è¯†åˆ«ç»†ç²’åº¦çš„ç¡å‰æƒ…ç»ªçŠ¶æ€
            - **ğŸ¼ æ™ºèƒ½éŸ³ä¹ç”Ÿæˆ**: åŸºäºSuno AIçš„éŸ³ä¹åˆ›ä½œï¼Œç¬¦åˆéŸ³ä¹æ²»ç–—ISOåŸåˆ™
            """)
        
        # é¢„è®¾æƒ…ç»ªé€‰æ‹©äº‹ä»¶
        def update_input_from_preset(preset):
            if preset:
                return emotion_presets[preset]
            return ""
        
        emotion_dropdown.change(
            update_input_from_preset,
            inputs=emotion_dropdown,
            outputs=emotion_input
        )
        
        # å¤„ç†æŒ‰é’®äº‹ä»¶
        process_btn.click(
            gradio_process_emotion,
            inputs=emotion_input,
            outputs=[emotion_result, audio_output, video_output, audio_info]
        )
    
    return app

def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºç•Œé¢
    app = create_interface()
    
    # å¯åŠ¨åº”ç”¨
    print("ğŸš€ æ­£åœ¨å¯åŠ¨ç¡çœ ç–—æ„ˆAI...")
    print("ğŸŒ å¯åŠ¨åå°†è‡ªåŠ¨ç”Ÿæˆå…¬å…±è®¿é—®é“¾æ¥...")
    
    # ä½¿ç”¨share=Trueç”Ÿæˆå…¬å…±é“¾æ¥
    app.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=True,  # è¿™æ˜¯å…³é”®ï¼è‡ªåŠ¨ç”Ÿæˆå…¬å…±é“¾æ¥
        debug=False,
        show_error=True
    )

if __name__ == "__main__":
    main()