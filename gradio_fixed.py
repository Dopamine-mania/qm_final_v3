#!/usr/bin/env python3
"""
ğŸŒ™ ç¡çœ ç–—æ„ˆAI - ä¿®å¤ç‰ˆæœ¬
ç›´æ¥ä»ç”Ÿæˆå±‚è·å–éŸ³è§†é¢‘æ•°æ®ï¼Œè¾“å‡ºéŸ³ç”»åŒæ­¥è§†é¢‘
"""

import gradio as gr
import asyncio
import sys
import time
import numpy as np
import cv2
import os
import subprocess
from pathlib import Path
from datetime import datetime
import tempfile

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from main import QMFinal3System
from layers.base_layer import LayerData

# å…¨å±€ç³»ç»Ÿå®ä¾‹
system = None

def init_system():
    """åˆå§‹åŒ–ç³»ç»Ÿ"""
    global system
    try:
        system = QMFinal3System()
        return "âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼"
    except Exception as e:
        return f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}"

def process_emotion_debug(user_input):
    """å¤„ç†æƒ…ç»ªè¾“å…¥å¹¶ç”ŸæˆéŸ³ç”»åŒæ­¥è§†é¢‘"""
    global system
    
    if not user_input or len(user_input.strip()) < 5:
        return "âš ï¸ è¯·è¾“å…¥è‡³å°‘5ä¸ªå­—ç¬¦çš„æƒ…ç»ªæè¿°", None, None, "è¾“å…¥å¤ªçŸ­"
    
    if not system:
        return "âŒ è¯·å…ˆç‚¹å‡»'åˆå§‹åŒ–ç³»ç»Ÿ'æŒ‰é’®", None, None, "ç³»ç»Ÿæœªåˆå§‹åŒ–"
    
    try:
        print(f"ğŸ”„ å¼€å§‹å¤„ç†æƒ…ç»ªè¾“å…¥: {user_input}")
        
        # åˆ›å»ºè¾“å…¥æ•°æ®
        input_data = LayerData(
            layer_name="gradio_fixed",
            timestamp=datetime.now(),
            data={"test_input": user_input},
            metadata={"source": "gradio_fixed", "user_input": user_input}
        )
        
        # æ·»åŠ æ–‡æœ¬è¾“å…¥åˆ°è¾“å…¥å±‚
        if system.layers:
            input_layer = system.layers[0]
            if hasattr(input_layer, 'add_text_input'):
                input_layer.add_text_input(user_input)
        
        # é€šè¿‡çœŸæ­£çš„ç³»ç»Ÿç®¡é“å¤„ç†
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        try:
            result = loop.run_until_complete(system.pipeline.process(input_data))
            
            # ç›´æ¥ä»ç”Ÿæˆå±‚è·å–æ•°æ®
            generation_layer = None
            for layer in system.layers:
                if hasattr(layer, '__class__') and 'GenerationLayer' in str(layer.__class__):
                    generation_layer = layer
                    break
            
            if generation_layer:
                print("âœ… æ‰¾åˆ°ç”Ÿæˆå±‚ï¼Œå¼€å§‹æå–éŸ³è§†é¢‘æ•°æ®...")
                
                # ä»ç”Ÿæˆå±‚çš„ç¼“å­˜ä¸­è·å–æ•°æ®
                if hasattr(generation_layer, 'layer_cache'):
                    cache_data = generation_layer.layer_cache
                    print(f"ğŸ“¦ ç”Ÿæˆå±‚ç¼“å­˜æ•°æ®: {list(cache_data.keys())}")
                    
                    # æŸ¥æ‰¾æœ€æ–°çš„ç”Ÿæˆå†…å®¹
                    latest_content = None
                    for key, value in cache_data.items():
                        if hasattr(value, 'data') and 'generated_content' in value.data:
                            latest_content = value.data['generated_content']
                            break
                    
                    if latest_content:
                        print("ğŸ¬ æ‰¾åˆ°ç”Ÿæˆå†…å®¹ï¼Œå¼€å§‹åˆæˆéŸ³ç”»åŒæ­¥è§†é¢‘...")
                        
                        # æå–éŸ³é¢‘
                        audio_content = latest_content.get('audio', {})
                        audio_array = audio_content.get('audio_array')
                        sample_rate = audio_content.get('sample_rate', 44100)
                        
                        # æå–è§†é¢‘
                        video_content = latest_content.get('video', {})
                        video_frames = video_content.get('frames', [])
                        fps = video_content.get('fps', 30)
                        
                        print(f"ğŸµ éŸ³é¢‘æ•°æ®: {type(audio_array)}, é‡‡æ ·ç‡: {sample_rate}")
                        print(f"ğŸ¬ è§†é¢‘æ•°æ®: {len(video_frames)} å¸§, FPS: {fps}")
                        
                        # åˆæˆéŸ³ç”»åŒæ­¥è§†é¢‘
                        if audio_array is not None and len(video_frames) > 0:
                            video_path = create_synchronized_video(audio_array, sample_rate, video_frames, fps)
                            
                            if video_path:
                                emotion_name = audio_content.get('emotion_name', 'å¤åˆæƒ…ç»ª')
                                emotion_info = f"ğŸ§  è¯†åˆ«æƒ…ç»ª: {emotion_name}\nğŸµ éŸ³é¢‘: {len(audio_array)/sample_rate:.1f}ç§’\nğŸ¬ è§†é¢‘: {len(video_frames)}å¸§"
                                
                                info_text = f"âœ… éŸ³ç”»åŒæ­¥è§†é¢‘ç”ŸæˆæˆåŠŸ!\næ–‡ä»¶è·¯å¾„: {video_path}\néŸ³é¢‘æ—¶é•¿: {len(audio_array)/sample_rate:.1f}ç§’\nè§†é¢‘å¸§æ•°: {len(video_frames)}\nå¸§ç‡: {fps}fps"
                                
                                return emotion_info, video_path, None, info_text
                            else:
                                return "âŒ è§†é¢‘åˆæˆå¤±è´¥", None, None, "æ— æ³•åˆæˆéŸ³ç”»åŒæ­¥è§†é¢‘"
                        else:
                            return "âš ï¸ éŸ³é¢‘æˆ–è§†é¢‘æ•°æ®ä¸å®Œæ•´", None, None, f"éŸ³é¢‘: {type(audio_array)}, è§†é¢‘å¸§: {len(video_frames)}"
                    else:
                        return "âŒ æœªæ‰¾åˆ°ç”Ÿæˆå†…å®¹", None, None, "ç”Ÿæˆå±‚ç¼“å­˜ä¸ºç©º"
                else:
                    return "âŒ ç”Ÿæˆå±‚ç¼“å­˜ä¸å­˜åœ¨", None, None, "æ— æ³•è®¿é—®ç”Ÿæˆå±‚ç¼“å­˜"
            else:
                return "âŒ æœªæ‰¾åˆ°ç”Ÿæˆå±‚", None, None, "ç³»ç»Ÿæ¶æ„é”™è¯¯"
                
        except Exception as e:
            print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return f"âŒ å¤„ç†å¤±è´¥: {str(e)}", None, None, f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}"
        finally:
            loop.close()
        
    except Exception as e:
        import traceback
        return f"âŒ å¤„ç†é”™è¯¯: {str(e)}", None, None, f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}"

def create_synchronized_video(audio_array, sample_rate, video_frames, fps):
    """åˆ›å»ºéŸ³ç”»åŒæ­¥è§†é¢‘"""
    try:
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        temp_dir = tempfile.mkdtemp()
        audio_path = os.path.join(temp_dir, "audio.wav")
        video_path = os.path.join(temp_dir, "video.mp4")
        output_path = os.path.join(temp_dir, "synchronized_video.mp4")
        
        # ä¿å­˜éŸ³é¢‘
        import soundfile as sf
        sf.write(audio_path, audio_array, sample_rate)
        print(f"âœ… éŸ³é¢‘ä¿å­˜åˆ°: {audio_path}")
        
        # åˆ›å»ºè§†é¢‘
        if len(video_frames) > 0:
            frame_height, frame_width = video_frames[0].shape[:2]
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(video_path, fourcc, fps, (frame_width, frame_height))
            
            for frame in video_frames:
                # ç¡®ä¿å¸§æ˜¯RGBæ ¼å¼
                if len(frame.shape) == 3 and frame.shape[2] == 3:
                    # RGBè½¬BGR (OpenCVä½¿ç”¨BGR)
                    frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                    out.write(frame_bgr)
                else:
                    # å¦‚æœæ˜¯ç°åº¦å›¾ï¼Œè½¬æ¢ä¸ºBGR
                    frame_bgr = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)
                    out.write(frame_bgr)
            
            out.release()
            print(f"âœ… è§†é¢‘ä¿å­˜åˆ°: {video_path}")
            
            # ä½¿ç”¨ffmpegåˆæˆéŸ³ç”»åŒæ­¥è§†é¢‘
            ffmpeg_cmd = [
                'ffmpeg', '-y',
                '-i', video_path,
                '-i', audio_path,
                '-c:v', 'libx264',
                '-c:a', 'aac',
                '-strict', 'experimental',
                '-shortest',
                output_path
            ]
            
            result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True)
            if result.returncode == 0:
                print(f"âœ… éŸ³ç”»åŒæ­¥è§†é¢‘ç”ŸæˆæˆåŠŸ: {output_path}")
                return output_path
            else:
                print(f"âŒ ffmpegåˆæˆå¤±è´¥: {result.stderr}")
                return None
        else:
            print("âŒ æ²¡æœ‰è§†é¢‘å¸§")
            return None
            
    except Exception as e:
        print(f"âŒ åˆ›å»ºåŒæ­¥è§†é¢‘æ—¶å‡ºé”™: {e}")
        return None

def create_interface():
    """åˆ›å»ºç•Œé¢"""
    with gr.Blocks(title="ğŸŒ™ ç¡çœ ç–—æ„ˆAI - ä¿®å¤ç‰ˆæœ¬") as app:
        gr.HTML("<h1 style='text-align: center;'>ğŸŒ™ ç¡çœ ç–—æ„ˆAI - éŸ³ç”»åŒæ­¥ç‰ˆæœ¬</h1>")
        
        with gr.Row():
            init_btn = gr.Button("ğŸš€ åˆå§‹åŒ–ç³»ç»Ÿ", variant="primary")
            init_status = gr.Textbox(label="ç³»ç»ŸçŠ¶æ€", lines=2)
        
        with gr.Row():
            with gr.Column():
                emotion_input = gr.Textbox(
                    label="ğŸ’­ æè¿°æ‚¨çš„æ„Ÿå—",
                    placeholder="ä¾‹å¦‚ï¼šæˆ‘æ„Ÿåˆ°å¾ˆç„¦è™‘ï¼Œå¿ƒè·³åŠ é€Ÿï¼Œéš¾ä»¥å…¥ç¡...",
                    lines=4
                )
                
                process_btn = gr.Button("ğŸ¬ ç”ŸæˆéŸ³ç”»åŒæ­¥ç–—æ„ˆè§†é¢‘", variant="primary")
            
            with gr.Column():
                emotion_result = gr.Textbox(label="ğŸ§  æƒ…ç»ªè¯†åˆ«ç»“æœ", lines=3)
                
                # éŸ³ç”»åŒæ­¥è§†é¢‘è¾“å‡º
                video_output = gr.Video(label="ğŸ¬ éŸ³ç”»åŒæ­¥ç–—æ„ˆè§†é¢‘")
                
                # å•ç‹¬çš„éŸ³é¢‘è¾“å‡ºï¼ˆå¤‡ç”¨ï¼‰
                audio_output = gr.Audio(label="ğŸµ éŸ³é¢‘ï¼ˆå¤‡ç”¨ï¼‰")
                
                info_output = gr.Textbox(label="ğŸ“Š è¯¦ç»†ä¿¡æ¯", lines=6)
        
        # ç»‘å®šäº‹ä»¶
        init_btn.click(init_system, outputs=init_status)
        process_btn.click(
            process_emotion_debug,
            inputs=emotion_input,
            outputs=[emotion_result, video_output, audio_output, info_output]
        )
    
    return app

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨éŸ³ç”»åŒæ­¥ç‰ˆæœ¬...")
    
    app = create_interface()
    app.launch(
        server_name="0.0.0.0",
        server_port=7862,
        share=True,
        debug=True
    )

if __name__ == "__main__":
    main()