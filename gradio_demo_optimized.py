#!/usr/bin/env python3
"""
ğŸŒ™ ç¡çœ ç–—æ„ˆAI - ä¼˜åŒ–æ¼”ç¤ºæ¨¡å¼
æ”¯æŒç”¨æˆ·é€‰æ‹©ï¼šçº¯éŸ³ä¹æ¨¡å¼ vs éŸ³ç”»ç»“åˆæ¨¡å¼
"""

import gradio as gr
import numpy as np
import cv2
import os
import tempfile
import time
from pathlib import Path
from datetime import datetime
import json

def generate_therapy_audio(duration=15, sample_rate=44100, emotion="ç„¦è™‘"):
    """ç”Ÿæˆç–—æ„ˆéŸ³é¢‘ï¼ˆå¿«é€Ÿç‰ˆæœ¬ï¼‰"""
    print(f"ğŸµ ç”Ÿæˆ{duration}ç§’ç–—æ„ˆéŸ³é¢‘ (é’ˆå¯¹{emotion}æƒ…ç»ª)")
    
    # æ ¹æ®æƒ…ç»ªè°ƒæ•´é¢‘ç‡å‚æ•°
    emotion_params = {
        "ç„¦è™‘": {"sync": 440, "guide": 330, "consolidate": 220},
        "ç–²æƒ«": {"sync": 380, "guide": 280, "consolidate": 200},
        "çƒ¦èº": {"sync": 460, "guide": 350, "consolidate": 240},
        "å¹³é™": {"sync": 400, "guide": 320, "consolidate": 210},
        "å‹åŠ›": {"sync": 480, "guide": 360, "consolidate": 230}
    }
    
    params = emotion_params.get(emotion, emotion_params["ç„¦è™‘"])
    
    # ä¸‰é˜¶æ®µéŸ³é¢‘ç”Ÿæˆ
    stage_duration = duration // 3
    t = np.linspace(0, stage_duration, int(sample_rate * stage_duration))
    
    # ç¬¬ä¸€é˜¶æ®µï¼šåŒæ­¥æœŸ
    print(f"ğŸµ ç¬¬ä¸€é˜¶æ®µ-åŒæ­¥æœŸ: {params['sync']}Hz")
    stage1 = 0.3 * np.sin(2 * np.pi * params['sync'] * t) * np.exp(-t/5)
    
    # ç¬¬äºŒé˜¶æ®µï¼šå¼•å¯¼æœŸ
    print(f"ğŸµ ç¬¬äºŒé˜¶æ®µ-å¼•å¯¼æœŸ: {params['guide']}Hz")
    stage2 = 0.2 * np.sin(2 * np.pi * params['guide'] * t) * np.exp(-t/8)
    
    # ç¬¬ä¸‰é˜¶æ®µï¼šå·©å›ºæœŸ
    print(f"ğŸµ ç¬¬ä¸‰é˜¶æ®µ-å·©å›ºæœŸ: {params['consolidate']}Hz")
    stage3 = 0.1 * np.sin(2 * np.pi * params['consolidate'] * t) * np.exp(-t/12)
    
    # åˆå¹¶ä¸‰é˜¶æ®µ
    audio_array = np.concatenate([stage1, stage2, stage3])
    
    # æ·»åŠ ç™½å™ªå£°å’Œè‡ªç„¶éŸ³æ•ˆ
    print("ğŸµ æ·»åŠ è‡ªç„¶éŸ³æ•ˆ...")
    noise = 0.03 * np.random.normal(0, 1, len(audio_array))
    
    # æ·»åŠ è½»å¾®çš„åŒå£°é“æ•ˆæœ
    if len(audio_array.shape) == 1:
        # åˆ›å»ºç«‹ä½“å£°
        left_channel = audio_array + noise
        right_channel = audio_array + 0.05 * np.sin(2 * np.pi * 100 * np.linspace(0, duration, len(audio_array)))
        audio_array = np.column_stack([left_channel, right_channel])
    
    # å½’ä¸€åŒ–
    audio_array = audio_array / np.max(np.abs(audio_array))
    
    return audio_array.astype(np.float32), sample_rate, params

def generate_simple_visual(duration=15, fps=30):
    """ç”Ÿæˆç®€åŒ–è§†è§‰å†…å®¹ï¼ˆå¿«é€Ÿç‰ˆæœ¬ï¼‰"""
    print(f"ğŸ¬ ç”Ÿæˆç®€åŒ–è§†è§‰å†…å®¹ ({duration}ç§’, {fps}fps)")
    
    # é™ä½å¸§ç‡å’Œåˆ†è¾¨ç‡ä»¥æé«˜é€Ÿåº¦
    actual_fps = 15  # é™ä½å¸§ç‡
    frame_count = int(duration * actual_fps)
    frames = []
    width, height = 480, 320  # é™ä½åˆ†è¾¨ç‡
    
    for i in range(frame_count):
        if i % 15 == 0:  # æ¯ç§’æ›´æ–°ä¸€æ¬¡
            current_second = i // actual_fps
            print(f"ğŸ¬ æ¸²æŸ“: ç¬¬{current_second+1}ç§’")
        
        # åˆ›å»ºç®€å•çš„æ¸å˜èƒŒæ™¯
        frame = np.zeros((height, width, 3), dtype=np.uint8)
        
        # æ—¶é—´è¿›åº¦
        progress = i / frame_count
        
        # ç®€åŒ–çš„é¢œè‰²å˜åŒ–
        if progress < 0.33:
            color = [int(255 * (1 - progress * 3)), int(100 * progress * 3), 50]
        elif progress < 0.66:
            stage_progress = (progress - 0.33) / 0.33
            color = [int(100 * (1 - stage_progress)), int(100 * (1 - stage_progress)), int(150 * stage_progress)]
        else:
            color = [30, 30, int(100 * (1 - (progress - 0.66) / 0.34))]
        
        # å¡«å……èƒŒæ™¯
        frame[:] = color
        
        # ç®€å•çš„ä¸­å¿ƒåœ†åœˆ
        radius = int(50 + 20 * np.sin(progress * 4 * np.pi))
        cv2.circle(frame, (width//2, height//2), radius, (255, 255, 255), 2)
        
        frames.append(frame)
    
    return frames, actual_fps

def create_audio_video_sync(audio_array, sample_rate, video_frames, fps):
    """å¿«é€ŸéŸ³ç”»åŒæ­¥åˆæˆ"""
    try:
        print("ğŸ¬ å¼€å§‹å¿«é€ŸéŸ³ç”»åŒæ­¥åˆæˆ...")
        
        temp_dir = tempfile.mkdtemp()
        audio_path = os.path.join(temp_dir, "audio.wav")
        video_path = os.path.join(temp_dir, "video.mp4")
        output_path = os.path.join(temp_dir, "synchronized.mp4")
        
        # ä¿å­˜éŸ³é¢‘
        import soundfile as sf
        sf.write(audio_path, audio_array, sample_rate)
        
        # åˆ›å»ºè§†é¢‘
        if len(video_frames) > 0:
            frame_height, frame_width = video_frames[0].shape[:2]
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(video_path, fourcc, fps, (frame_width, frame_height))
            
            for frame in video_frames:
                out.write(frame)
            
            out.release()
            
            # å¿«é€Ÿffmpegåˆæˆ
            try:
                import subprocess
                ffmpeg_cmd = [
                    'ffmpeg', '-y', '-loglevel', 'quiet',
                    '-i', video_path, '-i', audio_path,
                    '-c:v', 'libx264', '-preset', 'ultrafast',
                    '-c:a', 'aac', '-shortest', output_path
                ]
                
                result = subprocess.run(ffmpeg_cmd, capture_output=True)
                if result.returncode == 0:
                    print(f"âœ… å¿«é€ŸéŸ³ç”»åŒæ­¥å®Œæˆ")
                    return output_path
                else:
                    return video_path
            except:
                return video_path
        else:
            return None
            
    except Exception as e:
        print(f"âŒ éŸ³ç”»åŒæ­¥å¤±è´¥: {e}")
        return None

def process_emotion_optimized(user_input, output_mode, duration):
    """ä¼˜åŒ–çš„æƒ…ç»ªå¤„ç†å‡½æ•°"""
    if not user_input or len(user_input.strip()) < 5:
        return "âš ï¸ è¯·è¾“å…¥è‡³å°‘5ä¸ªå­—ç¬¦çš„æƒ…ç»ªæè¿°", None, None, "è¾“å…¥å¤ªçŸ­"
    
    try:
        start_time = time.time()
        
        # æƒ…ç»ªè¯†åˆ«
        print("ğŸ§  æ­¥éª¤1/3: æƒ…ç»ªè¯†åˆ«åˆ†æ")
        emotions = {
            "ç„¦è™‘": {"confidence": 0.85, "type": "ç¡å‰ç„¦è™‘çŠ¶æ€", "color": "è“ç´«è‰²"},
            "ç–²æƒ«": {"confidence": 0.82, "type": "èº«ä½“ç–²æƒ«çŠ¶æ€", "color": "æš–æ©™è‰²"},
            "çƒ¦èº": {"confidence": 0.88, "type": "æƒ…ç»ªçƒ¦èºçŠ¶æ€", "color": "çº¢ç´«è‰²"},
            "å¹³é™": {"confidence": 0.75, "type": "ç›¸å¯¹å¹³é™çŠ¶æ€", "color": "ç»¿è“è‰²"},
            "å‹åŠ›": {"confidence": 0.90, "type": "å¿ƒç†å‹åŠ›çŠ¶æ€", "color": "æ·±è“è‰²"}
        }
        
        detected_emotion = "ç„¦è™‘"
        for emotion_key in emotions.keys():
            if emotion_key in user_input:
                detected_emotion = emotion_key
                break
        
        emotion_info = emotions[detected_emotion]
        
        # ç”ŸæˆéŸ³é¢‘
        print("ğŸµ æ­¥éª¤2/3: ç”Ÿæˆä¸‰é˜¶æ®µç–—æ„ˆéŸ³é¢‘")
        audio_array, sample_rate, audio_params = generate_therapy_audio(
            duration=duration, 
            emotion=detected_emotion
        )
        
        # æ ¹æ®è¾“å‡ºæ¨¡å¼å†³å®šæ˜¯å¦ç”Ÿæˆè§†é¢‘
        video_output = None
        if output_mode == "éŸ³ç”»ç»“åˆ":
            print("ğŸ¬ æ­¥éª¤3/3: ç”ŸæˆéŸ³ç”»åŒæ­¥è§†é¢‘")
            video_frames, fps = generate_simple_visual(duration=duration)
            video_output = create_audio_video_sync(audio_array, sample_rate, video_frames, fps)
        else:
            print("ğŸµ æ­¥éª¤3/3: çº¯éŸ³ä¹æ¨¡å¼ - è·³è¿‡è§†é¢‘ç”Ÿæˆ")
        
        processing_time = time.time() - start_time
        
        # ç»„ç»‡è¿”å›ä¿¡æ¯
        emotion_result = f"""ğŸ§  æƒ…ç»ªè¯†åˆ«ç»“æœ:
æƒ…ç»ªç±»å‹: {detected_emotion}
ç½®ä¿¡åº¦: {emotion_info['confidence']:.1%}
çŠ¶æ€æè¿°: {emotion_info['type']}
è§†è§‰ä¸»é¢˜: {emotion_info['color']}è°ƒ
å¤„ç†æ—¶é—´: {processing_time:.1f}ç§’"""
        
        # éŸ³é¢‘ä¿¡æ¯
        audio_info = f"""âœ… ç–—æ„ˆéŸ³é¢‘ç”Ÿæˆå®Œæˆï¼

ğŸµ éŸ³é¢‘è¯¦æƒ…:
  - æ—¶é•¿: {duration}ç§’
  - é‡‡æ ·ç‡: {sample_rate}Hz
  - å£°é“: ç«‹ä½“å£°
  - é’ˆå¯¹æƒ…ç»ª: {detected_emotion}

ğŸ¼ ä¸‰é˜¶æ®µé¢‘ç‡è®¾è®¡:
  - åŒæ­¥æœŸ: {audio_params['sync']}Hz (åŒ¹é…{detected_emotion}æƒ…ç»ª)
  - å¼•å¯¼æœŸ: {audio_params['guide']}Hz (é€æ­¥å¼•å¯¼è½¬æ¢)
  - å·©å›ºæœŸ: {audio_params['consolidate']}Hz (æ·±åº¦æ”¾æ¾çŠ¶æ€)

âš¡ æ€§èƒ½ä¿¡æ¯:
  - è¾“å‡ºæ¨¡å¼: {output_mode}
  - å¤„ç†æ—¶é—´: {processing_time:.1f}ç§’
  - ç³»ç»ŸçŠ¶æ€: æ­£å¸¸
  
ğŸ§ ä½¿ç”¨å»ºè®®:
  - ä½©æˆ´è€³æœºè·å¾—æœ€ä½³æ•ˆæœ
  - åœ¨å®‰é™ç¯å¢ƒä¸­è†å¬
  - è·ŸéšéŸ³ä¹èŠ‚å¥è°ƒæ•´å‘¼å¸
  - è®©éŸ³ä¹å¼•å¯¼æ‚¨è¿›å…¥æ”¾æ¾çŠ¶æ€"""
        
        if output_mode == "éŸ³ç”»ç»“åˆ" and video_output:
            audio_info += f"""

ğŸ¬ è§†é¢‘ä¿¡æ¯:
  - è§†é¢‘æ–‡ä»¶: å·²ç”ŸæˆéŸ³ç”»åŒæ­¥è§†é¢‘
  - åˆ†è¾¨ç‡: 480x320 (ä¼˜åŒ–æ€§èƒ½)
  - å¸§ç‡: 15fps (ä¼˜åŒ–æ€§èƒ½)
  - è§†è§‰æ•ˆæœ: {emotion_info['color']}è°ƒæ¸å˜ + å‘¼å¸å¼•å¯¼"""
        
        audio_info += f"""

ğŸ“ æŠ€æœ¯è¯´æ˜:
  - æ¼”ç¤ºæ¨¡å¼ï¼šä½¿ç”¨æ•°å­¦åˆæˆéŸ³é¢‘
  - çœŸå®ç‰ˆæœ¬ï¼šå°†ä½¿ç”¨Suno AIç­‰å•†ä¸šAPI
  - éŸ³é¢‘è´¨é‡ï¼šCDçº§åˆ« (44.1kHz/16-bit)
  - ç–—æ„ˆåŸç†ï¼šåŸºäºISOéŸ³ä¹æ²»ç–—åŸåˆ™"""
        
        return emotion_result, video_output, (sample_rate, audio_array), audio_info
        
    except Exception as e:
        import traceback
        error_msg = f"âŒ å¤„ç†é”™è¯¯: {str(e)}"
        print(error_msg)
        traceback.print_exc()
        return error_msg, None, None, f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}"

def create_optimized_interface():
    """åˆ›å»ºä¼˜åŒ–çš„ç”¨æˆ·ç•Œé¢"""
    with gr.Blocks(
        title="ğŸŒ™ ç¡çœ ç–—æ„ˆAI - ä¼˜åŒ–æ¼”ç¤º",
        theme=gr.themes.Soft(primary_hue="purple", secondary_hue="blue")
    ) as app:
        
        gr.HTML("""
        <div style="text-align: center; padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; color: white; margin-bottom: 20px;">
            <h1>ğŸŒ™ ç¡çœ ç–—æ„ˆAI - ä¼˜åŒ–æ¼”ç¤º</h1>
            <p>ä¸ªæ€§åŒ–ä¸‰é˜¶æ®µéŸ³ä¹ç–—æ„ˆç³»ç»Ÿ</p>
            <p style="color: #ffeb3b;">âœ¨ æ”¯æŒçº¯éŸ³ä¹æ¨¡å¼å’ŒéŸ³ç”»ç»“åˆæ¨¡å¼</p>
        </div>
        """)
        
        with gr.Row():
            with gr.Column(scale=2):
                gr.Markdown("### ğŸ’­ æƒ…ç»ªè¾“å…¥")
                
                # æƒ…ç»ªé€‰æ‹©
                emotion_examples = gr.Dropdown(
                    choices=[
                        "ğŸ˜° æˆ‘æ„Ÿåˆ°å¾ˆç„¦è™‘ï¼Œå¿ƒè·³åŠ é€Ÿï¼Œéš¾ä»¥å…¥ç¡",
                        "ğŸ˜´ æˆ‘å¾ˆç–²æƒ«ï¼Œä½†å¤§è„‘è¿˜åœ¨æ´»è·ƒï¼Œæ— æ³•æ”¾æ¾",
                        "ğŸ˜¤ æˆ‘æ„Ÿåˆ°çƒ¦èºä¸å®‰ï¼Œå®¹æ˜“è¢«å°äº‹å½±å“",
                        "ğŸ˜Œ æˆ‘æ¯”è¾ƒå¹³é™ï¼Œä½†å¸Œæœ›æ›´æ·±å±‚çš„æ”¾æ¾",
                        "ğŸ¤¯ æœ€è¿‘å‹åŠ›å¾ˆå¤§ï¼Œæ€»æ˜¯æ„Ÿåˆ°ç´§å¼ "
                    ],
                    label="ğŸ­ å¿«é€Ÿé€‰æ‹©æƒ…ç»ª",
                    value="ğŸ˜° æˆ‘æ„Ÿåˆ°å¾ˆç„¦è™‘ï¼Œå¿ƒè·³åŠ é€Ÿï¼Œéš¾ä»¥å…¥ç¡"
                )
                
                emotion_input = gr.Textbox(
                    label="âœï¸ æˆ–è¯¦ç»†æè¿°æ‚¨çš„æ„Ÿå—",
                    placeholder="æè¿°æ‚¨çš„æƒ…ç»ªçŠ¶æ€...",
                    lines=3,
                    value="æˆ‘æ„Ÿåˆ°å¾ˆç„¦è™‘ï¼Œå¿ƒè·³åŠ é€Ÿï¼Œéš¾ä»¥å…¥ç¡"
                )
                
                gr.Markdown("### âš™ï¸ è¾“å‡ºè®¾ç½®")
                
                # è¾“å‡ºæ¨¡å¼é€‰æ‹©
                output_mode = gr.Radio(
                    choices=["çº¯éŸ³ä¹", "éŸ³ç”»ç»“åˆ"],
                    value="çº¯éŸ³ä¹",
                    label="ğŸ¬ è¾“å‡ºæ¨¡å¼"
                )
                
                # æ—¶é•¿é€‰æ‹©
                duration = gr.Slider(
                    minimum=10, maximum=60, value=15, step=5,
                    label="â±ï¸ éŸ³é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰"
                )
                
                process_btn = gr.Button("ğŸµ å¼€å§‹ç”Ÿæˆç–—æ„ˆå†…å®¹", variant="primary", size="lg")
                
                # æ¨¡å¼è¯´æ˜
                gr.HTML("""
                <div style="margin-top: 15px; padding: 10px; background-color: #f0f8ff; border-radius: 8px;">
                    <strong>ğŸ“Š æ¨¡å¼å¯¹æ¯”ï¼š</strong><br>
                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 10px; margin-top: 10px;">
                        <div>
                            <strong>ğŸµ çº¯éŸ³ä¹æ¨¡å¼</strong><br>
                            â€¢ å¤„ç†æ—¶é—´: 3-5ç§’<br>
                            â€¢ ä¸“æ³¨éŸ³ä¹ç–—æ„ˆ<br>
                            â€¢ é€‚åˆçº¯éŸ³ä¹çˆ±å¥½è€…
                        </div>
                        <div>
                            <strong>ğŸ¬ éŸ³ç”»ç»“åˆæ¨¡å¼</strong><br>
                            â€¢ å¤„ç†æ—¶é—´: 15-20ç§’<br>
                            â€¢ è§†å¬åŒé‡ç–—æ„ˆ<br>
                            â€¢ é€‚åˆå¤šåª’ä½“ä½“éªŒ
                        </div>
                    </div>
                </div>
                """)
            
            with gr.Column(scale=3):
                gr.Markdown("### ğŸ¬ ç”Ÿæˆç»“æœ")
                
                # æƒ…ç»ªè¯†åˆ«ç»“æœ
                emotion_result = gr.Textbox(
                    label="ğŸ§  æƒ…ç»ªè¯†åˆ«ç»“æœ",
                    lines=6,
                    interactive=False
                )
                
                # è§†é¢‘è¾“å‡ºï¼ˆæ¡ä»¶æ˜¾ç¤ºï¼‰
                video_output = gr.Video(
                    label="ğŸ¬ éŸ³ç”»åŒæ­¥ç–—æ„ˆè§†é¢‘",
                    height=300,
                    visible=True
                )
                
                # éŸ³é¢‘è¾“å‡ºï¼ˆä¸»è¦è¾“å‡ºï¼‰
                audio_output = gr.Audio(
                    label="ğŸµ ä¸‰é˜¶æ®µç–—æ„ˆéŸ³é¢‘",
                    type="numpy"
                )
                
                # è¯¦ç»†ä¿¡æ¯
                info_output = gr.Textbox(
                    label="ğŸ“Š è¯¦ç»†ä¿¡æ¯",
                    lines=18,
                    interactive=False
                )
        
        # ä½¿ç”¨æŒ‡å—
        gr.HTML("""
        <div style="margin-top: 20px; padding: 15px; background: #f8f9fa; border-radius: 10px; border: 1px solid #e9ecef;">
            <h3>ğŸ¯ ä½¿ç”¨æŒ‡å—</h3>
            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 15px;">
                <div>
                    <h4>ğŸµ çº¯éŸ³ä¹æ¨¡å¼</h4>
                    <ul>
                        <li>å¿«é€Ÿç”Ÿæˆï¼ˆ3-5ç§’ï¼‰</li>
                        <li>ä¸“æ³¨äºéŸ³ä¹ç–—æ„ˆæ•ˆæœ</li>
                        <li>é€‚åˆæ—¥å¸¸ä½¿ç”¨</li>
                        <li>èŠ‚çœå¤„ç†æ—¶é—´</li>
                    </ul>
                </div>
                <div>
                    <h4>ğŸ¬ éŸ³ç”»ç»“åˆæ¨¡å¼</h4>
                    <ul>
                        <li>å®Œæ•´ä½“éªŒï¼ˆ15-20ç§’ï¼‰</li>
                        <li>è§†å¬åŒé‡ç–—æ„ˆ</li>
                        <li>é€‚åˆæ¼”ç¤ºå±•ç¤º</li>
                        <li>æ²‰æµ¸å¼ä½“éªŒ</li>
                    </ul>
                </div>
                <div>
                    <h4>ğŸ§ æœ€ä½³å®è·µ</h4>
                    <ul>
                        <li>ä½©æˆ´è€³æœºè†å¬</li>
                        <li>é€‰æ‹©å®‰é™ç¯å¢ƒ</li>
                        <li>è·ŸéšéŸ³ä¹è°ƒæ•´å‘¼å¸</li>
                        <li>è®©éŸ³ä¹å¼•å¯¼æ”¾æ¾</li>
                    </ul>
                </div>
            </div>
        </div>
        """)
        
        # äº‹ä»¶ç»‘å®š
        def update_input_from_dropdown(selected):
            return selected.split(" ", 1)[1] if " " in selected else selected
        
        emotion_examples.change(
            update_input_from_dropdown,
            inputs=emotion_examples,
            outputs=emotion_input
        )
        
        process_btn.click(
            process_emotion_optimized,
            inputs=[emotion_input, output_mode, duration],
            outputs=[emotion_result, video_output, audio_output, info_output]
        )
    
    return app

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨ä¼˜åŒ–æ¼”ç¤ºæ¨¡å¼...")
    print("ğŸµ æ”¯æŒçº¯éŸ³ä¹æ¨¡å¼ï¼ˆå¿«é€Ÿï¼‰å’ŒéŸ³ç”»ç»“åˆæ¨¡å¼ï¼ˆå®Œæ•´ï¼‰")
    print("âš¡ çº¯éŸ³ä¹æ¨¡å¼ï¼š3-5ç§’å®Œæˆ")
    print("ğŸ¬ éŸ³ç”»ç»“åˆæ¨¡å¼ï¼š15-20ç§’å®Œæˆ")
    
    app = create_optimized_interface()
    app.launch(
        server_name="0.0.0.0",
        server_port=7866,
        share=True,
        debug=True
    )

if __name__ == "__main__":
    main()