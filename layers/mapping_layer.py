#!/usr/bin/env python3
"""
æ˜ å°„å±‚ (Mapping Layer) - Layer 3

KG-MLPæ··åˆæ˜ å°„æ¶æ„ï¼Œæ ¸å¿ƒåŠŸèƒ½åŒ…æ‹¬ï¼š
1. çŸ¥è¯†å›¾è°±é©±åŠ¨çš„ä¸“å®¶çŸ¥è¯†ç¼–ç 
2. æ·±åº¦å­¦ä¹ çš„ä¸ªæ€§åŒ–æ˜ å°„å­¦ä¹ 
3. 27ç»´æƒ…ç»ªå‘é‡åˆ°éŸ³ä¹å‚æ•°çš„æ˜ å°„
4. æ··åˆèåˆç­–ç•¥å’Œè‡ªé€‚åº”æƒé‡
5. ç¡çœ æ²»ç–—çš„é¢†åŸŸç‰¹åŒ–ä¼˜åŒ–

å¤„ç†æµç¨‹ï¼š
Fusion Layer â†’ Mapping Layer â†’ Generation Layer
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Tuple, Optional, Any, Union
from dataclasses import dataclass
from datetime import datetime
import asyncio
import logging
import json
from enum import Enum

from .base_layer import BaseLayer, LayerData, LayerConfig
from core.utils import (
    ConfigLoader, DataValidator, PerformanceMonitor, 
    get_project_root, normalize_vector, cosine_similarity
)

logger = logging.getLogger(__name__)

class MappingStrategy(Enum):
    """æ˜ å°„ç­–ç•¥æšä¸¾"""
    KNOWLEDGE_GRAPH = "knowledge_graph"      # çº¯çŸ¥è¯†å›¾è°±
    DEEP_LEARNING = "deep_learning"          # çº¯æ·±åº¦å­¦ä¹ 
    HYBRID_FUSION = "hybrid_fusion"          # æ··åˆèåˆ
    ADAPTIVE_WEIGHT = "adaptive_weight"      # è‡ªé€‚åº”æƒé‡

@dataclass
class MappingLayerConfig(LayerConfig):
    """æ˜ å°„å±‚é…ç½®"""
    # åŸºç¡€é…ç½®
    input_emotion_dim: int = 27
    output_music_dim: int = 64  # éŸ³ä¹å‚æ•°ç»´åº¦
    
    # æ˜ å°„ç­–ç•¥
    mapping_strategy: str = "hybrid_fusion"
    kg_weight: float = 0.6      # çŸ¥è¯†å›¾è°±æƒé‡
    mlp_weight: float = 0.4     # MLPæƒé‡
    
    # çŸ¥è¯†å›¾è°±é…ç½®
    kg_enabled: bool = True
    kg_embedding_dim: int = 128
    kg_relation_types: int = 8
    
    # MLPé…ç½®
    mlp_enabled: bool = True
    mlp_hidden_dims: List[int] = None
    mlp_dropout: float = 0.2
    mlp_activation: str = "relu"
    
    # ç¡çœ æ²»ç–—ç‰¹åŒ–
    sleep_therapy_mode: bool = True
    circadian_adaptation: bool = True
    therapy_stage_aware: bool = True
    
    # æ€§èƒ½é…ç½®
    use_gpu: bool = True
    batch_size: int = 1
    max_processing_time: float = 100.0  # ms
    
    def __post_init__(self):
        if self.mlp_hidden_dims is None:
            self.mlp_hidden_dims = [256, 128, 64]

class MusicParameter:
    """å•é˜¶æ®µéŸ³ä¹å‚æ•°ç»“æ„"""
    def __init__(self):
        # åŸºç¡€éŸ³ä¹å‚æ•°
        self.tempo_bpm: float = 60.0          # èŠ‚æ‹é€Ÿåº¦
        self.key_signature: str = "C_major"    # è°ƒæ€§
        self.time_signature: Tuple[int, int] = (4, 4)  # æ‹å·
        self.dynamics: str = "mp"              # åŠ›åº¦
        
        # éŸ³è‰²å’Œç»‡ä½“
        self.instrument_weights: Dict[str, float] = {}  # ä¹å™¨æƒé‡
        self.texture_complexity: float = 0.5   # ç»‡ä½“å¤æ‚åº¦
        self.harmonic_richness: float = 0.5    # å’Œå£°ä¸°å¯Œåº¦
        
        # æƒ…ç»ªè¡¨è¾¾
        self.valence_mapping: float = 0.0      # æ•ˆä»·æ˜ å°„
        self.arousal_mapping: float = 0.0      # å”¤é†’æ˜ å°„
        self.tension_level: float = 0.0        # å¼ åŠ›æ°´å¹³
        
        # æ²»ç–—ç‰¹åŒ–
        self.iso_stage: str = "synchronization"  # ISOé˜¶æ®µ
        self.stage_duration: float = 5.0         # é˜¶æ®µæŒç»­æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰
        
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ä¾¿äºä¼ é€’ç»™éŸ³ä¹ç”ŸæˆAPI"""
        return {
            'tempo_bpm': self.tempo_bpm,
            'key_signature': self.key_signature,
            'time_signature': self.time_signature,
            'dynamics': self.dynamics,
            'instrument_weights': self.instrument_weights,
            'texture_complexity': self.texture_complexity,
            'harmonic_richness': self.harmonic_richness,
            'valence_mapping': self.valence_mapping,
            'arousal_mapping': self.arousal_mapping,
            'tension_level': self.tension_level,
            'iso_stage': self.iso_stage,
            'stage_duration': self.stage_duration
        }

class ISOThreeStageParams:
    """ISOä¸‰é˜¶æ®µéŸ³ä¹å‚æ•°"""
    def __init__(self):
        self.match_stage: MusicParameter = MusicParameter()      # åŒ¹é…é˜¶æ®µ
        self.guide_stage: MusicParameter = MusicParameter()      # å¼•å¯¼é˜¶æ®µ  
        self.target_stage: MusicParameter = MusicParameter()     # ç›®æ ‡é˜¶æ®µ
        
        # è®¾ç½®é˜¶æ®µæ ‡è¯†
        self.match_stage.iso_stage = "match"
        self.guide_stage.iso_stage = "guide"
        self.target_stage.iso_stage = "target"
        
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'match_stage': self.match_stage.to_dict(),
            'guide_stage': self.guide_stage.to_dict(),
            'target_stage': self.target_stage.to_dict()
        }

class KnowledgeGraphModule:
    """çŸ¥è¯†å›¾è°±æ¨¡å—"""
    
    def __init__(self, config: MappingLayerConfig):
        self.config = config
        self.device = torch.device('cuda' if config.use_gpu and torch.cuda.is_available() else 'cpu')
        
        # åŠ è½½æƒ…ç»ª-éŸ³ä¹æ˜ å°„çŸ¥è¯†
        self.emotion_music_knowledge = self._load_emotion_music_knowledge()
        
        # æ„å»ºçŸ¥è¯†å›¾è°±åµŒå…¥
        self.emotion_embeddings = self._build_emotion_embeddings()
        self.music_embeddings = self._build_music_embeddings()
        self.relation_embeddings = self._build_relation_embeddings()
        
        logger.info(f"çŸ¥è¯†å›¾è°±æ¨¡å—åˆå§‹åŒ–å®Œæˆï¼Œè®¾å¤‡: {self.device}")
    
    def _load_emotion_music_knowledge(self) -> Dict[str, Any]:
        """åŠ è½½æƒ…ç»ª-éŸ³ä¹æ˜ å°„çŸ¥è¯†"""
        try:
            config_path = get_project_root() / "configs" / "emotion_27d.yaml"
            emotion_config = ConfigLoader.load_yaml(str(config_path))
            
            # æå–éŸ³ä¹æ²»ç–—æ˜ å°„
            music_therapy_mapping = emotion_config.get('music_therapy_mapping', {})
            
            # æ„å»ºçŸ¥è¯†å›¾è°±
            knowledge = {
                'emotion_to_tempo': {},
                'emotion_to_key': {},
                'emotion_to_dynamics': {},
                'emotion_to_instruments': {},
                'emotion_relationships': emotion_config.get('emotion_relationships', {}),
                'therapy_strategies': music_therapy_mapping
            }
            
            # åŸºäºç°æœ‰é…ç½®æ„å»ºæ˜ å°„è§„åˆ™
            all_emotions = {}
            all_emotions.update(emotion_config.get('base_emotions', {}))
            all_emotions.update(emotion_config.get('sleep_specific_emotions', {}))
            
            for emotion_key, emotion_data in all_emotions.items():
                valence = emotion_data.get('valence', 0.0)
                arousal = emotion_data.get('arousal', 0.0)
                
                # åŸºäºæ•ˆä»·-å”¤é†’çš„éŸ³ä¹æ˜ å°„è§„åˆ™
                knowledge['emotion_to_tempo'][emotion_key] = self._valence_arousal_to_tempo(valence, arousal)
                knowledge['emotion_to_key'][emotion_key] = self._valence_arousal_to_key(valence, arousal)
                knowledge['emotion_to_dynamics'][emotion_key] = self._valence_arousal_to_dynamics(valence, arousal)
                knowledge['emotion_to_instruments'][emotion_key] = self._valence_arousal_to_instruments(valence, arousal)
            
            return knowledge
            
        except Exception as e:
            logger.error(f"åŠ è½½æƒ…ç»ª-éŸ³ä¹çŸ¥è¯†å¤±è´¥: {e}")
            return self._get_default_knowledge()
    
    def _valence_arousal_to_tempo(self, valence: float, arousal: float) -> float:
        """æ•ˆä»·-å”¤é†’åˆ°èŠ‚æ‹é€Ÿåº¦çš„æ˜ å°„"""
        # é«˜å”¤é†’ â†’ å¿«èŠ‚æ‹ï¼Œä½å”¤é†’ â†’ æ…¢èŠ‚æ‹
        # ç¡çœ æ²»ç–—ï¼šæ•´ä½“åå‘è¾ƒæ…¢çš„èŠ‚æ‹
        base_tempo = 60.0  # ç¡çœ æ²»ç–—åŸºç¡€èŠ‚æ‹
        arousal_factor = max(0.1, min(2.0, 1.0 + arousal * 0.8))  # 0.1-2.0å€æ•°
        
        # è´Ÿæ•ˆä»·æƒ…ç»ªåœ¨ç¡çœ æ²»ç–—ä¸­éœ€è¦æ›´æ…¢çš„èŠ‚æ‹
        if valence < 0 and self.config.sleep_therapy_mode:
            arousal_factor *= 0.8
        
        return base_tempo * arousal_factor
    
    def _valence_arousal_to_key(self, valence: float, arousal: float) -> str:
        """æ•ˆä»·-å”¤é†’åˆ°è°ƒæ€§çš„æ˜ å°„"""
        # æ­£æ•ˆä»· â†’ å¤§è°ƒï¼Œè´Ÿæ•ˆä»· â†’ å°è°ƒ
        # é«˜å”¤é†’ â†’ å‡è°ƒï¼Œä½å”¤é†’ â†’ é™è°ƒ
        
        if valence >= 0:
            # å¤§è°ƒç³»
            if arousal > 0.3:
                return "D_major"  # æ˜äº®
            elif arousal > -0.3:
                return "C_major"  # å¹³å’Œ
            else:
                return "F_major"  # æ¸©æš–
        else:
            # å°è°ƒç³»
            if arousal > 0.3:
                return "A_minor"  # æ¿€çƒˆ
            elif arousal > -0.3:
                return "D_minor"  # å¿§éƒ
            else:
                return "G_minor"  # æ·±æ²‰
    
    def _valence_arousal_to_dynamics(self, valence: float, arousal: float) -> str:
        """æ•ˆä»·-å”¤é†’åˆ°åŠ›åº¦çš„æ˜ å°„"""
        # é«˜å”¤é†’ â†’ å¼ºåŠ›åº¦ï¼Œä½å”¤é†’ â†’ å¼±åŠ›åº¦
        # ç¡çœ æ²»ç–—ï¼šæ•´ä½“åå‘è¾ƒå¼±çš„åŠ›åº¦
        
        if self.config.sleep_therapy_mode:
            if arousal > 0.5:
                return "mp"  # ä¸­å¼±
            elif arousal > 0.0:
                return "p"   # å¼±
            else:
                return "pp"  # æå¼±
        else:
            if arousal > 0.5:
                return "mf"  # ä¸­å¼º
            elif arousal > -0.5:
                return "mp"  # ä¸­å¼±
            else:
                return "p"   # å¼±
    
    def _valence_arousal_to_instruments(self, valence: float, arousal: float) -> Dict[str, float]:
        """æ•ˆä»·-å”¤é†’åˆ°ä¹å™¨æƒé‡çš„æ˜ å°„"""
        instruments = {
            'piano': 0.0,
            'strings': 0.0,
            'woodwinds': 0.0,
            'brass': 0.0,
            'percussion': 0.0,
            'harp': 0.0,
            'choir': 0.0,
            'ambient': 0.0
        }
        
        # åŸºäºæ•ˆä»·-å”¤é†’çš„ä¹å™¨é€‰æ‹©
        if valence >= 0:  # æ­£æ•ˆä»·
            instruments['piano'] = 0.6
            instruments['strings'] = 0.4
            if arousal > 0.3:
                instruments['woodwinds'] = 0.3
            else:
                instruments['harp'] = 0.3
        else:  # è´Ÿæ•ˆä»·
            instruments['strings'] = 0.7
            instruments['piano'] = 0.3
            if arousal > 0.3:
                instruments['woodwinds'] = 0.2
            else:
                instruments['ambient'] = 0.4
        
        # ç¡çœ æ²»ç–—æ¨¡å¼è°ƒæ•´
        if self.config.sleep_therapy_mode:
            # å¢åŠ èˆ’ç¼“ä¹å™¨æƒé‡
            instruments['ambient'] = max(instruments['ambient'], 0.3)
            instruments['harp'] = max(instruments['harp'], 0.2)
            # å‡å°‘æ¿€çƒˆä¹å™¨æƒé‡
            instruments['brass'] = min(instruments['brass'], 0.1)
            instruments['percussion'] = min(instruments['percussion'], 0.1)
        
        return instruments
    
    def _build_emotion_embeddings(self) -> torch.Tensor:
        """æ„å»ºæƒ…ç»ªåµŒå…¥"""
        # 27ç»´æƒ…ç»ªçš„åµŒå…¥è¡¨ç¤º
        emotion_embeddings = torch.randn(27, self.config.kg_embedding_dim).to(self.device)
        return emotion_embeddings
    
    def _build_music_embeddings(self) -> torch.Tensor:
        """æ„å»ºéŸ³ä¹å‚æ•°åµŒå…¥"""
        # éŸ³ä¹å‚æ•°çš„åµŒå…¥è¡¨ç¤º
        music_embeddings = torch.randn(self.config.output_music_dim, self.config.kg_embedding_dim).to(self.device)
        return music_embeddings
    
    def _build_relation_embeddings(self) -> torch.Tensor:
        """æ„å»ºå…³ç³»åµŒå…¥"""
        # æƒ…ç»ª-éŸ³ä¹å…³ç³»çš„åµŒå…¥è¡¨ç¤º
        relation_embeddings = torch.randn(self.config.kg_relation_types, self.config.kg_embedding_dim).to(self.device)
        return relation_embeddings
    
    def _get_default_knowledge(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤çŸ¥è¯†"""
        return {
            'emotion_to_tempo': {},
            'emotion_to_key': {},
            'emotion_to_dynamics': {},
            'emotion_to_instruments': {},
            'emotion_relationships': {},
            'therapy_strategies': {}
        }
    
    def map_emotion_to_music(self, emotion_vector: torch.Tensor) -> MusicParameter:
        """åŸºäºçŸ¥è¯†å›¾è°±æ˜ å°„æƒ…ç»ªåˆ°éŸ³ä¹å‚æ•°"""
        # è®¡ç®—æƒ…ç»ªåµŒå…¥
        emotion_embedding = torch.matmul(emotion_vector, self.emotion_embeddings)
        
        # é€šè¿‡çŸ¥è¯†å›¾è°±å…³ç³»è®¡ç®—éŸ³ä¹å‚æ•°
        music_embedding = self._apply_kg_relations(emotion_embedding)
        
        # è½¬æ¢ä¸ºå…·ä½“çš„éŸ³ä¹å‚æ•°
        music_params = self._embedding_to_music_params(music_embedding, emotion_vector)
        
        return music_params
    
    def _apply_kg_relations(self, emotion_embedding: torch.Tensor) -> torch.Tensor:
        """åº”ç”¨çŸ¥è¯†å›¾è°±å…³ç³»"""
        # é€šè¿‡å…³ç³»åµŒå…¥è®¡ç®—éŸ³ä¹åµŒå…¥
        # è¿™é‡Œä½¿ç”¨ç®€åŒ–çš„çº¿æ€§å˜æ¢ï¼Œå®é™…å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„å›¾ç¥ç»ç½‘ç»œ
        
        # å…³ç³»å˜æ¢
        relation_weights = torch.randn(self.config.kg_relation_types, self.config.kg_embedding_dim, self.config.kg_embedding_dim).to(self.device)
        
        # åº”ç”¨å¤šç§å…³ç³»
        music_embedding = torch.zeros(self.config.kg_embedding_dim).to(self.device)
        for i in range(self.config.kg_relation_types):
            relation_output = torch.matmul(emotion_embedding, relation_weights[i])
            music_embedding += relation_output / self.config.kg_relation_types
        
        return music_embedding
    
    def _embedding_to_music_params(self, music_embedding: torch.Tensor, emotion_vector: torch.Tensor) -> MusicParameter:
        """å°†éŸ³ä¹åµŒå…¥è½¬æ¢ä¸ºå…·ä½“çš„éŸ³ä¹å‚æ•°"""
        music_params = MusicParameter()
        
        # é€šè¿‡çº¿æ€§å˜æ¢è·å–éŸ³ä¹å‚æ•°
        param_weights = torch.randn(self.config.kg_embedding_dim, 13).to(self.device)  # 13ä¸ªåŸºç¡€å‚æ•°
        raw_params = torch.matmul(music_embedding, param_weights)
        
        # æ˜ å°„åˆ°å…·ä½“å‚æ•°
        music_params.tempo_bpm = max(40.0, min(120.0, 60.0 + raw_params[0].item() * 20.0))
        music_params.valence_mapping = torch.tanh(raw_params[1]).item()
        music_params.arousal_mapping = torch.tanh(raw_params[2]).item()
        music_params.tension_level = torch.sigmoid(raw_params[3]).item()
        music_params.texture_complexity = torch.sigmoid(raw_params[4]).item()
        music_params.harmonic_richness = torch.sigmoid(raw_params[5]).item()
        music_params.therapy_intensity = torch.sigmoid(raw_params[6]).item()
        music_params.sleep_readiness = torch.sigmoid(raw_params[7]).item()
        
        # ä¹å™¨æƒé‡
        instrument_names = ['piano', 'strings', 'woodwinds', 'brass', 'percussion']
        instrument_weights = torch.softmax(raw_params[8:13], dim=0)
        for i, name in enumerate(instrument_names):
            music_params.instrument_weights[name] = instrument_weights[i].item()
        
        # è°ƒæ€§å’ŒåŠ›åº¦é€šè¿‡è§„åˆ™æ˜ å°„
        avg_valence = torch.mean(emotion_vector * torch.tensor([
            # è¿™é‡Œéœ€è¦æ ¹æ®27ç»´æƒ…ç»ªçš„æ•ˆä»·æƒé‡
            -0.8, -0.6, -0.7, -0.6, 0.6, 0.8, 0.7, 0.5, 0.0,  # åŸºç¡€9ç»´
            -0.4, -0.7, -0.3, -0.2, -0.1, -0.5, -0.8, -0.2, -0.4,  # ç¡çœ æƒ…ç»ª10-18
            -0.1, -0.3, -0.6, -0.5, -0.6, -0.4, -0.3, -0.5, -0.7   # ç¡çœ æƒ…ç»ª19-27
        ]).to(self.device)).item()
        
        avg_arousal = torch.mean(emotion_vector * torch.tensor([
            # è¿™é‡Œéœ€è¦æ ¹æ®27ç»´æƒ…ç»ªçš„å”¤é†’æƒé‡
            0.7, 0.8, 0.3, -0.2, 0.4, 0.6, 0.5, -0.3, 0.0,  # åŸºç¡€9ç»´
            0.3, 0.6, -0.6, -0.4, 0.9, 0.4, 0.5, 0.8, 0.2,  # ç¡çœ æƒ…ç»ª10-18
            -0.7, 0.7, 0.4, -0.1, 0.6, 0.3, 0.2, 0.4, 0.1   # ç¡çœ æƒ…ç»ª19-27
        ]).to(self.device)).item()
        
        music_params.key_signature = self._valence_arousal_to_key(avg_valence, avg_arousal)
        music_params.dynamics = self._valence_arousal_to_dynamics(avg_valence, avg_arousal)
        
        return music_params

class MLPMappingModule(nn.Module):
    """MLPæ˜ å°„æ¨¡å—"""
    
    def __init__(self, config: MappingLayerConfig):
        super().__init__()
        self.config = config
        
        # æ„å»ºMLPç½‘ç»œ
        layers = []
        input_dim = config.input_emotion_dim
        
        for hidden_dim in config.mlp_hidden_dims:
            layers.append(nn.Linear(input_dim, hidden_dim))
            if config.mlp_activation == "relu":
                layers.append(nn.ReLU())
            elif config.mlp_activation == "gelu":
                layers.append(nn.GELU())
            elif config.mlp_activation == "tanh":
                layers.append(nn.Tanh())
            
            if config.mlp_dropout > 0:
                layers.append(nn.Dropout(config.mlp_dropout))
            
            input_dim = hidden_dim
        
        # è¾“å‡ºå±‚
        layers.append(nn.Linear(input_dim, config.output_music_dim))
        
        self.mlp = nn.Sequential(*layers)
        
        # ä¸ªæ€§åŒ–é€‚åº”å±‚
        self.personal_adaptation = nn.Linear(config.output_music_dim, config.output_music_dim)
        
        # æ²»ç–—é˜¶æ®µæ„ŸçŸ¥å±‚
        if config.therapy_stage_aware:
            self.therapy_stage_layer = nn.Linear(config.output_music_dim + 3, config.output_music_dim)  # +3 for ISO stages
        
        logger.info(f"MLPæ˜ å°„æ¨¡å—åˆå§‹åŒ–å®Œæˆï¼Œç½‘ç»œç»“æ„: {config.mlp_hidden_dims}")
    
    def forward(self, emotion_vector: torch.Tensor, therapy_stage: Optional[torch.Tensor] = None) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        # åŸºç¡€MLPæ˜ å°„
        music_features = self.mlp(emotion_vector)
        
        # ä¸ªæ€§åŒ–é€‚åº”
        adapted_features = self.personal_adaptation(music_features)
        
        # æ²»ç–—é˜¶æ®µæ„ŸçŸ¥
        if therapy_stage is not None and self.config.therapy_stage_aware:
            combined_features = torch.cat([adapted_features, therapy_stage], dim=-1)
            final_features = self.therapy_stage_layer(combined_features)
        else:
            final_features = adapted_features
        
        return final_features
    
    def extract_music_parameters(self, music_features: torch.Tensor) -> MusicParameter:
        """ä»ç‰¹å¾å‘é‡æå–éŸ³ä¹å‚æ•°"""
        music_params = MusicParameter()
        
        # åŸºç¡€å‚æ•°æ˜ å°„
        music_params.tempo_bpm = max(40.0, min(120.0, 60.0 + music_features[0].item() * 30.0))
        music_params.valence_mapping = torch.tanh(music_features[1]).item()
        music_params.arousal_mapping = torch.tanh(music_features[2]).item()
        music_params.tension_level = torch.sigmoid(music_features[3]).item()
        music_params.texture_complexity = torch.sigmoid(music_features[4]).item()
        music_params.harmonic_richness = torch.sigmoid(music_features[5]).item()
        music_params.therapy_intensity = torch.sigmoid(music_features[6]).item()
        music_params.sleep_readiness = torch.sigmoid(music_features[7]).item()
        
        # ä¹å™¨æƒé‡
        instrument_features = music_features[8:16]
        instrument_weights = torch.softmax(instrument_features, dim=0)
        instrument_names = ['piano', 'strings', 'woodwinds', 'brass', 'percussion', 'harp', 'choir', 'ambient']
        
        for i, name in enumerate(instrument_names):
            music_params.instrument_weights[name] = instrument_weights[i].item()
        
        # å…¶ä»–å‚æ•°
        music_params.key_signature = self._features_to_key(music_features[16:20])
        music_params.dynamics = self._features_to_dynamics(music_features[20:24])
        
        return music_params
    
    def _features_to_key(self, key_features: torch.Tensor) -> str:
        """ç‰¹å¾åˆ°è°ƒæ€§çš„æ˜ å°„"""
        key_probs = torch.softmax(key_features, dim=0)
        key_names = ["C_major", "D_major", "F_major", "G_minor", "A_minor", "D_minor"]
        
        if len(key_names) > len(key_probs):
            key_names = key_names[:len(key_probs)]
        
        key_idx = torch.argmax(key_probs).item()
        return key_names[key_idx] if key_idx < len(key_names) else "C_major"
    
    def _features_to_dynamics(self, dynamics_features: torch.Tensor) -> str:
        """ç‰¹å¾åˆ°åŠ›åº¦çš„æ˜ å°„"""
        dynamics_probs = torch.softmax(dynamics_features, dim=0)
        dynamics_names = ["pp", "p", "mp", "mf"]
        
        if len(dynamics_names) > len(dynamics_probs):
            dynamics_names = dynamics_names[:len(dynamics_probs)]
        
        dynamics_idx = torch.argmax(dynamics_probs).item()
        return dynamics_names[dynamics_idx] if dynamics_idx < len(dynamics_names) else "mp"

class MappingLayer(BaseLayer):
    """æ˜ å°„å±‚ - KG-MLPæ··åˆæƒ…ç»ªåˆ°éŸ³ä¹å‚æ•°æ˜ å°„"""
    
    def __init__(self, config: MappingLayerConfig):
        super().__init__(config)
        self.config = config
        self.layer_name = "mapping_layer"
        
        # åˆå§‹åŒ–è®¾å¤‡
        self.device = torch.device('cuda' if config.use_gpu and torch.cuda.is_available() else 'cpu')
        logger.info(f"æ˜ å°„å±‚ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆå§‹åŒ–çŸ¥è¯†å›¾è°±æ¨¡å—
        if config.kg_enabled:
            self.kg_module = KnowledgeGraphModule(config)
        else:
            self.kg_module = None
        
        # åˆå§‹åŒ–MLPæ¨¡å—
        if config.mlp_enabled:
            self.mlp_module = MLPMappingModule(config).to(self.device)
        else:
            self.mlp_module = None
        
        # æ··åˆèåˆæƒé‡
        self.kg_weight = config.kg_weight
        self.mlp_weight = config.mlp_weight
        
        # æ€§èƒ½ç›‘æ§
        self.performance_monitor = PerformanceMonitor()
        
        # æ•°æ®éªŒè¯å™¨
        self.data_validator = DataValidator()
        
        logger.info(f"æ˜ å°„å±‚åˆå§‹åŒ–å®Œæˆï¼Œç­–ç•¥: {config.mapping_strategy}")
    
    def _extract_emotion_vector(self, input_data: Dict[str, Any]) -> torch.Tensor:
        """ä»è¾“å…¥æ•°æ®æå–æƒ…ç»ªå‘é‡"""
        if 'emotion_analysis' in input_data:
            emotion_analysis = input_data['emotion_analysis']
            if 'emotion_vector' in emotion_analysis:
                emotion_vector = torch.tensor(emotion_analysis['emotion_vector'], dtype=torch.float32).to(self.device)
                return emotion_vector
        
        # å¦‚æœæ²¡æœ‰æƒ…ç»ªå‘é‡ï¼Œåˆ›å»ºé»˜è®¤çš„ä¸­æ€§å‘é‡
        logger.warning("æœªæ‰¾åˆ°æƒ…ç»ªå‘é‡ï¼Œä½¿ç”¨ä¸­æ€§å‘é‡")
        neutral_vector = torch.zeros(self.config.input_emotion_dim, dtype=torch.float32).to(self.device)
        neutral_vector[8] = 1.0  # ä¸­æ€§æƒ…ç»ªä½ç½®
        return neutral_vector
    
    def _extract_therapy_context(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """æå–æ²»ç–—ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        therapy_context = {
            'iso_stage': 'synchronization',
            'therapy_session_time': 0.0,
            'user_feedback': None,
            'circadian_phase': 'evening'
        }
        
        # ä»è¾“å…¥æ•°æ®ä¸­æå–æ²»ç–—ä¸Šä¸‹æ–‡
        if 'therapy_context' in input_data:
            therapy_context.update(input_data['therapy_context'])
        
        return therapy_context
    
    def _kg_mapping(self, emotion_vector: torch.Tensor) -> MusicParameter:
        """çŸ¥è¯†å›¾è°±æ˜ å°„"""
        if self.kg_module is None:
            return MusicParameter()
        
        return self.kg_module.map_emotion_to_music(emotion_vector)
    
    def _mlp_mapping(self, emotion_vector: torch.Tensor, therapy_context: Dict[str, Any]) -> MusicParameter:
        """MLPæ˜ å°„"""
        if self.mlp_module is None:
            return MusicParameter()
        
        # æ„å»ºæ²»ç–—é˜¶æ®µå‘é‡
        therapy_stage_vector = None
        if self.config.therapy_stage_aware:
            iso_stage = therapy_context.get('iso_stage', 'synchronization')
            therapy_stage_vector = torch.zeros(3).to(self.device)
            if iso_stage == 'synchronization':
                therapy_stage_vector[0] = 1.0
            elif iso_stage == 'guidance':
                therapy_stage_vector[1] = 1.0
            elif iso_stage == 'consolidation':
                therapy_stage_vector[2] = 1.0
        
        # MLPå‰å‘ä¼ æ’­
        music_features = self.mlp_module(emotion_vector.unsqueeze(0), therapy_stage_vector.unsqueeze(0) if therapy_stage_vector is not None else None)
        
        # æå–éŸ³ä¹å‚æ•°
        music_params = self.mlp_module.extract_music_parameters(music_features.squeeze(0))
        
        return music_params
    
    def _hybrid_fusion(self, kg_params: MusicParameter, mlp_params: MusicParameter) -> MusicParameter:
        """æ··åˆèåˆKGå’ŒMLPçš„ç»“æœ"""
        fused_params = MusicParameter()
        
        # çº¿æ€§èåˆåŸºç¡€å‚æ•°
        fused_params.tempo_bpm = self.kg_weight * kg_params.tempo_bpm + self.mlp_weight * mlp_params.tempo_bpm
        fused_params.valence_mapping = self.kg_weight * kg_params.valence_mapping + self.mlp_weight * mlp_params.valence_mapping
        fused_params.arousal_mapping = self.kg_weight * kg_params.arousal_mapping + self.mlp_weight * mlp_params.arousal_mapping
        fused_params.tension_level = self.kg_weight * kg_params.tension_level + self.mlp_weight * mlp_params.tension_level
        fused_params.texture_complexity = self.kg_weight * kg_params.texture_complexity + self.mlp_weight * mlp_params.texture_complexity
        fused_params.harmonic_richness = self.kg_weight * kg_params.harmonic_richness + self.mlp_weight * mlp_params.harmonic_richness
        fused_params.therapy_intensity = self.kg_weight * kg_params.therapy_intensity + self.mlp_weight * mlp_params.therapy_intensity
        fused_params.sleep_readiness = self.kg_weight * kg_params.sleep_readiness + self.mlp_weight * mlp_params.sleep_readiness
        
        # èåˆä¹å™¨æƒé‡
        all_instruments = set(kg_params.instrument_weights.keys()) | set(mlp_params.instrument_weights.keys())
        for instrument in all_instruments:
            kg_weight = kg_params.instrument_weights.get(instrument, 0.0)
            mlp_weight = mlp_params.instrument_weights.get(instrument, 0.0)
            fused_params.instrument_weights[instrument] = self.kg_weight * kg_weight + self.mlp_weight * mlp_weight
        
        # åˆ†ç±»å‚æ•°é‡‡ç”¨æƒé‡é€‰æ‹©
        if self.kg_weight > self.mlp_weight:
            fused_params.key_signature = kg_params.key_signature
            fused_params.dynamics = kg_params.dynamics
            fused_params.iso_stage = kg_params.iso_stage
        else:
            fused_params.key_signature = mlp_params.key_signature
            fused_params.dynamics = mlp_params.dynamics
            fused_params.iso_stage = mlp_params.iso_stage
        
        fused_params.time_signature = kg_params.time_signature  # ä¿æŒé»˜è®¤
        
        return fused_params
    
    def _generate_iso_three_stages(self, emotion_vector: torch.Tensor, therapy_context: Dict[str, Any]) -> ISOThreeStageParams:
        """ç”ŸæˆISOä¸‰é˜¶æ®µéŸ³ä¹å‚æ•°"""
        iso_params = ISOThreeStageParams()
        
        # è·å–åŸºç¡€æƒ…ç»ªæ˜ å°„å‚æ•°
        base_kg_params = self._kg_mapping(emotion_vector) if self.kg_module else MusicParameter()
        base_mlp_params = self._mlp_mapping(emotion_vector, therapy_context) if self.mlp_module else MusicParameter()
        base_params = self._hybrid_fusion(base_kg_params, base_mlp_params)
        
        # 1. åŒ¹é…é˜¶æ®µ (Match Stage) - ä¸å½“å‰æƒ…ç»ªçŠ¶æ€åŒæ­¥
        iso_params.match_stage = self._create_match_stage_params(base_params, emotion_vector, therapy_context)
        
        # 2. å¼•å¯¼é˜¶æ®µ (Guide Stage) - é€æ­¥è¿‡æ¸¡åˆ°ç›®æ ‡çŠ¶æ€  
        iso_params.guide_stage = self._create_guide_stage_params(base_params, emotion_vector, therapy_context)
        
        # 3. ç›®æ ‡é˜¶æ®µ (Target Stage) - è¾¾åˆ°ç¡çœ å‡†å¤‡çŠ¶æ€
        iso_params.target_stage = self._create_target_stage_params(base_params, emotion_vector, therapy_context)
        
        return iso_params
    
    def _create_match_stage_params(self, base_params: MusicParameter, emotion_vector: torch.Tensor, therapy_context: Dict[str, Any]) -> MusicParameter:
        """åˆ›å»ºåŒ¹é…é˜¶æ®µå‚æ•° - ä¸ç”¨æˆ·å½“å‰æƒ…ç»ªçŠ¶æ€åŒæ­¥"""
        match_params = MusicParameter()
        
        # å¤åˆ¶åŸºç¡€å‚æ•°
        match_params.tempo_bpm = base_params.tempo_bpm
        match_params.key_signature = base_params.key_signature
        match_params.dynamics = base_params.dynamics
        match_params.valence_mapping = base_params.valence_mapping
        match_params.arousal_mapping = base_params.arousal_mapping
        match_params.tension_level = base_params.tension_level
        match_params.instrument_weights = base_params.instrument_weights.copy()
        
        # åŒ¹é…é˜¶æ®µç‰¹åŒ–ï¼šä¿æŒä¸å½“å‰æƒ…ç»ªçš„é«˜åº¦ä¸€è‡´æ€§
        match_params.iso_stage = "match"
        match_params.stage_duration = 2.0  # 2åˆ†é’ŸåŒ¹é…æœŸ
        match_params.therapy_intensity = 0.3  # ä½æ²»ç–—å¼ºåº¦ï¼Œä¸»è¦æ˜¯å»ºç«‹åŒæ­¥
        match_params.sleep_readiness = 0.2   # ä½ç¡çœ å‡†å¤‡åº¦
        
        # å¼ºåŒ–å½“å‰æƒ…ç»ªç‰¹å¾
        if abs(match_params.arousal_mapping) > 0.5:  # é«˜å”¤é†’çŠ¶æ€
            match_params.tempo_bpm = min(100.0, match_params.tempo_bpm * 1.1)  # ç¨å¾®å¢åŠ èŠ‚æ‹åŒ¹é…
        
        return match_params
    
    def _create_guide_stage_params(self, base_params: MusicParameter, emotion_vector: torch.Tensor, therapy_context: Dict[str, Any]) -> MusicParameter:
        """åˆ›å»ºå¼•å¯¼é˜¶æ®µå‚æ•° - é€æ­¥å¼•å¯¼å‘ç›®æ ‡çŠ¶æ€è¿‡æ¸¡"""
        guide_params = MusicParameter()
        
        # åŸºäºåŸºç¡€å‚æ•°è¿›è¡Œå¼•å¯¼æ€§è°ƒæ•´
        guide_params.tempo_bpm = max(50.0, base_params.tempo_bpm * 0.85)  # é€æ­¥é™ä½èŠ‚æ‹
        guide_params.key_signature = self._transition_to_calming_key(base_params.key_signature)
        guide_params.dynamics = self._transition_to_calming_dynamics(base_params.dynamics)
        
        # å¼•å¯¼è¿‡æ¸¡ä¸­çš„å‚æ•°
        guide_params.valence_mapping = base_params.valence_mapping * 0.7 + 0.3 * 0.2  # å‘ä¸­æ€§åæ­£è½¬ç§»
        guide_params.arousal_mapping = base_params.arousal_mapping * 0.6  # é™ä½å”¤é†’æ°´å¹³
        guide_params.tension_level = base_params.tension_level * 0.7  # é™ä½å¼ åŠ›
        
        # ä¹å™¨è¿‡æ¸¡ï¼šå¢åŠ èˆ’ç¼“ä¹å™¨æ¯”é‡
        guide_params.instrument_weights = base_params.instrument_weights.copy()
        guide_params.instrument_weights['strings'] = min(0.8, guide_params.instrument_weights.get('strings', 0.0) + 0.2)
        guide_params.instrument_weights['harp'] = min(0.6, guide_params.instrument_weights.get('harp', 0.0) + 0.3)
        guide_params.instrument_weights['ambient'] = min(0.5, guide_params.instrument_weights.get('ambient', 0.0) + 0.2)
        
        # å¼•å¯¼é˜¶æ®µç‰¹åŒ–
        guide_params.iso_stage = "guide"
        guide_params.stage_duration = 6.0  # 6åˆ†é’Ÿå¼•å¯¼æœŸ
        guide_params.therapy_intensity = 0.7  # ä¸­é«˜æ²»ç–—å¼ºåº¦
        guide_params.sleep_readiness = 0.5   # ä¸­ç­‰ç¡çœ å‡†å¤‡åº¦
        guide_params.texture_complexity = base_params.texture_complexity * 0.8  # ç®€åŒ–ç»‡ä½“
        guide_params.harmonic_richness = base_params.harmonic_richness * 0.9   # ç®€åŒ–å’Œå£°
        
        return guide_params
    
    def _create_target_stage_params(self, base_params: MusicParameter, emotion_vector: torch.Tensor, therapy_context: Dict[str, Any]) -> MusicParameter:
        """åˆ›å»ºç›®æ ‡é˜¶æ®µå‚æ•° - è¾¾åˆ°ç†æƒ³çš„ç¡çœ å‡†å¤‡çŠ¶æ€"""
        target_params = MusicParameter()
        
        # ç›®æ ‡ï¼šæ·±åº¦æ”¾æ¾çš„ç¡çœ å‡†å¤‡çŠ¶æ€
        target_params.tempo_bpm = 45.0  # å›ºå®šçš„ææ…¢èŠ‚æ‹
        target_params.key_signature = "C_major"  # æœ€å®‰å…¨å¹³å’Œçš„è°ƒæ€§
        target_params.dynamics = "pp"   # æå¼±åŠ›åº¦
        target_params.time_signature = (4, 4)  # ç¨³å®šæ‹å·
        
        # ç›®æ ‡æƒ…ç»ªçŠ¶æ€ï¼šä½å”¤é†’ã€ä¸­æ€§åæ­£æ•ˆä»·
        target_params.valence_mapping = 0.2    # è½»å¾®æ­£æ•ˆä»·
        target_params.arousal_mapping = -0.7   # æä½å”¤é†’
        target_params.tension_level = 0.1      # æä½å¼ åŠ›
        
        # ç¡çœ ä¼˜åŒ–ä¹å™¨é…ç½®
        target_params.instrument_weights = {
            'piano': 0.3,      # æ¸©å’Œé’¢ç´
            'strings': 0.5,    # ä¸»å¯¼å¼¦ä¹
            'harp': 0.4,       # èˆ’ç¼“ç«–ç´
            'ambient': 0.6,    # ç¯å¢ƒéŸ³æ•ˆ
            'woodwinds': 0.2,  # è½»æŸ”æœ¨ç®¡
            'choir': 0.3,      # å¤©ç±äººå£°
            'brass': 0.0,      # æ— é“œç®¡
            'percussion': 0.0  # æ— æ‰“å‡»ä¹
        }
        
        # ç›®æ ‡é˜¶æ®µç‰¹åŒ–
        target_params.iso_stage = "target"
        target_params.stage_duration = 7.0  # 7åˆ†é’Ÿå·©å›ºæœŸ
        target_params.therapy_intensity = 0.9  # é«˜æ²»ç–—å¼ºåº¦
        target_params.sleep_readiness = 0.9    # é«˜ç¡çœ å‡†å¤‡åº¦
        target_params.texture_complexity = 0.2 # æç®€ç»‡ä½“
        target_params.harmonic_richness = 0.3  # ç®€å•å’Œå£°
        
        return target_params
    
    def _transition_to_calming_key(self, current_key: str) -> str:
        """è°ƒæ€§å‘èˆ’ç¼“æ–¹å‘è¿‡æ¸¡"""
        # å¤§è°ƒç³»ä¿æŒï¼Œå°è°ƒç³»å‘ç›¸å¯¹å¤§è°ƒæˆ–å¹³è¡Œå¤§è°ƒè¿‡æ¸¡
        key_transitions = {
            "A_minor": "C_major",    # å…³ç³»å¤§è°ƒ
            "D_minor": "F_major",    # å…³ç³»å¤§è°ƒ
            "G_minor": "Bb_major",   # å…³ç³»å¤§è°ƒ
            "E_minor": "G_major",    # å…³ç³»å¤§è°ƒ
            "B_minor": "D_major",    # å…³ç³»å¤§è°ƒ
            "F#_minor": "A_major",   # å…³ç³»å¤§è°ƒ
        }
        return key_transitions.get(current_key, current_key)
    
    def _transition_to_calming_dynamics(self, current_dynamics: str) -> str:
        """åŠ›åº¦å‘èˆ’ç¼“æ–¹å‘è¿‡æ¸¡"""
        dynamics_transitions = {
            "ff": "mf",   # æå¼ºâ†’ä¸­å¼º
            "f": "mp",    # å¼ºâ†’ä¸­å¼±
            "mf": "p",    # ä¸­å¼ºâ†’å¼±
            "mp": "p",    # ä¸­å¼±â†’å¼±
            "p": "pp",    # å¼±â†’æå¼±
            "pp": "pp"    # ä¿æŒæå¼±
        }
        return dynamics_transitions.get(current_dynamics, "p")
    
    def _apply_sleep_therapy_adaptation(self, music_params: MusicParameter, therapy_context: Dict[str, Any]) -> MusicParameter:
        """åº”ç”¨ç¡çœ æ²»ç–—é€‚åº”ï¼ˆå•é˜¶æ®µç‰ˆæœ¬ï¼Œä¿æŒå‘åå…¼å®¹ï¼‰"""
        if not self.config.sleep_therapy_mode:
            return music_params
        
        # æ ¹æ®æ²»ç–—é˜¶æ®µè°ƒæ•´å‚æ•°
        iso_stage = therapy_context.get('iso_stage', 'synchronization')
        
        if iso_stage == 'synchronization':
            # åŒæ­¥é˜¶æ®µï¼šä¸å½“å‰æƒ…ç»ªçŠ¶æ€åŒ¹é…
            pass  # ä¿æŒåŸå‚æ•°
        elif iso_stage == 'guidance':
            # å¼•å¯¼é˜¶æ®µï¼šé€æ­¥å‘ç›®æ ‡çŠ¶æ€è¿‡æ¸¡
            music_params.tempo_bpm = max(50.0, music_params.tempo_bpm * 0.9)
            music_params.therapy_intensity = min(1.0, music_params.therapy_intensity * 1.1)
        elif iso_stage == 'consolidation':
            # å·©å›ºé˜¶æ®µï¼šç¨³å®šåœ¨ç›®æ ‡çŠ¶æ€
            music_params.tempo_bpm = max(45.0, music_params.tempo_bpm * 0.8)
            music_params.sleep_readiness = min(1.0, music_params.sleep_readiness * 1.2)
        
        # æ˜¼å¤œèŠ‚å¾‹é€‚åº”
        if self.config.circadian_adaptation:
            circadian_phase = therapy_context.get('circadian_phase', 'evening')
            if circadian_phase == 'evening':
                music_params.tempo_bpm = max(45.0, music_params.tempo_bpm * 0.9)
                music_params.sleep_readiness = min(1.0, music_params.sleep_readiness * 1.1)
        
        return music_params
    
    async def _process_impl(self, input_data: LayerData) -> LayerData:
        """æ˜ å°„å±‚å¤„ç†å®ç°"""
        self.performance_monitor.start_timer("mapping_layer_processing")
        
        try:
            # éªŒè¯è¾“å…¥æ•°æ®
            if not input_data.data:
                raise ValueError("è¾“å…¥æ•°æ®ä¸ºç©º")
            
            # æå–æƒ…ç»ªå‘é‡
            emotion_vector = self._extract_emotion_vector(input_data.data)
            
            # æå–æ²»ç–—ä¸Šä¸‹æ–‡
            therapy_context = self._extract_therapy_context(input_data.data)
            
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨ISOä¸‰é˜¶æ®µæ¨¡å¼
            use_iso_three_stages = therapy_context.get('enable_iso_three_stages', True)
            
            if use_iso_three_stages and self.config.sleep_therapy_mode:
                # ğŸµ ISOä¸‰é˜¶æ®µéŸ³ä¹å‚æ•°ç”Ÿæˆ
                iso_params = self._generate_iso_three_stages(emotion_vector, therapy_context)
                
                # è®¡ç®—ä¸‰é˜¶æ®µæ˜ å°„ç½®ä¿¡åº¦
                match_confidence = self._calculate_mapping_confidence(emotion_vector, iso_params.match_stage)
                guide_confidence = self._calculate_mapping_confidence(emotion_vector, iso_params.guide_stage) 
                target_confidence = self._calculate_mapping_confidence(emotion_vector, iso_params.target_stage)
                mapping_confidence = (match_confidence + guide_confidence + target_confidence) / 3.0
                
                # åˆ›å»ºISOä¸‰é˜¶æ®µè¾“å‡ºæ•°æ®
                output_data = LayerData(
                    layer_name=self.layer_name,
                    timestamp=datetime.now(),
                    data={
                        'iso_three_stage_params': iso_params.to_dict(),
                        'mapping_info': {
                            'strategy': self.config.mapping_strategy,
                            'kg_weight': self.kg_weight,
                            'mlp_weight': self.mlp_weight,
                            'therapy_context': therapy_context,
                            'mapping_confidence': mapping_confidence,
                            'stage_confidences': {
                                'match': match_confidence,
                                'guide': guide_confidence,
                                'target': target_confidence
                            },
                            'iso_mode': True
                        },
                        'emotion_vector': emotion_vector.cpu().numpy().tolist(),
                        'processing_info': {
                            'kg_enabled': self.config.kg_enabled,
                            'mlp_enabled': self.config.mlp_enabled,
                            'sleep_therapy_mode': self.config.sleep_therapy_mode,
                            'iso_three_stages_enabled': True,
                            'total_therapy_duration': 15.0  # 2+6+7åˆ†é’Ÿ
                        }
                    },
                    metadata={
                        'source_layer': input_data.layer_name,
                        'processing_device': str(self.device),
                        'mapping_strategy': self.config.mapping_strategy,
                        'output_format': 'iso_three_stages'
                    },
                    confidence=mapping_confidence
                )
            else:
                # ä¼ ç»Ÿå•é˜¶æ®µæ˜ å°„ï¼ˆå‘åå…¼å®¹ï¼‰
                if self.config.mapping_strategy == MappingStrategy.KNOWLEDGE_GRAPH.value:
                    music_params = self._kg_mapping(emotion_vector)
                elif self.config.mapping_strategy == MappingStrategy.DEEP_LEARNING.value:
                    music_params = self._mlp_mapping(emotion_vector, therapy_context)
                elif self.config.mapping_strategy == MappingStrategy.HYBRID_FUSION.value:
                    kg_params = self._kg_mapping(emotion_vector)
                    mlp_params = self._mlp_mapping(emotion_vector, therapy_context)
                    music_params = self._hybrid_fusion(kg_params, mlp_params)
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„æ˜ å°„ç­–ç•¥: {self.config.mapping_strategy}")
                
                # åº”ç”¨ç¡çœ æ²»ç–—é€‚åº”
                music_params = self._apply_sleep_therapy_adaptation(music_params, therapy_context)
                
                # è®¡ç®—æ˜ å°„ç½®ä¿¡åº¦
                mapping_confidence = self._calculate_mapping_confidence(emotion_vector, music_params)
                
                # åˆ›å»ºå•é˜¶æ®µè¾“å‡ºæ•°æ®
                output_data = LayerData(
                    layer_name=self.layer_name,
                    timestamp=datetime.now(),
                    data={
                        'music_parameters': music_params.to_dict(),
                        'mapping_info': {
                            'strategy': self.config.mapping_strategy,
                            'kg_weight': self.kg_weight,
                            'mlp_weight': self.mlp_weight,
                            'therapy_context': therapy_context,
                            'mapping_confidence': mapping_confidence,
                            'iso_mode': False
                        },
                        'emotion_vector': emotion_vector.cpu().numpy().tolist(),
                        'processing_info': {
                            'kg_enabled': self.config.kg_enabled,
                            'mlp_enabled': self.config.mlp_enabled,
                            'sleep_therapy_mode': self.config.sleep_therapy_mode,
                            'iso_three_stages_enabled': False
                        }
                    },
                    metadata={
                        'source_layer': input_data.layer_name,
                        'processing_device': str(self.device),
                        'mapping_strategy': self.config.mapping_strategy,
                        'output_format': 'single_stage'
                    },
                    confidence=mapping_confidence
                )
            
            # è®°å½•å¤„ç†æ—¶é—´
            processing_time = self.performance_monitor.end_timer("mapping_layer_processing")
            output_data.processing_time = processing_time
            
            # è®°å½•ç»Ÿè®¡ä¿¡æ¯
            self.total_processed += 1
            self.total_processing_time += processing_time
            
            # æ—¥å¿—è®°å½•ï¼ˆåŒºåˆ†ISOä¸‰é˜¶æ®µæ¨¡å¼å’Œå•é˜¶æ®µæ¨¡å¼ï¼‰
            if use_iso_three_stages and self.config.sleep_therapy_mode:
                logger.info(f"æ˜ å°„å±‚å¤„ç†å®Œæˆ (ISOä¸‰é˜¶æ®µ) - "
                           f"åŒ¹é…: {iso_params.match_stage.tempo_bpm:.1f}BPM â†’ "
                           f"å¼•å¯¼: {iso_params.guide_stage.tempo_bpm:.1f}BPM â†’ "
                           f"ç›®æ ‡: {iso_params.target_stage.tempo_bpm:.1f}BPM, "
                           f"ç½®ä¿¡åº¦: {mapping_confidence:.3f}, è€—æ—¶: {processing_time*1000:.1f}ms")
            else:
                logger.info(f"æ˜ å°„å±‚å¤„ç†å®Œæˆ - èŠ‚æ‹: {music_params.tempo_bpm:.1f}BPM, "
                           f"è°ƒæ€§: {music_params.key_signature}, ç½®ä¿¡åº¦: {mapping_confidence:.3f}, "
                           f"è€—æ—¶: {processing_time*1000:.1f}ms")
            
            return output_data
            
        except Exception as e:
            self.error_count += 1
            logger.error(f"æ˜ å°„å±‚å¤„ç†å¤±è´¥: {e}")
            
            # åˆ›å»ºé”™è¯¯è¾“å‡º
            error_data = LayerData(
                layer_name=self.layer_name,
                timestamp=datetime.now(),
                data={
                    'error': str(e),
                    'music_parameters': MusicParameter().to_dict(),  # é»˜è®¤å‚æ•°
                    'mapping_info': {
                        'strategy': self.config.mapping_strategy,
                        'error': True
                    }
                },
                metadata={'error': True, 'source_layer': input_data.layer_name},
                confidence=0.0
            )
            
            processing_time = self.performance_monitor.end_timer("mapping_layer_processing")
            error_data.processing_time = processing_time
            
            return error_data
    
    def _calculate_mapping_confidence(self, emotion_vector: torch.Tensor, music_params: MusicParameter) -> float:
        """è®¡ç®—æ˜ å°„ç½®ä¿¡åº¦"""
        # åŸºäºæƒ…ç»ªå‘é‡çš„ç¡®å®šæ€§
        emotion_entropy = -torch.sum(emotion_vector * torch.log(emotion_vector + 1e-8)).item()
        emotion_confidence = 1.0 / (1.0 + emotion_entropy)
        
        # åŸºäºéŸ³ä¹å‚æ•°çš„åˆç†æ€§
        param_confidence = 1.0
        
        # æ£€æŸ¥æ˜¯å¦æœ‰tempo_bpmå±æ€§ï¼ˆå…¼å®¹MusicParameterå’ŒISOThreeStageParamsï¼‰
        if hasattr(music_params, 'tempo_bpm'):
            if music_params.tempo_bpm < 30 or music_params.tempo_bpm > 150:
                param_confidence *= 0.8
            if hasattr(music_params, 'therapy_intensity') and (music_params.therapy_intensity < 0.1 or music_params.therapy_intensity > 1.0):
                param_confidence *= 0.9
        
        # ç»¼åˆç½®ä¿¡åº¦
        overall_confidence = (emotion_confidence * 0.6 + param_confidence * 0.4)
        
        return float(np.clip(overall_confidence, 0.0, 1.0))
    
    # ==================== æ ‡å‡†åŒ–æ¥å£ï¼ˆç”¨æˆ·è§„èŒƒï¼‰ ====================
    
    def get_kg_initial_mapping(self, emotion_vector: torch.Tensor) -> Dict[str, float]:
        """
        çŸ¥è¯†å›¾è°±åˆå§‹æ˜ å°„ - åŸºäºGEMSåŸç†çš„è§„åˆ™é›†
        è¾“å…¥: 27ç»´æƒ…ç»ªå‘é‡ [0.0-1.0]
        è¾“å‡º: 8ä¸ªæ ‡å‡†éŸ³ä¹å‚æ•° [æ ‡å‡†åŒ–æ•°å€¼]
        """
        # å°†tensorè½¬ä¸ºnumpyä¾¿äºå¤„ç†
        if isinstance(emotion_vector, torch.Tensor):
            emotion_values = emotion_vector.cpu().numpy()
        else:
            emotion_values = np.array(emotion_vector)
        
        # æ‰¾åˆ°ä¸»å¯¼æƒ…ç»ªï¼ˆæœ€é«˜å¼ºåº¦ï¼‰
        dominant_emotion_idx = np.argmax(emotion_values)
        dominant_intensity = emotion_values[dominant_emotion_idx]
        
        # åŸºäºGEMSåŸç†çš„éŸ³ä¹æ˜ å°„è§„åˆ™ï¼ˆç®€åŒ–ç‰ˆï¼‰
        # 27ç»´æƒ…ç»ªæ˜ å°„ï¼š0-8åŸºç¡€æƒ…ç»ªï¼Œ9-26ç¡çœ ç‰¹åŒ–æƒ…ç»ª
        
        # é»˜è®¤å‚æ•°ï¼ˆä¸­æ€§çŠ¶æ€ï¼‰
        kg_params = {
            'tempo': 0.5,           # 0.0-1.0 æ˜ å°„åˆ° 60-120 BPM
            'mode': 0.0,            # 0.0=å¤§è°ƒ, 1.0=å°è°ƒ
            'dynamics': 0.5,        # 0.0-1.0 éŸ³é‡å¼ºåº¦
            'harmony_consonance': 0.7,  # 0.0-1.0 å’Œå£°åå’Œåº¦
            'timbre_preference': 0.5,   # 0.0-1.0 éŸ³è‰²åå¥½
            'pitch_register': 0.5,      # 0.0-1.0 éŸ³é«˜éŸ³åŸŸ
            'density': 0.5,         # 0.0-1.0 å¯†åº¦
            'emotional_envelope_direction': 0.0  # -1.0åˆ°1.0 æƒ…ç»ªæ–¹å‘
        }
        
        # GEMSåŸç†è§„åˆ™é›†
        if dominant_emotion_idx == 1:  # fear_anxiety (ç„¦è™‘)
            kg_params.update({
                'tempo': 0.7 + dominant_intensity * 0.2,  # è¾ƒå¿«èŠ‚æ‹
                'mode': 0.8,                              # åå°è°ƒ
                'dynamics': 0.3,                          # è¾ƒå¼±åŠ›åº¦ï¼ˆç¡çœ æ²»ç–—ï¼‰
                'harmony_consonance': 0.3,                # è¾ƒä¸åå’Œ
                'timbre_preference': 0.2,                 # æŸ”å’ŒéŸ³è‰²
                'pitch_register': 0.6,                    # ä¸­é«˜éŸ³åŸŸ
                'density': 0.3,                           # ä½å¯†åº¦
                'emotional_envelope_direction': -0.8      # ä¸‹é™è¶‹åŠ¿
            })
        elif dominant_emotion_idx >= 13 and dominant_emotion_idx <= 15:  # hyperarousal (è¿‡åº¦è§‰é†’)
            kg_params.update({
                'tempo': 0.8 + dominant_intensity * 0.1,  # å¿«èŠ‚æ‹
                'mode': 0.0,                              # å¤§è°ƒ
                'dynamics': 0.2,                          # å¾ˆå¼±åŠ›åº¦
                'harmony_consonance': 0.5,                # ä¸­ç­‰åå’Œ
                'timbre_preference': 0.1,                 # ææŸ”å’Œ
                'pitch_register': 0.3,                    # ä½éŸ³åŸŸ
                'density': 0.2,                           # æä½å¯†åº¦
                'emotional_envelope_direction': -0.9      # å¼ºçƒˆä¸‹é™
            })
        elif dominant_emotion_idx >= 18 and dominant_emotion_idx <= 20:  # peaceful (å¹³é™)
            kg_params.update({
                'tempo': 0.2,                             # å¾ˆæ…¢èŠ‚æ‹
                'mode': 0.0,                              # å¤§è°ƒ
                'dynamics': 0.1,                          # æå¼±åŠ›åº¦
                'harmony_consonance': 0.9,                # é«˜åº¦åå’Œ
                'timbre_preference': 0.0,                 # æœ€æŸ”å’ŒéŸ³è‰²
                'pitch_register': 0.2,                    # ä½éŸ³åŸŸ
                'density': 0.1,                           # æä½å¯†åº¦
                'emotional_envelope_direction': 0.1       # å¾®ä¸Šå‡ï¼ˆç§¯æï¼‰
            })
        elif dominant_emotion_idx >= 9 and dominant_emotion_idx <= 12:  # sleep_anxiety (ç¡çœ ç„¦è™‘)
            kg_params.update({
                'tempo': 0.4,                             # ä¸­æ…¢èŠ‚æ‹
                'mode': 0.6,                              # åå°è°ƒ
                'dynamics': 0.2,                          # å¼±åŠ›åº¦
                'harmony_consonance': 0.6,                # è¾ƒåå’Œ
                'timbre_preference': 0.3,                 # æŸ”å’ŒéŸ³è‰²
                'pitch_register': 0.4,                    # ä¸­ä½éŸ³åŸŸ
                'density': 0.3,                           # ä½å¯†åº¦
                'emotional_envelope_direction': -0.6      # ä¸‹é™è¶‹åŠ¿
            })
        
        # æ ¹æ®ç¡çœ æ²»ç–—æ¨¡å¼è°ƒæ•´
        if self.config.sleep_therapy_mode:
            kg_params['tempo'] = min(kg_params['tempo'], 0.6)  # é™åˆ¶æœ€å¤§èŠ‚æ‹
            kg_params['dynamics'] = min(kg_params['dynamics'], 0.3)  # é™åˆ¶éŸ³é‡
            kg_params['emotional_envelope_direction'] = min(kg_params['emotional_envelope_direction'], 0.0)  # åå‘ä¸‹é™
        
        return kg_params
    
    def apply_mlp_personalization(self, kg_parameters: Dict[str, float], 
                                  emotion_vector: torch.Tensor, 
                                  user_profile_data: Optional[Dict[str, Any]] = None) -> Dict[str, float]:
        """
        MLPä¸ªæ€§åŒ–å¾®è°ƒæœºåˆ¶
        è¾“å…¥: KGåˆå§‹å‚æ•° + æƒ…ç»ªå‘é‡ + ç”¨æˆ·åå¥½æ•°æ®
        è¾“å‡º: ä¸ªæ€§åŒ–è°ƒæ•´åçš„éŸ³ä¹å‚æ•°
        """
        personalized_params = kg_parameters.copy()
        
        # æ¨¡æ‹Ÿç”¨æˆ·åå¥½æ•°æ®ï¼ˆå¦‚æœæ²¡æœ‰æä¾›ï¼‰
        if user_profile_data is None:
            user_profile_data = {
                'tempo_preference': 0.0,      # -0.2åˆ°0.2çš„åå¥½è°ƒæ•´
                'mode_preference': 0.0,       # è°ƒå¼åå¥½
                'volume_sensitivity': 1.0,    # éŸ³é‡æ•æ„Ÿåº¦
                'harmony_preference': 0.0,    # å’Œå£°åå¥½
                'therapy_responsiveness': 1.0 # æ²»ç–—å“åº”åº¦
            }
        
        # ä¸ªæ€§åŒ–è°ƒæ•´é€»è¾‘
        
        # 1. èŠ‚æ‹ä¸ªæ€§åŒ–ï¼šåŸºäºç”¨æˆ·å†å²åå¥½
        tempo_adjustment = user_profile_data.get('tempo_preference', 0.0)
        personalized_params['tempo'] = np.clip(
            personalized_params['tempo'] + tempo_adjustment, 0.0, 1.0
        )
        
        # 2. è°ƒå¼ä¸ªæ€§åŒ–ï¼šæ ¹æ®ç”¨æˆ·æƒ…ç»ªååº”æ¨¡å¼
        if user_profile_data.get('mode_preference', 0.0) != 0.0:
            mode_shift = user_profile_data['mode_preference'] * 0.1
            personalized_params['mode'] = np.clip(
                personalized_params['mode'] + mode_shift, 0.0, 1.0
            )
        
        # 3. åŠ¨æ€ä¸ªæ€§åŒ–ï¼šåŸºäºæƒ…ç»ªå¼ºåº¦å’Œç”¨æˆ·æ•æ„Ÿåº¦
        emotion_intensity = torch.max(emotion_vector).item()
        volume_sensitivity = user_profile_data.get('volume_sensitivity', 1.0)
        
        if emotion_intensity > 0.7:  # é«˜å¼ºåº¦æƒ…ç»ª
            personalized_params['dynamics'] *= (0.8 * volume_sensitivity)
        
        # 4. å’Œå£°ä¸ªæ€§åŒ–ï¼šæ ¹æ®ç”¨æˆ·éŸ³ä¹èƒŒæ™¯
        harmony_pref = user_profile_data.get('harmony_preference', 0.0)
        personalized_params['harmony_consonance'] = np.clip(
            personalized_params['harmony_consonance'] + harmony_pref * 0.2, 0.0, 1.0
        )
        
        # 5. æ²»ç–—å“åº”åº¦è°ƒæ•´ï¼šæ ¹æ®ç”¨æˆ·æ²»ç–—æ•ˆæœå†å²
        therapy_responsiveness = user_profile_data.get('therapy_responsiveness', 1.0)
        if therapy_responsiveness > 1.0:  # å“åº”åº¦é«˜çš„ç”¨æˆ·
            personalized_params['emotional_envelope_direction'] *= 1.2
            personalized_params['emotional_envelope_direction'] = np.clip(
                personalized_params['emotional_envelope_direction'], -1.0, 1.0
            )
        
        # 6. æ˜¼å¤œèŠ‚å¾‹è°ƒæ•´
        current_hour = datetime.now().hour
        if 22 <= current_hour or current_hour <= 6:  # æ·±å¤œ/å‡Œæ™¨
            personalized_params['tempo'] *= 0.9
            personalized_params['dynamics'] *= 0.8
        
        return personalized_params
    
    def map_emotion_to_music(self, emotion_vector: torch.Tensor, 
                           user_profile_data: Optional[Dict[str, Any]] = None) -> Dict[str, float]:
        """
        ä¸»æ˜ å°„å‡½æ•°ï¼šæƒ…ç»ªå‘é‡ â†’ æ ‡å‡†åŒ–éŸ³ä¹å‚æ•°
        
        Args:
            emotion_vector: 27ç»´æƒ…ç»ªå‘é‡ [0.0-1.0]
            user_profile_data: ç”¨æˆ·åå¥½æ•°æ®ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            æ ‡å‡†åŒ–éŸ³ä¹å‚æ•°å­—å…¸ï¼ŒåŒ…å«8ä¸ªæ ‡å‡†å‚æ•°
        """
        # é˜¶æ®µ1ï¼šçŸ¥è¯†å›¾è°±åˆå§‹æ˜ å°„
        kg_parameters = self.get_kg_initial_mapping(emotion_vector)
        
        # é˜¶æ®µ2ï¼šMLPä¸ªæ€§åŒ–å¾®è°ƒ
        final_parameters = self.apply_mlp_personalization(
            kg_parameters, emotion_vector, user_profile_data
        )
        
        # ç¡®ä¿æ‰€æœ‰å‚æ•°åœ¨æœ‰æ•ˆèŒƒå›´å†…
        for key, value in final_parameters.items():
            if key == 'emotional_envelope_direction':
                final_parameters[key] = np.clip(value, -1.0, 1.0)
            else:
                final_parameters[key] = np.clip(value, 0.0, 1.0)
        
        return final_parameters
    
    def convert_to_detailed_params(self, standard_params: Dict[str, float]) -> MusicParameter:
        """
        å·¥å…·å‡½æ•°ï¼šå°†æ ‡å‡†åŒ–å‚æ•°è½¬æ¢ä¸ºè¯¦ç»†çš„MusicParameterå¯¹è±¡
        ç”¨äºä¸ç°æœ‰ISOä¸‰é˜¶æ®µåŠŸèƒ½å…¼å®¹
        """
        music_param = MusicParameter()
        
        # è½¬æ¢åŸºç¡€å‚æ•°
        music_param.tempo_bpm = 60.0 + standard_params['tempo'] * 60.0  # 60-120 BPM
        music_param.key_signature = "C_major" if standard_params['mode'] < 0.5 else "A_minor"
        
        # åŠ›åº¦æ˜ å°„
        dynamics_map = ["pp", "p", "mp", "mf"]
        dynamics_idx = int(standard_params['dynamics'] * 3.99)
        music_param.dynamics = dynamics_map[min(dynamics_idx, 3)]
        
        # æƒ…ç»ªæ˜ å°„
        music_param.valence_mapping = 1.0 - standard_params['mode']  # å¤§è°ƒ=æ­£æ•ˆä»·
        music_param.arousal_mapping = standard_params['tempo'] * 2.0 - 1.0  # -1åˆ°1
        music_param.tension_level = 1.0 - standard_params['harmony_consonance']
        
        # ç»‡ä½“å’Œå¯†åº¦
        music_param.texture_complexity = standard_params['density']
        music_param.harmonic_richness = standard_params['harmony_consonance']
        
        return music_param
    
    # ==================== åŸæœ‰åŠŸèƒ½ä¿æŒä¸å˜ ====================
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–æ˜ å°„å±‚çŠ¶æ€"""
        base_status = super().get_status()
        
        # æ·»åŠ æ˜ å°„å±‚ç‰¹æœ‰çš„çŠ¶æ€ä¿¡æ¯
        mapping_status = {
            'mapping_strategy': self.config.mapping_strategy,
            'kg_enabled': self.config.kg_enabled,
            'mlp_enabled': self.config.mlp_enabled,
            'device': str(self.device),
            'gpu_available': torch.cuda.is_available(),
            'sleep_therapy_mode': self.config.sleep_therapy_mode,
            'kg_weight': self.kg_weight,
            'mlp_weight': self.mlp_weight,
            'performance_stats': self.performance_monitor.get_all_stats()
        }
        
        base_status.update(mapping_status)
        return base_status