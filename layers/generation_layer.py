#!/usr/bin/env python3
"""
ç”Ÿæˆå±‚ (Generation Layer) - Layer 4

å®æ—¶éŸ³è§†é¢‘å†…å®¹ç”Ÿæˆï¼Œæ ¸å¿ƒåŠŸèƒ½åŒ…æ‹¬ï¼š
1. éŸ³ä¹åˆæˆå’Œç”Ÿæˆ
2. æ²»ç–—æ€§è§†é¢‘å†…å®¹ç”Ÿæˆ
3. å®æ—¶è‡ªé€‚åº”ç”Ÿæˆ
4. å¤šç§ç”Ÿæˆç­–ç•¥æ”¯æŒ
5. ç¡çœ æ²»ç–—ä¸“ç”¨ä¼˜åŒ–

å¤„ç†æµç¨‹ï¼š
Mapping Layer â†’ Generation Layer â†’ Rendering Layer
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Tuple, Optional, Any, Union
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
import asyncio
import logging
import json
import io
import requests
import time
from pathlib import Path

from .base_layer import BaseLayer, LayerData, LayerConfig
from core.utils import (
    ConfigLoader, DataValidator, PerformanceMonitor, 
    get_project_root, normalize_vector
)
from core.theory.iso_principle import ISOPrinciple, ISOStage, EmotionState

logger = logging.getLogger(__name__)

# æ£€æŸ¥å¯é€‰ä¾èµ–
try:
    import librosa
    import soundfile as sf
    AUDIO_AVAILABLE = True
except ImportError:
    AUDIO_AVAILABLE = False
    logger.warning("librosa/soundfileä¸å¯ç”¨ï¼ŒéŸ³é¢‘ç”ŸæˆåŠŸèƒ½å°†å—é™")

try:
    import PIL.Image
    import PIL.ImageDraw
    import PIL.ImageFont
    VIDEO_AVAILABLE = True
except ImportError:
    VIDEO_AVAILABLE = False
    logger.warning("PILä¸å¯ç”¨ï¼Œè§†é¢‘ç”ŸæˆåŠŸèƒ½å°†å—é™")

class GenerationStrategy(Enum):
    """ç”Ÿæˆç­–ç•¥æšä¸¾"""
    RULE_BASED = "rule_based"          # åŸºäºè§„åˆ™çš„ç”Ÿæˆ
    NEURAL_SYNTHESIS = "neural_synthesis"  # ç¥ç»ç½‘ç»œåˆæˆ
    TEMPLATE_BASED = "template_based"      # åŸºäºæ¨¡æ¿çš„ç”Ÿæˆ
    HYBRID = "hybrid"                      # æ··åˆç­–ç•¥

class ContentType(Enum):
    """å†…å®¹ç±»å‹æšä¸¾"""
    AUDIO = "audio"
    VIDEO = "video"
    BOTH = "both"

@dataclass
class GenerationLayerConfig(LayerConfig):
    """ç”Ÿæˆå±‚é…ç½®"""
    # åŸºç¡€é…ç½®
    output_sample_rate: int = 44100  # éŸ³é¢‘é‡‡æ ·ç‡
    output_channels: int = 2         # ç«‹ä½“å£°
    video_fps: int = 30              # è§†é¢‘å¸§ç‡
    video_resolution: Tuple[int, int] = (1920, 1080)  # è§†é¢‘åˆ†è¾¨ç‡
    
    # ç”Ÿæˆç­–ç•¥
    generation_strategy: str = "hybrid"
    content_type: str = "both"  # audio, video, both
    
    # éŸ³é¢‘ç”Ÿæˆé…ç½®
    audio_enabled: bool = True
    audio_duration: float = 60.0     # é»˜è®¤1åˆ†é’Ÿ
    audio_buffer_size: int = 4096
    audio_synthesis_method: str = "procedural"  # procedural, neural
    
    # è§†é¢‘ç”Ÿæˆé…ç½®
    video_enabled: bool = True
    video_duration: float = 60.0     # é»˜è®¤1åˆ†é’Ÿ
    video_style: str = "ambient"     # ambient, abstract, nature
    
    # æ²»ç–—ç‰¹åŒ–é…ç½®
    therapy_optimized: bool = True
    iso_stage_aware: bool = True
    binaural_beats: bool = True
    
    # æ€§èƒ½é…ç½®
    use_gpu: bool = True
    batch_size: int = 1
    max_processing_time: float = 200.0  # ms
    
    # ç¼“å­˜é…ç½®
    enable_caching: bool = True
    cache_size: int = 100

class MusicParameter:
    """éŸ³ä¹å‚æ•°ï¼ˆä»mapping_layerå¯¼å…¥çš„æ•°æ®ç»“æ„ï¼‰"""
    def __init__(self, data: Dict[str, Any] = None):
        if data is None:
            data = {}
        
        # åŸºç¡€éŸ³ä¹å‚æ•°
        self.tempo_bpm = data.get('tempo_bpm', 60.0)
        self.key_signature = data.get('key_signature', 'C_major')
        self.time_signature = data.get('time_signature', (4, 4))
        self.dynamics = data.get('dynamics', 'mp')
        
        # éŸ³è‰²å’Œç»‡ä½“
        self.instrument_weights = data.get('instrument_weights', {})
        self.texture_complexity = data.get('texture_complexity', 0.5)
        self.harmonic_richness = data.get('harmonic_richness', 0.5)
        
        # æƒ…ç»ªè¡¨è¾¾
        self.valence_mapping = data.get('valence_mapping', 0.0)
        self.arousal_mapping = data.get('arousal_mapping', 0.0)
        self.tension_level = data.get('tension_level', 0.0)
        
        # æ²»ç–—ç‰¹åŒ–
        self.iso_stage = data.get('iso_stage', 'synchronization')
        self.therapy_intensity = data.get('therapy_intensity', 0.5)
        self.sleep_phase_alignment = data.get('sleep_phase_alignment', 0.0)

class SunoAPIClient:
    """Suno APIå®¢æˆ·ç«¯ - ç”¨äºçœŸå®éŸ³ä¹ç”Ÿæˆ"""
    
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or "your-suno-api-key"  # éœ€è¦ç”¨æˆ·æä¾›çœŸå®å¯†é’¥
        self.base_url = "https://api.suno.ai/v1"  # Suno API endpoint
        self.session = requests.Session()
        
        # è®¾ç½®APIè¯·æ±‚å¤´
        self.session.headers.update({
            'Authorization': f'Bearer {self.api_key}',
            'Content-Type': 'application/json',
            'User-Agent': 'qm_final3-music-therapy/1.0'
        })
        
        logger.info("Suno APIå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
    
    def generate_three_stage_music(self, emotion_data: Dict[str, Any], 
                                 music_params: MusicParameter) -> Dict[str, Any]:
        """ç”Ÿæˆä¸‰é˜¶æ®µè¿è´¯éŸ³ä¹å™äº‹"""
        try:
            # æ„å»ºä¸‰é˜¶æ®µæç¤ºè¯
            stage_prompts = self._build_three_stage_prompts(emotion_data, music_params)
            
            # è°ƒç”¨Suno APIç”Ÿæˆä¸‰æ®µè¿è´¯éŸ³ä¹
            music_response = self._call_suno_api(stage_prompts)
            
            # å¤„ç†å“åº”
            if music_response and 'audio_url' in music_response:
                # ä¸‹è½½éŸ³é¢‘æ–‡ä»¶
                audio_data = self._download_audio(music_response['audio_url'])
                
                return {
                    'success': True,
                    'audio_data': audio_data,
                    'stage_prompts': stage_prompts,
                    'suno_response': music_response,
                    'three_stage_narrative': True
                }
            else:
                # å¦‚æœAPIè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨fallbackç”Ÿæˆ
                return self._fallback_generation(emotion_data, music_params)
                
        except Exception as e:
            logger.error(f"Suno APIè°ƒç”¨å¤±è´¥: {e}")
            return self._fallback_generation(emotion_data, music_params)
    
    def _build_three_stage_prompts(self, emotion_data: Dict[str, Any], 
                                 music_params: MusicParameter) -> Dict[str, str]:
        """æ„å»ºä¸‰é˜¶æ®µéŸ³ä¹ç”Ÿæˆæç¤ºè¯"""
        # è·å–ç”¨æˆ·å½“å‰æƒ…ç»ª
        current_emotion = emotion_data.get('primary_emotion', {}).get('name', 'ç„¦è™‘')
        
        # æ˜ å°„æƒ…ç»ªåˆ°éŸ³ä¹æè¿°
        emotion_to_music = {
            'ç„¦è™‘': {'initial': 'anxious, restless, fast tempo', 'target': 'calm, peaceful, slow'},
            'ç–²æƒ«': {'initial': 'tired, heavy, sluggish', 'target': 'relaxed, floating, gentle'},
            'ä¸­æ€§': {'initial': 'neutral, balanced, moderate', 'target': 'serene, tranquil, soft'},
            'å¤±çœ ': {'initial': 'racing thoughts, tense, irregular', 'target': 'sleepy, drowsy, minimal'},
            'çƒ¦èº': {'initial': 'agitated, sharp, dissonant', 'target': 'smooth, harmonious, flowing'}
        }
        
        music_desc = emotion_to_music.get(current_emotion, emotion_to_music['ç„¦è™‘'])
        
        # æ„å»ºä¸‰é˜¶æ®µæç¤ºè¯
        stage_prompts = {
            'synchronization': f"""Create ambient sleep therapy music - Stage 1 (Synchronization): 
Match the user's current emotional state of {current_emotion}. 
Musical style: {music_desc['initial']}, {music_params.tempo_bpm} BPM, 
{music_params.key_signature} key. Duration: 30 seconds. 
Focus: emotional resonance and matching current mood.""",
            
            'guidance': f"""Create ambient sleep therapy music - Stage 2 (Guidance): 
Gradually transition from {current_emotion} towards calm relaxation. 
Musical progression: slowly decrease tempo from {music_params.tempo_bpm} to {max(40, music_params.tempo_bpm-20)} BPM,
softer dynamics, gentler textures. Duration: 60 seconds.
Focus: smooth emotional transition and guidance.""",
            
            'consolidation': f"""Create ambient sleep therapy music - Stage 3 (Consolidation): 
Establish deep relaxation and sleepiness. 
Musical style: {music_desc['target']}, very slow tempo (30-40 BPM), 
minimal textures, sleep-inducing harmonies. Duration: 30 seconds.
Focus: consolidate peaceful state for sleep."""
        }
        
        return stage_prompts
    
    def _call_suno_api(self, stage_prompts: Dict[str, str]) -> Optional[Dict[str, Any]]:
        """è°ƒç”¨Suno APIç”ŸæˆéŸ³ä¹"""
        try:
            # ç»„åˆä¸‰é˜¶æ®µæç¤ºä¸ºä¸€ä¸ªè¿è´¯çš„éŸ³ä¹è¯·æ±‚
            combined_prompt = f"""
{stage_prompts['synchronization']}

Then smoothly transition to:
{stage_prompts['guidance']}

Finally conclude with:
{stage_prompts['consolidation']}

Create this as one continuous, seamless 2-minute ambient therapy track for sleep induction.
"""
            
            # æ„å»ºAPIè¯·æ±‚
            request_data = {
                "prompt": combined_prompt,
                "duration": 120,  # 2åˆ†é’Ÿæ€»æ—¶é•¿
                "style": "ambient therapy",
                "mood": "calming progressive",
                "format": "wav"
            }
            
            # å‘é€è¯·æ±‚ï¼ˆæ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨æ¨¡æ‹Ÿå“åº”ï¼Œå®é™…éœ€è¦çœŸå®APIï¼‰
            response = self._simulate_suno_response(request_data)
            
            return response
            
        except Exception as e:
            logger.error(f"Suno APIè¯·æ±‚å¤±è´¥: {e}")
            return None
    
    def _simulate_suno_response(self, request_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ¨¡æ‹ŸSuno APIå“åº”ï¼ˆå¾…æ›¿æ¢ä¸ºçœŸå®APIè°ƒç”¨ï¼‰"""
        # æ¨¡æ‹ŸAPIå“åº”
        return {
            'id': f'suno_track_{int(time.time())}',
            'status': 'completed',
            'audio_url': 'https://example.com/generated_track.wav',  # æ¨¡æ‹ŸURL
            'duration': request_data.get('duration', 120),
            'prompt_used': request_data.get('prompt', ''),
            'metadata': {
                'style': request_data.get('style', 'ambient'),
                'format': request_data.get('format', 'wav'),
                'generated_at': datetime.now().isoformat()
            }
        }
    
    def _download_audio(self, audio_url: str) -> Optional[np.ndarray]:
        """ä¸‹è½½ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶"""
        try:
            # å¯¹äºæ¨¡æ‹ŸURLï¼Œç”Ÿæˆæ¨¡æ‹ŸéŸ³é¢‘æ•°æ®
            if 'example.com' in audio_url:
                return self._generate_mock_audio()
            
            # çœŸå®ä¸‹è½½é€»è¾‘ï¼ˆå®é™…ä½¿ç”¨æ—¶å¯ç”¨ï¼‰
            # response = self.session.get(audio_url)
            # if response.status_code == 200:
            #     audio_data = response.content
            #     # ä½¿ç”¨librosaæˆ–soundfileè§£æéŸ³é¢‘
            #     return audio_array
            
            return self._generate_mock_audio()
            
        except Exception as e:
            logger.error(f"éŸ³é¢‘ä¸‹è½½å¤±è´¥: {e}")
            return None
    
    def _generate_mock_audio(self) -> np.ndarray:
        """ç”Ÿæˆæ¨¡æ‹ŸéŸ³é¢‘æ•°æ®ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        # ç”Ÿæˆ2åˆ†é’Ÿçš„ä¸‰é˜¶æ®µæ¨¡æ‹ŸéŸ³é¢‘
        sample_rate = 44100
        duration = 120  # 2åˆ†é’Ÿ
        total_samples = int(sample_rate * duration)
        
        # åˆ›å»ºä¸‰é˜¶æ®µéŸ³é¢‘
        t = np.linspace(0, duration, total_samples)
        
        # é˜¶æ®µ1 (0-30s): è¾ƒå¿«èŠ‚å¥ï¼ŒåŒ¹é…å½“å‰æƒ…ç»ª
        stage1_end = int(sample_rate * 30)
        stage1_freq = 440.0  # A4
        stage1 = 0.3 * np.sin(2 * np.pi * stage1_freq * t[:stage1_end])
        
        # é˜¶æ®µ2 (30-90s): è¿‡æ¸¡é˜¶æ®µï¼Œé¢‘ç‡é€æ¸é™ä½
        stage2_start = stage1_end
        stage2_end = int(sample_rate * 90)
        stage2_samples = stage2_end - stage2_start
        freq_transition = np.linspace(440.0, 220.0, stage2_samples)
        stage2_t = t[stage2_start:stage2_end] - t[stage2_start]
        stage2 = 0.2 * np.sin(2 * np.pi * freq_transition * stage2_t)
        
        # é˜¶æ®µ3 (90-120s): ä½é¢‘ï¼ŒåŠ©çœ 
        stage3_start = stage2_end
        stage3_samples = total_samples - stage3_start
        stage3_freq = 110.0  # ä½é¢‘åŠ©çœ 
        stage3_t = t[stage3_start:] - t[stage3_start]
        stage3 = 0.1 * np.sin(2 * np.pi * stage3_freq * stage3_t)
        
        # åˆå¹¶ä¸‰é˜¶æ®µ
        audio = np.concatenate([stage1, stage2, stage3])
        
        # æ·»åŠ æ¸å˜ä»¥é¿å…çªå˜
        fade_length = int(sample_rate * 2)  # 2ç§’æ¸å˜
        
        # é˜¶æ®µé—´æ¸å˜
        audio[stage1_end-fade_length:stage1_end] *= np.linspace(1, 0, fade_length)
        audio[stage1_end:stage1_end+fade_length] *= np.linspace(0, 1, fade_length)
        
        audio[stage2_end-fade_length:stage2_end] *= np.linspace(1, 0, fade_length)
        audio[stage2_end:stage2_end+fade_length] *= np.linspace(0, 1, fade_length)
        
        # ç«‹ä½“å£°
        stereo_audio = np.column_stack([audio, audio])
        
        return stereo_audio
    
    def _fallback_generation(self, emotion_data: Dict[str, Any], 
                           music_params: MusicParameter) -> Dict[str, Any]:
        """å¤‡ç”¨ç”Ÿæˆæ–¹æ¡ˆï¼ˆå½“APIä¸å¯ç”¨æ—¶ï¼‰"""
        logger.info("ä½¿ç”¨å¤‡ç”¨éŸ³ä¹ç”Ÿæˆæ–¹æ¡ˆ")
        
        audio_data = self._generate_mock_audio()
        
        return {
            'success': True,
            'audio_data': audio_data,
            'fallback_used': True,
            'three_stage_narrative': True,
            'message': 'APIä¸å¯ç”¨ï¼Œä½¿ç”¨æœ¬åœ°ç”Ÿæˆ'
        }

class ProceduralAudioGenerator:
    """ç¨‹åºåŒ–éŸ³é¢‘ç”Ÿæˆå™¨"""
    
    def __init__(self, config: GenerationLayerConfig):
        self.config = config
        self.sample_rate = config.output_sample_rate
        self.channels = config.output_channels
        
        # éŸ³é¢‘ç”Ÿæˆå‚æ•°
        self.base_frequencies = {
            'C': 261.63, 'D': 293.66, 'E': 329.63, 'F': 349.23,
            'G': 392.00, 'A': 440.00, 'B': 493.88
        }
        
        # å’Œå¼¦è¿›è¡Œ
        self.chord_progressions = {
            'relaxing': ['C', 'Am', 'F', 'G'],
            'calming': ['Am', 'F', 'C', 'G'],
            'peaceful': ['F', 'C', 'G', 'Am']
        }
        
        logger.info(f"ç¨‹åºåŒ–éŸ³é¢‘ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆï¼Œé‡‡æ ·ç‡: {self.sample_rate}Hz")
    
    def generate_sine_wave(self, frequency: float, duration: float, 
                          amplitude: float = 0.5, phase: float = 0.0) -> np.ndarray:
        """ç”Ÿæˆæ­£å¼¦æ³¢"""
        t = np.linspace(0, duration, int(self.sample_rate * duration), False)
        wave = amplitude * np.sin(2 * np.pi * frequency * t + phase)
        return wave
    
    def generate_binaural_beats(self, base_freq: float, beat_freq: float, 
                               duration: float) -> np.ndarray:
        """ç”ŸæˆåŒè€³èŠ‚æ‹"""
        left_freq = base_freq
        right_freq = base_freq + beat_freq
        
        left_channel = self.generate_sine_wave(left_freq, duration)
        right_channel = self.generate_sine_wave(right_freq, duration)
        
        # åˆå¹¶ä¸ºç«‹ä½“å£°
        stereo_audio = np.column_stack((left_channel, right_channel))
        return stereo_audio
    
    def generate_chord(self, root_note: str, chord_type: str, 
                      duration: float) -> np.ndarray:
        """ç”Ÿæˆå’Œå¼¦"""
        base_freq = self.base_frequencies.get(root_note, 440.0)
        
        # å®šä¹‰å’Œå¼¦éŸ³ç¨‹
        chord_intervals = {
            'major': [0, 4, 7],
            'minor': [0, 3, 7],
            'dim': [0, 3, 6],
            'aug': [0, 4, 8]
        }
        
        intervals = chord_intervals.get(chord_type, [0, 4, 7])
        chord_audio = np.zeros(int(self.sample_rate * duration))
        
        for interval in intervals:
            freq = base_freq * (2 ** (interval / 12.0))
            note_audio = self.generate_sine_wave(freq, duration, amplitude=0.3)
            chord_audio += note_audio
        
        # ç«‹ä½“å£°å¤„ç†
        if self.channels == 2:
            chord_audio = np.column_stack((chord_audio, chord_audio))
        
        return chord_audio
    
    def generate_ambient_texture(self, music_params: MusicParameter, 
                                duration: float) -> np.ndarray:
        """ç”Ÿæˆç¯å¢ƒéŸ³ä¹çº¹ç†"""
        # åŸºç¡€é¢‘ç‡
        base_freq = self.base_frequencies.get(music_params.key_signature.split('_')[0], 440.0)
        
        # ç”ŸæˆåŸºç¡€éŸ³è°ƒ
        base_tone = self.generate_sine_wave(base_freq, duration, amplitude=0.3)
        
        # æ·»åŠ å’Œå£°
        harmony1 = self.generate_sine_wave(base_freq * 1.25, duration, amplitude=0.2)
        harmony2 = self.generate_sine_wave(base_freq * 1.5, duration, amplitude=0.15)
        
        # åˆæˆ
        combined_audio = base_tone + harmony1 + harmony2
        
        # åº”ç”¨åŒ…ç»œ
        envelope = self._generate_envelope(duration, attack=0.1, decay=0.1, sustain=0.8, release=0.1)
        combined_audio *= envelope
        
        # æ·»åŠ æ²»ç–—æ€§å…ƒç´ 
        if self.config.binaural_beats:
            # æ ¹æ®ISOé˜¶æ®µè°ƒæ•´åŒè€³èŠ‚æ‹é¢‘ç‡
            beat_freq = self._get_binaural_frequency(music_params.iso_stage)
            binaural = self.generate_binaural_beats(base_freq, beat_freq, duration)
            
            # æ··åˆåŒè€³èŠ‚æ‹
            if self.channels == 2:
                combined_audio = np.column_stack((combined_audio, combined_audio))
            
            combined_audio = 0.7 * combined_audio + 0.3 * binaural
        else:
            if self.channels == 2:
                combined_audio = np.column_stack((combined_audio, combined_audio))
        
        return combined_audio
    
    def _generate_envelope(self, duration: float, attack: float = 0.1, 
                          decay: float = 0.1, sustain: float = 0.8, 
                          release: float = 0.1) -> np.ndarray:
        """ç”ŸæˆADSRåŒ…ç»œ"""
        total_samples = int(self.sample_rate * duration)
        envelope = np.zeros(total_samples)
        
        attack_samples = int(attack * total_samples)
        decay_samples = int(decay * total_samples)
        release_samples = int(release * total_samples)
        sustain_samples = total_samples - attack_samples - decay_samples - release_samples
        
        # Attack
        envelope[:attack_samples] = np.linspace(0, 1, attack_samples)
        
        # Decay
        envelope[attack_samples:attack_samples + decay_samples] = \
            np.linspace(1, sustain, decay_samples)
        
        # Sustain
        envelope[attack_samples + decay_samples:attack_samples + decay_samples + sustain_samples] = sustain
        
        # Release
        envelope[-release_samples:] = np.linspace(sustain, 0, release_samples)
        
        return envelope
    
    def _get_binaural_frequency(self, iso_stage: str) -> float:
        """æ ¹æ®ISOé˜¶æ®µè·å–åŒè€³èŠ‚æ‹é¢‘ç‡"""
        frequencies = {
            'synchronization': 10.0,  # Alphaæ³¢æ®µ
            'guidance': 8.0,          # Alpha-Thetaè¿‡æ¸¡
            'consolidation': 6.0      # Thetaæ³¢æ®µ
        }
        return frequencies.get(iso_stage, 8.0)

class ProceduralVideoGenerator:
    """ç¨‹åºåŒ–è§†é¢‘ç”Ÿæˆå™¨"""
    
    def __init__(self, config: GenerationLayerConfig):
        self.config = config
        self.width, self.height = config.video_resolution
        self.fps = config.video_fps
        
        # å½“å‰é£æ ¼
        self.style = config.video_style
        
        # è§†é¢‘é£æ ¼é…ç½®
        self.style_configs = {
            'ambient': {
                'background_color': (20, 30, 50),
                'primary_color': (100, 150, 200),
                'animation_speed': 0.5
            },
            'abstract': {
                'background_color': (10, 10, 20),
                'primary_color': (150, 100, 200),
                'animation_speed': 0.8
            },
            'nature': {
                'background_color': (30, 50, 30),
                'primary_color': (100, 200, 100),
                'animation_speed': 0.3
            },
            # æ–°å¢é˜¶æ®µç‰¹å®šé£æ ¼
            'dynamic_ambient': {
                'background_color': (25, 35, 55),
                'primary_color': (120, 170, 220),
                'animation_speed': 0.6
            },
            'transitional_flow': {
                'background_color': (18, 25, 40),
                'primary_color': (90, 130, 180),
                'animation_speed': 0.4
            },
            'calm_static': {
                'background_color': (15, 20, 35),
                'primary_color': (70, 100, 140),
                'animation_speed': 0.2
            }
        }
        
        logger.info(f"ç¨‹åºåŒ–è§†é¢‘ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆï¼Œåˆ†è¾¨ç‡: {self.width}x{self.height}")
    
    def generate_frame(self, frame_index: int, music_params: MusicParameter) -> np.ndarray:
        """ç”Ÿæˆå•å¸§è§†é¢‘"""
        if not VIDEO_AVAILABLE:
            # è¿”å›çº¯è‰²å¸§
            return np.full((self.height, self.width, 3), 50, dtype=np.uint8)
        
        # åˆ›å»ºç”»å¸ƒ
        current_style = getattr(self, 'style', self.config.video_style)
        if current_style not in self.style_configs:
            current_style = 'ambient'
        
        image = PIL.Image.new('RGB', (self.width, self.height), 
                             color=self.style_configs[current_style]['background_color'])
        draw = PIL.ImageDraw.Draw(image)
        
        # åŸºäºéŸ³ä¹å‚æ•°ç”Ÿæˆè§†è§‰æ•ˆæœ
        self._draw_emotion_visualization(draw, music_params, frame_index)
        
        # æ·»åŠ æ²»ç–—æ€§å…ƒç´ 
        if self.config.therapy_optimized:
            self._draw_therapy_elements(draw, music_params, frame_index)
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        frame = np.array(image)
        return frame
    
    def _draw_emotion_visualization(self, draw, music_params: MusicParameter, frame_index: int):
        """ç»˜åˆ¶æƒ…ç»ªå¯è§†åŒ–"""
        # åŸºäºæƒ…ç»ªæ˜ å°„ç»˜åˆ¶æ¸å˜èƒŒæ™¯
        center_x, center_y = self.width // 2, self.height // 2
        
        # æƒ…ç»ªå¼ºåº¦å½±å“åœ†å½¢å¤§å°
        radius = int(200 + 100 * abs(music_params.valence_mapping))
        
        # æƒ…ç»ªæ•ˆä»·å½±å“é¢œè‰²
        if music_params.valence_mapping > 0:
            color = (100, 150, 255)  # ç§¯ææƒ…ç»ª - è“è‰²
        else:
            color = (255, 150, 100)  # æ¶ˆææƒ…ç»ª - æ©™è‰²
        
        # ç»˜åˆ¶è„‰åŠ¨åœ†å½¢
        pulse_factor = 0.8 + 0.2 * np.sin(frame_index * 0.1)
        current_radius = int(radius * pulse_factor)
        
        draw.ellipse([center_x - current_radius, center_y - current_radius,
                     center_x + current_radius, center_y + current_radius],
                    fill=color, outline=None)
    
    def _draw_therapy_elements(self, draw, music_params: MusicParameter, frame_index: int):
        """ç»˜åˆ¶æ²»ç–—æ€§å…ƒç´ """
        # æ ¹æ®ISOé˜¶æ®µç»˜åˆ¶ä¸åŒçš„æ²»ç–—å…ƒç´ 
        if music_params.iso_stage == 'synchronization':
            self._draw_sync_patterns(draw, frame_index)
        elif music_params.iso_stage == 'guidance':
            self._draw_guidance_flow(draw, frame_index)
        elif music_params.iso_stage == 'consolidation':
            self._draw_consolidation_calm(draw, frame_index)
    
    def _draw_sync_patterns(self, draw, frame_index: int):
        """ç»˜åˆ¶åŒæ­¥æ¨¡å¼"""
        # ç»˜åˆ¶åŒæ­¥åœ†ç¯
        center_x, center_y = self.width // 2, self.height // 2
        for i in range(3):
            radius = 100 + i * 50
            alpha = int(100 * (1 - i * 0.3))
            color = (255, 255, 255, alpha)
            
            draw.ellipse([center_x - radius, center_y - radius,
                         center_x + radius, center_y + radius],
                        outline=color, width=2)
    
    def _draw_guidance_flow(self, draw, frame_index: int):
        """ç»˜åˆ¶å¼•å¯¼æµåŠ¨"""
        # ç»˜åˆ¶æµåŠ¨æ›²çº¿
        y_offset = int(50 * np.sin(frame_index * 0.05))
        
        for i in range(0, self.width, 20):
            x = i
            y = self.height // 2 + y_offset + int(20 * np.sin(i * 0.01))
            draw.ellipse([x - 5, y - 5, x + 5, y + 5], fill=(200, 200, 255))
    
    def _draw_consolidation_calm(self, draw, frame_index: int):
        """ç»˜åˆ¶å·©å›ºå®é™"""
        # ç»˜åˆ¶é™æ€æ˜Ÿç‚¹
        np.random.seed(42)  # å›ºå®šç§å­ç¡®ä¿æ˜Ÿç‚¹ä½ç½®ä¸€è‡´
        for _ in range(50):
            x = np.random.randint(0, self.width)
            y = np.random.randint(0, self.height)
            size = np.random.randint(1, 3)
            
            # æ˜Ÿç‚¹é—ªçƒæ•ˆæœ
            brightness = int(100 + 50 * np.sin(frame_index * 0.02 + x * 0.001))
            color = (brightness, brightness, brightness)
            
            draw.ellipse([x - size, y - size, x + size, y + size], fill=color)
    
    def generate_video_sequence(self, music_params: MusicParameter, 
                              duration: float) -> List[np.ndarray]:
        """ç”Ÿæˆè§†é¢‘åºåˆ—"""
        total_frames = int(duration * self.fps)
        frames = []
        
        logger.info(f"ğŸ¬ å¼€å§‹ç”Ÿæˆè§†é¢‘åºåˆ—ï¼Œå…±{total_frames}å¸§ï¼ˆé¢„è®¡è€—æ—¶: {total_frames/15:.1f}ç§’ï¼‰")
        
        for frame_index in range(total_frames):
            frame = self.generate_frame(frame_index, music_params)
            frames.append(frame)
            
            # æ¯10å¸§è®°å½•ä¸€æ¬¡è¿›åº¦ï¼ˆ15fpsæ—¶çº¦æ¯0.67ç§’ï¼‰
            if frame_index % 10 == 0 or frame_index == total_frames - 1:
                progress = (frame_index + 1) / total_frames * 100
                logger.info(f"ğŸ“¹ è§†é¢‘ç”Ÿæˆè¿›åº¦: {frame_index+1}/{total_frames} ({progress:.1f}%)")
        
        logger.info(f"âœ… è§†é¢‘åºåˆ—ç”Ÿæˆå®Œæˆ")
        return frames

class GenerationLayer(BaseLayer):
    """ç”Ÿæˆå±‚ - å®æ—¶éŸ³è§†é¢‘å†…å®¹ç”Ÿæˆ"""
    
    def __init__(self, config: GenerationLayerConfig):
        super().__init__(config)
        self.config = config
        self.layer_name = "generation_layer"
        
        # åˆå§‹åŒ–ç”Ÿæˆå™¨
        self.audio_generator = ProceduralAudioGenerator(config) if config.audio_enabled else None
        self.video_generator = ProceduralVideoGenerator(config) if config.video_enabled else None
        
        # åˆå§‹åŒ–Suno APIå®¢æˆ·ç«¯
        self.suno_client = SunoAPIClient() if config.audio_enabled else None
        
        # åˆå§‹åŒ–ISOåŸåˆ™
        if config.iso_stage_aware:
            self.iso_principle = ISOPrinciple()
        else:
            self.iso_principle = None
        
        # æ€§èƒ½ç›‘æ§
        self.performance_monitor = PerformanceMonitor()
        
        # ç”Ÿæˆç¼“å­˜
        self.content_cache = {} if config.enable_caching else None
        
        logger.info(f"ç”Ÿæˆå±‚åˆå§‹åŒ–å®Œæˆ - éŸ³é¢‘: {config.audio_enabled}, è§†é¢‘: {config.video_enabled}")
    
    def _extract_music_parameters(self, input_data: Dict[str, Any]) -> MusicParameter:
        """ä»æ˜ å°„å±‚è¾“å‡ºæå–éŸ³ä¹å‚æ•°"""
        music_params_data = input_data.get('music_parameters', {})
        
        # å¦‚æœæ˜¯å­—å…¸æ ¼å¼ï¼Œè½¬æ¢ä¸ºMusicParameterå¯¹è±¡
        if isinstance(music_params_data, dict):
            return MusicParameter(music_params_data)
        elif hasattr(music_params_data, 'tempo_bpm'):
            # å·²ç»æ˜¯MusicParameterå¯¹è±¡
            return music_params_data
        else:
            # ä½¿ç”¨é»˜è®¤å‚æ•°
            logger.warning("æ— æ³•è§£æéŸ³ä¹å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            return MusicParameter()
    
    def _generate_audio_content(self, music_params: MusicParameter, emotion_data: Dict[str, Any] = None) -> Dict[str, Any]:
        """ç”ŸæˆéŸ³é¢‘å†…å®¹ - ä½¿ç”¨Suno APIè¿›è¡Œä¸‰é˜¶æ®µéŸ³ä¹å™äº‹ç”Ÿæˆ"""
        if not self.suno_client:
            return self._fallback_to_procedural_audio(music_params)
        
        try:
            # ä½¿ç”¨Suno APIç”Ÿæˆä¸‰é˜¶æ®µè¿è´¯éŸ³ä¹
            logger.info("ğŸµ å¼€å§‹è°ƒç”¨Suno APIç”Ÿæˆä¸‰é˜¶æ®µéŸ³ä¹å™äº‹...")
            
            if emotion_data is None:
                emotion_data = {'primary_emotion': {'name': 'ç„¦è™‘'}}  # é»˜è®¤æƒ…ç»ª
            
            suno_result = self.suno_client.generate_three_stage_music(emotion_data, music_params)
            
            if suno_result.get('success'):
                audio_data = suno_result['audio_data']
                
                # ç¡®ä¿æ˜¯numpyæ•°ç»„
                if not isinstance(audio_data, np.ndarray):
                    audio_data = np.array(audio_data)
                
                # å½’ä¸€åŒ–éŸ³é¢‘
                if np.max(np.abs(audio_data)) > 0:
                    audio_data = audio_data / np.max(np.abs(audio_data))
                
                # è½¬æ¢ä¸ºå­—èŠ‚æµï¼ˆå¦‚æœéœ€è¦ï¼‰
                audio_bytes = None
                if AUDIO_AVAILABLE:
                    try:
                        buffer = io.BytesIO()
                        sf.write(buffer, audio_data, self.config.output_sample_rate, format='WAV')
                        audio_bytes = buffer.getvalue()
                    except Exception as e:
                        logger.warning(f"éŸ³é¢‘ç¼–ç å¤±è´¥: {e}")
                
                # æ„å»ºè¿”å›ç»“æœ
                result = {
                    'audio_array': audio_data,
                    'audio_bytes': audio_bytes,
                    'sample_rate': self.config.output_sample_rate,
                    'channels': self.config.output_channels,
                    'duration': 120.0,  # 2åˆ†é’Ÿä¸‰é˜¶æ®µéŸ³ä¹
                    'format': 'WAV',
                    'three_stage_narrative': True,
                    'stage_prompts': suno_result.get('stage_prompts', {}),
                    'suno_metadata': suno_result.get('suno_response', {}),
                    'fallback_used': suno_result.get('fallback_used', False)
                }
                
                if suno_result.get('fallback_used'):
                    logger.info("âœ… ä¸‰é˜¶æ®µéŸ³ä¹ç”Ÿæˆå®Œæˆï¼ˆä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆï¼‰")
                else:
                    logger.info("âœ… ä¸‰é˜¶æ®µéŸ³ä¹ç”Ÿæˆå®Œæˆï¼ˆSuno APIï¼‰")
                
                return result
            else:
                # å¦‚æœSuno APIå®Œå…¨å¤±è´¥ï¼Œä½¿ç”¨åŸæœ‰çš„ç¨‹åºåŒ–ç”Ÿæˆ
                logger.warning("Suno APIç”Ÿæˆå¤±è´¥ï¼Œå›é€€åˆ°ç¨‹åºåŒ–ç”Ÿæˆ")
                return self._fallback_to_procedural_audio(music_params)
                
        except Exception as e:
            logger.error(f"Suno APIéŸ³é¢‘ç”Ÿæˆå¤±è´¥: {e}")
            return self._fallback_to_procedural_audio(music_params)
    
    def _fallback_to_procedural_audio(self, music_params: MusicParameter) -> Dict[str, Any]:
        """å¤‡ç”¨ç¨‹åºåŒ–éŸ³é¢‘ç”Ÿæˆ"""
        if not self.audio_generator:
            return {'error': 'éŸ³é¢‘ç”Ÿæˆå™¨æœªå¯ç”¨'}
        
        try:
            # ä½¿ç”¨åŸæœ‰çš„ç¨‹åºåŒ–ç”Ÿæˆå™¨
            audio_data = self.audio_generator.generate_ambient_texture(
                music_params, 
                self.config.audio_duration
            )
            
            # ç¡®ä¿æ˜¯numpyæ•°ç»„
            if not isinstance(audio_data, np.ndarray):
                audio_data = np.array(audio_data)
            
            # å½’ä¸€åŒ–éŸ³é¢‘
            if np.max(np.abs(audio_data)) > 0:
                audio_data = audio_data / np.max(np.abs(audio_data))
            
            # è½¬æ¢ä¸ºå­—èŠ‚æµï¼ˆå¦‚æœéœ€è¦ï¼‰
            audio_bytes = None
            if AUDIO_AVAILABLE:
                try:
                    buffer = io.BytesIO()
                    sf.write(buffer, audio_data, self.config.output_sample_rate, format='WAV')
                    audio_bytes = buffer.getvalue()
                except Exception as e:
                    logger.warning(f"éŸ³é¢‘ç¼–ç å¤±è´¥: {e}")
            
            return {
                'audio_array': audio_data,
                'audio_bytes': audio_bytes,
                'sample_rate': self.config.output_sample_rate,
                'channels': self.config.output_channels,
                'duration': self.config.audio_duration,
                'format': 'WAV',
                'fallback_used': True
            }
            
        except Exception as e:
            logger.error(f"å¤‡ç”¨éŸ³é¢‘ç”Ÿæˆå¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _generate_video_content(self, music_params: MusicParameter) -> Dict[str, Any]:
        """ç”Ÿæˆè§†é¢‘å†…å®¹"""
        if not self.video_generator:
            return {'error': 'è§†é¢‘ç”Ÿæˆå™¨æœªå¯ç”¨'}
        
        try:
            # ç”Ÿæˆè§†é¢‘å¸§åºåˆ—
            frames = self.video_generator.generate_video_sequence(
                music_params, 
                self.config.video_duration
            )
            
            return {
                'frames': frames,
                'fps': self.config.video_fps,
                'resolution': self.config.video_resolution,
                'duration': self.config.video_duration,
                'total_frames': len(frames),
                'format': 'RGB'
            }
            
        except Exception as e:
            logger.error(f"è§†é¢‘ç”Ÿæˆå¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _synchronize_audio_video(self, audio_data: Dict[str, Any], 
                               video_data: Dict[str, Any]) -> Dict[str, Any]:
        """åŒæ­¥éŸ³è§†é¢‘å†…å®¹"""
        # æ£€æŸ¥æ—¶é•¿ä¸€è‡´æ€§
        audio_duration = audio_data.get('duration', 0)
        video_duration = video_data.get('duration', 0)
        
        if abs(audio_duration - video_duration) > 0.1:  # 100msè¯¯å·®å®¹å¿
            logger.warning(f"éŸ³è§†é¢‘æ—¶é•¿ä¸ä¸€è‡´: éŸ³é¢‘{audio_duration}s, è§†é¢‘{video_duration}s")
        
        # åˆ›å»ºåŒæ­¥å…ƒæ•°æ®
        sync_metadata = {
            'audio_duration': audio_duration,
            'video_duration': video_duration,
            'synchronized': True,
            'sync_method': 'temporal_alignment',
            'sync_accuracy': 1.0 - abs(audio_duration - video_duration) / max(audio_duration, video_duration)
        }
        
        return {
            'audio': audio_data,
            'video': video_data,
            'sync_metadata': sync_metadata
        }
    
    def _cache_content(self, cache_key: str, content: Dict[str, Any]):
        """ç¼“å­˜ç”Ÿæˆçš„å†…å®¹"""
        if self.content_cache is not None:
            # ç®€å•çš„LRUç¼“å­˜
            if len(self.content_cache) >= self.config.cache_size:
                # ç§»é™¤æœ€æ—§çš„æ¡ç›®
                oldest_key = next(iter(self.content_cache))
                del self.content_cache[oldest_key]
            
            self.content_cache[cache_key] = content
            logger.debug(f"å†…å®¹å·²ç¼“å­˜: {cache_key}")
    
    def _get_cached_content(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """è·å–ç¼“å­˜çš„å†…å®¹"""
        if self.content_cache is not None:
            return self.content_cache.get(cache_key)
        return None
    
    def _generate_cache_key(self, music_params: MusicParameter) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # åŸºäºéŸ³ä¹å‚æ•°ç”Ÿæˆå“ˆå¸Œé”®
        key_data = {
            'tempo': music_params.tempo_bpm,
            'key': music_params.key_signature,
            'iso_stage': music_params.iso_stage,
            'valence': music_params.valence_mapping,
            'arousal': music_params.arousal_mapping
        }
        
        import hashlib
        key_str = json.dumps(key_data, sort_keys=True)
        return hashlib.md5(key_str.encode()).hexdigest()
    
    async def _process_impl(self, input_data: LayerData) -> LayerData:
        """ç”Ÿæˆå±‚å¤„ç†å®ç°"""
        self.performance_monitor.start_timer("generation_layer_processing")
        
        try:
            # éªŒè¯è¾“å…¥æ•°æ®
            if not input_data.data:
                raise ValueError("è¾“å…¥æ•°æ®ä¸ºç©º")
            
            # åˆå§‹åŒ–ç¼“å­˜å˜é‡
            cached_content = None
            cache_key = "unknown"
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ISOä¸‰é˜¶æ®µå‚æ•°
            if 'iso_three_stage_params' in input_data.data:
                # ä¸‰é˜¶æ®µç”Ÿæˆæ¨¡å¼
                content = await self._generate_three_stage_content(input_data.data)
                cache_key = "three_stage_dynamic"
            else:
                # ä¼ ç»Ÿå•é˜¶æ®µç”Ÿæˆæ¨¡å¼
                music_params = self._extract_music_parameters(input_data.data)
                
                # ç”Ÿæˆç¼“å­˜é”®
                cache_key = self._generate_cache_key(music_params)
                
                # æ£€æŸ¥ç¼“å­˜
                cached_content = self._get_cached_content(cache_key)
                if cached_content:
                    logger.info(f"ä½¿ç”¨ç¼“å­˜å†…å®¹: {cache_key}")
                    content = cached_content
                else:
                    # ç”Ÿæˆæ–°å†…å®¹
                    content = {}
                    
                    # ç”ŸæˆéŸ³é¢‘å†…å®¹
                    if self.config.audio_enabled:
                        # æå–æƒ…ç»ªæ•°æ®ä¼ é€’ç»™Suno API
                        emotion_data = input_data.data.get('emotion_analysis', {})
                        audio_content = self._generate_audio_content(music_params, emotion_data)
                        content['audio'] = audio_content
                    
                    # ç”Ÿæˆè§†é¢‘å†…å®¹
                    if self.config.video_enabled:
                        video_content = self._generate_video_content(music_params)
                        content['video'] = video_content
                    
                    # éŸ³è§†é¢‘åŒæ­¥
                    if self.config.audio_enabled and self.config.video_enabled:
                        if 'error' not in content['audio'] and 'error' not in content['video']:
                            content = self._synchronize_audio_video(content['audio'], content['video'])
                
                # ç¼“å­˜å†…å®¹
                self._cache_content(cache_key, content)
            
            # åˆ›å»ºè¾“å‡ºæ•°æ®
            music_params_dict = {}
            if 'iso_three_stage_params' not in input_data.data:
                # å•é˜¶æ®µæ¨¡å¼
                music_params_dict = music_params.__dict__ if hasattr(music_params, '__dict__') else {}
            
            output_data = LayerData(
                layer_name=self.layer_name,
                timestamp=datetime.now(),
                data={
                    'generated_content': content,
                    'music_parameters': music_params_dict,
                    'generation_info': {
                        'strategy': self.config.generation_strategy,
                        'content_type': self.config.content_type,
                        'cached': cached_content is not None,
                        'cache_key': cache_key
                    }
                },
                metadata={
                    'source_layer': input_data.layer_name,
                    'generation_strategy': self.config.generation_strategy,
                    'content_types': [t for t in ['audio', 'video'] if getattr(self.config, f"{t}_enabled")]
                },
                confidence=input_data.confidence
            )
            
            # è®°å½•å¤„ç†æ—¶é—´
            processing_time = self.performance_monitor.end_timer("generation_layer_processing")
            output_data.processing_time = processing_time
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.total_processed += 1
            self.total_processing_time += processing_time
            
            logger.info(f"ç”Ÿæˆå±‚å¤„ç†å®Œæˆ - å†…å®¹ç±»å‹: {self.config.content_type}, "
                       f"ç¼“å­˜: {cached_content is not None}, è€—æ—¶: {processing_time*1000:.1f}ms")
            
            return output_data
            
        except Exception as e:
            self.error_count += 1
            logger.error(f"ç”Ÿæˆå±‚å¤„ç†å¤±è´¥: {e}")
            
            # åˆ›å»ºé”™è¯¯è¾“å‡º
            error_data = LayerData(
                layer_name=self.layer_name,
                timestamp=datetime.now(),
                data={
                    'error': str(e),
                    'generated_content': {}
                },
                metadata={'error': True, 'source_layer': input_data.layer_name},
                confidence=0.0
            )
            
            processing_time = self.performance_monitor.end_timer("generation_layer_processing")
            error_data.processing_time = processing_time
            
            return error_data
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        if self.content_cache is not None:
            self.content_cache.clear()
            logger.info("ç”Ÿæˆå±‚ç¼“å­˜å·²æ¸…ç©º")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        if self.content_cache is not None:
            return {
                'cache_size': len(self.content_cache),
                'max_cache_size': self.config.cache_size,
                'cache_enabled': True
            }
        return {'cache_enabled': False}
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–ç”Ÿæˆå±‚çŠ¶æ€"""
        base_status = super().get_status()
        
        # æ·»åŠ ç”Ÿæˆå±‚ç‰¹æœ‰çš„çŠ¶æ€ä¿¡æ¯
        generation_status = {
            'generation_strategy': self.config.generation_strategy,
            'content_type': self.config.content_type,
            'audio_enabled': self.config.audio_enabled,
            'video_enabled': self.config.video_enabled,
            'audio_available': AUDIO_AVAILABLE,
            'video_available': VIDEO_AVAILABLE,
            'cache_stats': self.get_cache_stats(),
            'performance_stats': self.performance_monitor.get_all_stats()
        }
        
        base_status.update(generation_status)
        return base_status
    
    async def _generate_three_stage_content(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ç”ŸæˆISOä¸‰é˜¶æ®µéŸ³ç”»åŒæ­¥å†…å®¹"""
        logger.info("ğŸ­ å¼€å§‹ç”ŸæˆISOä¸‰é˜¶æ®µéŸ³ç”»åŒæ­¥å†…å®¹")
        
        iso_params = input_data['iso_three_stage_params']
        stages = ['match_stage', 'guide_stage', 'target_stage']
        
        # ä¸‰é˜¶æ®µå†…å®¹å®¹å™¨
        three_stage_content = {
            'stages': {},
            'continuous_narrative': True,
            'total_duration': 0.0,
            'sync_metadata': {
                'stage_transitions': [],
                'narrative_flow': 'smooth',
                'cross_stage_continuity': True
            }
        }
        
        # ä¸ºæ¯ä¸ªé˜¶æ®µç”Ÿæˆå†…å®¹
        for stage_idx, stage_name in enumerate(stages):
            stage_params = iso_params[stage_name]
            logger.info(f"ğŸ¼ ç”Ÿæˆé˜¶æ®µ {stage_idx + 1}: {stage_name}")
            
            # è½¬æ¢é˜¶æ®µå‚æ•°ä¸ºMusicParameterå¯¹è±¡
            music_params = self._convert_stage_to_music_params(stage_params, stage_name)
            
            # ç”Ÿæˆé˜¶æ®µå†…å®¹
            stage_content = {}
            
            # ç”ŸæˆéŸ³é¢‘å†…å®¹
            if self.config.audio_enabled:
                stage_audio = await self._generate_stage_audio(music_params, stage_idx, stage_name)
                stage_content['audio'] = stage_audio
            
            # ç”Ÿæˆè§†é¢‘å†…å®¹
            if self.config.video_enabled:
                stage_video = await self._generate_stage_video(music_params, stage_idx, stage_name)
                stage_content['video'] = stage_video
            
            # é˜¶æ®µéŸ³è§†é¢‘åŒæ­¥
            if self.config.audio_enabled and self.config.video_enabled:
                if 'error' not in stage_content['audio'] and 'error' not in stage_content['video']:
                    stage_content = self._synchronize_stage_content(stage_content, stage_name)
            
            # æ·»åŠ é˜¶æ®µå…ƒæ•°æ®
            stage_content['stage_info'] = {
                'stage_name': stage_name,
                'stage_index': stage_idx,
                'stage_duration': stage_params['stage_duration'],
                'therapy_intensity': stage_params.get('therapy_intensity', 0.5),
                'sleep_readiness': stage_params.get('sleep_readiness', 0.5),
                'tempo_bpm': stage_params['tempo_bpm'],
                'emotional_target': stage_params.get('emotional_target', 'neutral')
            }
            
            three_stage_content['stages'][stage_name] = stage_content
            three_stage_content['total_duration'] += stage_params['stage_duration']
            
            # è®°å½•é˜¶æ®µè½¬æ¢ä¿¡æ¯
            if stage_idx > 0:
                prev_stage = stages[stage_idx - 1]
                transition_info = {
                    'from_stage': prev_stage,
                    'to_stage': stage_name,
                    'transition_point': three_stage_content['total_duration'] - stage_params['stage_duration'],
                    'transition_method': 'smooth_crossfade',
                    'continuity_score': 0.9  # é«˜è¿è´¯æ€§
                }
                three_stage_content['sync_metadata']['stage_transitions'].append(transition_info)
        
        # åˆ›å»ºè¿è´¯çš„ä¸‰é˜¶æ®µå™äº‹
        three_stage_content = await self._create_narrative_continuity(three_stage_content)
        
        logger.info(f"âœ… ISOä¸‰é˜¶æ®µå†…å®¹ç”Ÿæˆå®Œæˆï¼Œæ€»æ—¶é•¿: {three_stage_content['total_duration']:.1f}åˆ†é’Ÿ")
        
        return three_stage_content
    
    def _convert_stage_to_music_params(self, stage_params: Dict[str, Any], stage_name: str) -> 'MusicParameter':
        """å°†é˜¶æ®µå‚æ•°è½¬æ¢ä¸ºMusicParameterå¯¹è±¡"""
        music_params = MusicParameter()
        
        # åŸºç¡€éŸ³ä¹å‚æ•°
        music_params.tempo_bpm = stage_params.get('tempo_bpm', 60.0)
        music_params.key_signature = stage_params.get('key_signature', 'C_major')
        music_params.dynamics = stage_params.get('dynamics', 'mp')
        music_params.valence_mapping = stage_params.get('valence_mapping', 0.0)
        music_params.arousal_mapping = stage_params.get('arousal_mapping', 0.0)
        music_params.tension_level = stage_params.get('tension_level', 0.0)
        
        # é˜¶æ®µç‰¹å®šå‚æ•°
        music_params.iso_stage = stage_name
        if hasattr(music_params, 'therapy_intensity'):
            music_params.therapy_intensity = stage_params.get('therapy_intensity', 0.5)
        if hasattr(music_params, 'sleep_readiness'):
            music_params.sleep_readiness = stage_params.get('sleep_readiness', 0.5)
        
        # ä¹å™¨é…ç½®
        music_params.instrument_weights = stage_params.get('instrument_weights', {
            'sine_wave': 0.3,
            'ambient_pad': 0.4,
            'nature_sounds': 0.3
        })
        
        return music_params
    
    async def _generate_stage_audio(self, music_params: 'MusicParameter', stage_idx: int, stage_name: str) -> Dict[str, Any]:
        """ä¸ºç‰¹å®šé˜¶æ®µç”ŸæˆéŸ³é¢‘å†…å®¹"""
        logger.info(f"ğŸµ ç”Ÿæˆ{stage_name}éŸ³é¢‘å†…å®¹")
        
        try:
            # æ ¹æ®é˜¶æ®µè°ƒæ•´éŸ³é¢‘ç”Ÿæˆå‚æ•°
            stage_duration = getattr(music_params, 'therapy_intensity', 0.5) * 60.0 + 60.0  # 1-2åˆ†é’Ÿ
            
            # ä½¿ç”¨å¢å¼ºçš„éŸ³é¢‘ç”Ÿæˆå™¨
            if self.audio_generator:
                # ç”ŸæˆåŸºç¡€éŸ³é¢‘
                audio_data = self.audio_generator.generate_ambient_texture(music_params, stage_duration)
                
                # æ·»åŠ é˜¶æ®µç‰¹å®šçš„å¤„ç†
                audio_data = self._apply_stage_audio_effects(audio_data, stage_name, stage_idx)
                
                # å½’ä¸€åŒ–
                if np.max(np.abs(audio_data)) > 0:
                    audio_data = audio_data / np.max(np.abs(audio_data))
                
                # è½¬æ¢ä¸ºå­—èŠ‚æµ
                audio_bytes = None
                if AUDIO_AVAILABLE:
                    try:
                        buffer = io.BytesIO()
                        sf.write(buffer, audio_data, self.config.output_sample_rate, format='WAV')
                        audio_bytes = buffer.getvalue()
                    except Exception as e:
                        logger.warning(f"é˜¶æ®µéŸ³é¢‘ç¼–ç å¤±è´¥: {e}")
                
                return {
                    'audio_array': audio_data,
                    'audio_bytes': audio_bytes,
                    'sample_rate': self.config.output_sample_rate,
                    'channels': self.config.output_channels,
                    'duration': stage_duration,
                    'stage_name': stage_name,
                    'stage_index': stage_idx,
                    'format': 'WAV'
                }
            else:
                return {'error': f'{stage_name}éŸ³é¢‘ç”Ÿæˆå™¨æœªå¯ç”¨'}
                
        except Exception as e:
            logger.error(f"{stage_name}éŸ³é¢‘ç”Ÿæˆå¤±è´¥: {e}")
            return {'error': str(e)}
    
    async def _generate_stage_video(self, music_params: 'MusicParameter', stage_idx: int, stage_name: str) -> Dict[str, Any]:
        """ä¸ºç‰¹å®šé˜¶æ®µç”Ÿæˆè§†é¢‘å†…å®¹"""
        logger.info(f"ğŸ¬ ç”Ÿæˆ{stage_name}è§†é¢‘å†…å®¹")
        
        try:
            stage_duration = getattr(music_params, 'therapy_intensity', 0.5) * 60.0 + 60.0
            
            if self.video_generator:
                # æ ¹æ®é˜¶æ®µè°ƒæ•´è§†é¢‘é£æ ¼
                original_style = self.video_generator.style
                self.video_generator.style = self._get_stage_video_style(stage_name)
                
                # ç”Ÿæˆè§†é¢‘å¸§
                frames = self.video_generator.generate_video_sequence(music_params, stage_duration)
                
                # åº”ç”¨é˜¶æ®µç‰¹å®šçš„è§†è§‰æ•ˆæœ
                frames = self._apply_stage_video_effects(frames, stage_name, stage_idx)
                
                # æ¢å¤åŸå§‹é£æ ¼
                self.video_generator.style = original_style
                
                return {
                    'frames': frames,
                    'fps': self.config.video_fps,
                    'resolution': self.config.video_resolution,
                    'duration': stage_duration,
                    'total_frames': len(frames),
                    'stage_name': stage_name,
                    'stage_index': stage_idx,
                    'format': 'RGB'
                }
            else:
                return {'error': f'{stage_name}è§†é¢‘ç”Ÿæˆå™¨æœªå¯ç”¨'}
                
        except Exception as e:
            logger.error(f"{stage_name}è§†é¢‘ç”Ÿæˆå¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _apply_stage_audio_effects(self, audio_data: np.ndarray, stage_name: str, stage_idx: int) -> np.ndarray:
        """åº”ç”¨é˜¶æ®µç‰¹å®šçš„éŸ³é¢‘æ•ˆæœ"""
        if len(audio_data) == 0:
            return audio_data
        
        # å¤„ç†ç«‹ä½“å£°å’Œå•å£°é“
        is_stereo = len(audio_data.shape) == 2 and audio_data.shape[1] == 2
        audio_length = audio_data.shape[0]
            
        if stage_name == 'match_stage':
            # åŒ¹é…é˜¶æ®µï¼šå¢åŠ ä¸€äº›åŠ¨æ€å˜åŒ–
            fade_length = int(audio_length * 0.1)  # 10%æ·¡å…¥
            if fade_length > 0:
                fade_in = np.linspace(0, 1, fade_length)
                if is_stereo:
                    fade_in = fade_in.reshape(-1, 1)  # ä½¿å…¶å¯ä»¥å¹¿æ’­åˆ°ç«‹ä½“å£°
                audio_data[:fade_length] *= fade_in
            
        elif stage_name == 'guide_stage':
            # å¼•å¯¼é˜¶æ®µï¼šæ·»åŠ æ¸è¿›å¼é™ä½çš„æ•ˆæœ
            guide_envelope = np.linspace(1.0, 0.7, audio_length)
            if is_stereo:
                guide_envelope = guide_envelope.reshape(-1, 1)  # ä½¿å…¶å¯ä»¥å¹¿æ’­åˆ°ç«‹ä½“å£°
            audio_data *= guide_envelope
            
        elif stage_name == 'target_stage':
            # ç›®æ ‡é˜¶æ®µï¼šæœ€å¤§ç¨‹åº¦çš„å¹³é™å¤„ç†
            fade_length = int(audio_length * 0.2)  # 20%æ·¡å‡º
            if fade_length > 0:
                fade_out = np.linspace(1, 0.3, fade_length)
                if is_stereo:
                    fade_out = fade_out.reshape(-1, 1)  # ä½¿å…¶å¯ä»¥å¹¿æ’­åˆ°ç«‹ä½“å£°
                audio_data[-fade_length:] *= fade_out
        
        return audio_data
    
    def _get_stage_video_style(self, stage_name: str) -> str:
        """è·å–é˜¶æ®µå¯¹åº”çš„è§†é¢‘é£æ ¼"""
        stage_styles = {
            'match_stage': 'dynamic_ambient',     # åŠ¨æ€ç¯å¢ƒ
            'guide_stage': 'transitional_flow',   # è¿‡æ¸¡æµåŠ¨
            'target_stage': 'calm_static'         # å¹³é™é™æ€
        }
        return stage_styles.get(stage_name, 'ambient')
    
    def _apply_stage_video_effects(self, frames: List[np.ndarray], stage_name: str, stage_idx: int) -> List[np.ndarray]:
        """åº”ç”¨é˜¶æ®µç‰¹å®šçš„è§†è§‰æ•ˆæœ"""
        if not frames:
            return frames
            
        processed_frames = []
        
        for i, frame in enumerate(frames):
            if stage_name == 'match_stage':
                # åŒ¹é…é˜¶æ®µï¼šä¿æŒåŸå§‹äº®åº¦å’ŒåŠ¨æ€
                processed_frame = frame
                
            elif stage_name == 'guide_stage':
                # å¼•å¯¼é˜¶æ®µï¼šé€æ¸é™ä½äº®åº¦å’Œå¯¹æ¯”åº¦
                progress = i / len(frames) if len(frames) > 0 else 0
                brightness_factor = 1.0 - progress * 0.3  # æœ€å¤šé™ä½30%
                processed_frame = np.clip(frame * brightness_factor, 0, 255).astype(np.uint8)
                
            elif stage_name == 'target_stage':
                # ç›®æ ‡é˜¶æ®µï¼šæœ€ä½äº®åº¦ï¼Œæœ€å¤§å¹³é™æ„Ÿ
                brightness_factor = 0.5  # é™ä½50%äº®åº¦
                processed_frame = np.clip(frame * brightness_factor, 0, 255).astype(np.uint8)
            else:
                processed_frame = frame
            
            processed_frames.append(processed_frame)
        
        logger.info(f"âœ¨ {stage_name}è§†è§‰æ•ˆæœåº”ç”¨å®Œæˆï¼Œå¤„ç†{len(processed_frames)}å¸§")
        return processed_frames
    
    def _synchronize_stage_content(self, stage_content: Dict[str, Any], stage_name: str) -> Dict[str, Any]:
        """åŒæ­¥é˜¶æ®µéŸ³è§†é¢‘å†…å®¹"""
        audio_data = stage_content['audio']
        video_data = stage_content['video']
        
        # æ£€æŸ¥æ—¶é•¿ä¸€è‡´æ€§
        audio_duration = audio_data.get('duration', 0)
        video_duration = video_data.get('duration', 0)
        
        if abs(audio_duration - video_duration) > 0.1:
            logger.warning(f"{stage_name}éŸ³è§†é¢‘æ—¶é•¿ä¸ä¸€è‡´: éŸ³é¢‘{audio_duration}s, è§†é¢‘{video_duration}s")
        
        # åˆ›å»ºé˜¶æ®µåŒæ­¥å…ƒæ•°æ®
        sync_metadata = {
            'stage_name': stage_name,
            'audio_duration': audio_duration,
            'video_duration': video_duration,
            'synchronized': True,
            'sync_method': 'stage_temporal_alignment',
            'sync_accuracy': 1.0 - abs(audio_duration - video_duration) / max(audio_duration, video_duration, 1.0),
            'stage_specific_sync': True
        }
        
        return {
            'audio': audio_data,
            'video': video_data,
            'sync_metadata': sync_metadata
        }
    
    async def _create_narrative_continuity(self, three_stage_content: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºä¸‰é˜¶æ®µå™äº‹è¿è´¯æ€§"""
        logger.info("ğŸ“– åˆ›å»ºè¿è´¯çš„ä¸‰é˜¶æ®µéŸ³ä¹å™äº‹")
        
        stages = ['match_stage', 'guide_stage', 'target_stage']
        
        # åˆ†æå„é˜¶æ®µç‰¹å¾ï¼Œç¡®ä¿å¹³æ»‘è¿‡æ¸¡
        narrative_analysis = {
            'tempo_progression': [],
            'valence_progression': [],
            'arousal_progression': [],
            'visual_continuity': [],
            'therapy_progression': []
        }
        
        # æ”¶é›†å„é˜¶æ®µæ•°æ®
        for stage_name in stages:
            if stage_name in three_stage_content['stages']:
                stage_info = three_stage_content['stages'][stage_name]['stage_info']
                
                narrative_analysis['tempo_progression'].append(stage_info['tempo_bpm'])
                narrative_analysis['therapy_progression'].append(stage_info['therapy_intensity'])
        
        # éªŒè¯å™äº‹ä¸€è‡´æ€§
        tempo_progression = narrative_analysis['tempo_progression']
        therapy_progression = narrative_analysis['therapy_progression']
        
        tempo_decrease = all(tempo_progression[i] >= tempo_progression[i+1] 
                           for i in range(len(tempo_progression)-1)) if len(tempo_progression) > 1 else True
        
        therapy_increase = all(therapy_progression[i] <= therapy_progression[i+1] 
                             for i in range(len(therapy_progression)-1)) if len(therapy_progression) > 1 else True
        
        # æ·»åŠ å™äº‹è¿è´¯æ€§å…ƒæ•°æ®
        three_stage_content['narrative_analysis'] = narrative_analysis
        three_stage_content['narrative_quality'] = {
            'tempo_coherence': tempo_decrease,
            'therapy_coherence': therapy_increase,
            'overall_coherence_score': 0.9 if tempo_decrease and therapy_increase else 0.7,
            'narrative_type': 'guided_relaxation_progression'
        }
        
        logger.info(f"âœ… å™äº‹è¿è´¯æ€§è¯„åˆ†: {three_stage_content['narrative_quality']['overall_coherence_score']:.2f}")
        
        return three_stage_content