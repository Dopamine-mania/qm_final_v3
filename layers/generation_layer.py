#!/usr/bin/env python3
"""
ç”Ÿæˆå±‚ (Generation Layer) - Layer 4

å®æ—¶éŸ³è§†é¢‘å†…å®¹ç”Ÿæˆï¼Œæ ¸å¿ƒåŠŸèƒ½åŒ…æ‹¬ï¼š
1. éŸ³ä¹åˆæˆå’Œç”Ÿæˆ
2. æ²»ç–—æ€§è§†é¢‘å†…å®¹ç”Ÿæˆ
3. å®æ—¶è‡ªé€‚åº”ç”Ÿæˆ
4. å¤šç§ç”Ÿæˆç­–ç•¥æ”¯æŒ
5. ç¡çœ æ²»ç–—ä¸“ç”¨ä¼˜åŒ–

å¤„ç†æµç¨‹ï¼š
Mapping Layer â†’ Generation Layer â†’ Rendering Layer
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Tuple, Optional, Any, Union
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
import asyncio
import logging
import json
import io
from pathlib import Path

from .base_layer import BaseLayer, LayerData, LayerConfig
from core.utils import (
    ConfigLoader, DataValidator, PerformanceMonitor, 
    get_project_root, normalize_vector
)
from core.theory.iso_principle import ISOPrinciple, ISOStage, EmotionState

logger = logging.getLogger(__name__)

# æ£€æŸ¥å¯é€‰ä¾èµ–
try:
    import librosa
    import soundfile as sf
    AUDIO_AVAILABLE = True
except ImportError:
    AUDIO_AVAILABLE = False
    logger.warning("librosa/soundfileä¸å¯ç”¨ï¼ŒéŸ³é¢‘ç”ŸæˆåŠŸèƒ½å°†å—é™")

try:
    import PIL.Image
    import PIL.ImageDraw
    import PIL.ImageFont
    VIDEO_AVAILABLE = True
except ImportError:
    VIDEO_AVAILABLE = False
    logger.warning("PILä¸å¯ç”¨ï¼Œè§†é¢‘ç”ŸæˆåŠŸèƒ½å°†å—é™")

class GenerationStrategy(Enum):
    """ç”Ÿæˆç­–ç•¥æšä¸¾"""
    RULE_BASED = "rule_based"          # åŸºäºè§„åˆ™çš„ç”Ÿæˆ
    NEURAL_SYNTHESIS = "neural_synthesis"  # ç¥ç»ç½‘ç»œåˆæˆ
    TEMPLATE_BASED = "template_based"      # åŸºäºæ¨¡æ¿çš„ç”Ÿæˆ
    HYBRID = "hybrid"                      # æ··åˆç­–ç•¥

class ContentType(Enum):
    """å†…å®¹ç±»å‹æšä¸¾"""
    AUDIO = "audio"
    VIDEO = "video"
    BOTH = "both"

@dataclass
class GenerationLayerConfig(LayerConfig):
    """ç”Ÿæˆå±‚é…ç½®"""
    # åŸºç¡€é…ç½®
    output_sample_rate: int = 44100  # éŸ³é¢‘é‡‡æ ·ç‡
    output_channels: int = 2         # ç«‹ä½“å£°
    video_fps: int = 30              # è§†é¢‘å¸§ç‡
    video_resolution: Tuple[int, int] = (1920, 1080)  # è§†é¢‘åˆ†è¾¨ç‡
    
    # ç”Ÿæˆç­–ç•¥
    generation_strategy: str = "hybrid"
    content_type: str = "both"  # audio, video, both
    
    # éŸ³é¢‘ç”Ÿæˆé…ç½®
    audio_enabled: bool = True
    audio_duration: float = 60.0     # é»˜è®¤1åˆ†é’Ÿ
    audio_buffer_size: int = 4096
    audio_synthesis_method: str = "procedural"  # procedural, neural
    
    # è§†é¢‘ç”Ÿæˆé…ç½®
    video_enabled: bool = True
    video_duration: float = 60.0     # é»˜è®¤1åˆ†é’Ÿ
    video_style: str = "ambient"     # ambient, abstract, nature
    
    # æ²»ç–—ç‰¹åŒ–é…ç½®
    therapy_optimized: bool = True
    iso_stage_aware: bool = True
    binaural_beats: bool = True
    
    # æ€§èƒ½é…ç½®
    use_gpu: bool = True
    batch_size: int = 1
    max_processing_time: float = 200.0  # ms
    
    # ç¼“å­˜é…ç½®
    enable_caching: bool = True
    cache_size: int = 100

class MusicParameter:
    """éŸ³ä¹å‚æ•°ï¼ˆä»mapping_layerå¯¼å…¥çš„æ•°æ®ç»“æ„ï¼‰"""
    def __init__(self, data: Dict[str, Any] = None):
        if data is None:
            data = {}
        
        # åŸºç¡€éŸ³ä¹å‚æ•°
        self.tempo_bpm = data.get('tempo_bpm', 60.0)
        self.key_signature = data.get('key_signature', 'C_major')
        self.time_signature = data.get('time_signature', (4, 4))
        self.dynamics = data.get('dynamics', 'mp')
        
        # éŸ³è‰²å’Œç»‡ä½“
        self.instrument_weights = data.get('instrument_weights', {})
        self.texture_complexity = data.get('texture_complexity', 0.5)
        self.harmonic_richness = data.get('harmonic_richness', 0.5)
        
        # æƒ…ç»ªè¡¨è¾¾
        self.valence_mapping = data.get('valence_mapping', 0.0)
        self.arousal_mapping = data.get('arousal_mapping', 0.0)
        self.tension_level = data.get('tension_level', 0.0)
        
        # æ²»ç–—ç‰¹åŒ–
        self.iso_stage = data.get('iso_stage', 'synchronization')
        self.therapy_intensity = data.get('therapy_intensity', 0.5)
        self.sleep_phase_alignment = data.get('sleep_phase_alignment', 0.0)

class ProceduralAudioGenerator:
    """ç¨‹åºåŒ–éŸ³é¢‘ç”Ÿæˆå™¨"""
    
    def __init__(self, config: GenerationLayerConfig):
        self.config = config
        self.sample_rate = config.output_sample_rate
        self.channels = config.output_channels
        
        # éŸ³é¢‘ç”Ÿæˆå‚æ•°
        self.base_frequencies = {
            'C': 261.63, 'D': 293.66, 'E': 329.63, 'F': 349.23,
            'G': 392.00, 'A': 440.00, 'B': 493.88
        }
        
        # å’Œå¼¦è¿›è¡Œ
        self.chord_progressions = {
            'relaxing': ['C', 'Am', 'F', 'G'],
            'calming': ['Am', 'F', 'C', 'G'],
            'peaceful': ['F', 'C', 'G', 'Am']
        }
        
        logger.info(f"ç¨‹åºåŒ–éŸ³é¢‘ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆï¼Œé‡‡æ ·ç‡: {self.sample_rate}Hz")
    
    def generate_sine_wave(self, frequency: float, duration: float, 
                          amplitude: float = 0.5, phase: float = 0.0) -> np.ndarray:
        """ç”Ÿæˆæ­£å¼¦æ³¢"""
        t = np.linspace(0, duration, int(self.sample_rate * duration), False)
        wave = amplitude * np.sin(2 * np.pi * frequency * t + phase)
        return wave
    
    def generate_binaural_beats(self, base_freq: float, beat_freq: float, 
                               duration: float) -> np.ndarray:
        """ç”ŸæˆåŒè€³èŠ‚æ‹"""
        left_freq = base_freq
        right_freq = base_freq + beat_freq
        
        left_channel = self.generate_sine_wave(left_freq, duration)
        right_channel = self.generate_sine_wave(right_freq, duration)
        
        # åˆå¹¶ä¸ºç«‹ä½“å£°
        stereo_audio = np.column_stack((left_channel, right_channel))
        return stereo_audio
    
    def generate_chord(self, root_note: str, chord_type: str, 
                      duration: float) -> np.ndarray:
        """ç”Ÿæˆå’Œå¼¦"""
        base_freq = self.base_frequencies.get(root_note, 440.0)
        
        # å®šä¹‰å’Œå¼¦éŸ³ç¨‹
        chord_intervals = {
            'major': [0, 4, 7],
            'minor': [0, 3, 7],
            'dim': [0, 3, 6],
            'aug': [0, 4, 8]
        }
        
        intervals = chord_intervals.get(chord_type, [0, 4, 7])
        chord_audio = np.zeros(int(self.sample_rate * duration))
        
        for interval in intervals:
            freq = base_freq * (2 ** (interval / 12.0))
            note_audio = self.generate_sine_wave(freq, duration, amplitude=0.3)
            chord_audio += note_audio
        
        # ç«‹ä½“å£°å¤„ç†
        if self.channels == 2:
            chord_audio = np.column_stack((chord_audio, chord_audio))
        
        return chord_audio
    
    def generate_ambient_texture(self, music_params: MusicParameter, 
                                duration: float) -> np.ndarray:
        """ç”Ÿæˆç¯å¢ƒéŸ³ä¹çº¹ç†"""
        # åŸºç¡€é¢‘ç‡
        base_freq = self.base_frequencies.get(music_params.key_signature.split('_')[0], 440.0)
        
        # ç”ŸæˆåŸºç¡€éŸ³è°ƒ
        base_tone = self.generate_sine_wave(base_freq, duration, amplitude=0.3)
        
        # æ·»åŠ å’Œå£°
        harmony1 = self.generate_sine_wave(base_freq * 1.25, duration, amplitude=0.2)
        harmony2 = self.generate_sine_wave(base_freq * 1.5, duration, amplitude=0.15)
        
        # åˆæˆ
        combined_audio = base_tone + harmony1 + harmony2
        
        # åº”ç”¨åŒ…ç»œ
        envelope = self._generate_envelope(duration, attack=0.1, decay=0.1, sustain=0.8, release=0.1)
        combined_audio *= envelope
        
        # æ·»åŠ æ²»ç–—æ€§å…ƒç´ 
        if self.config.binaural_beats:
            # æ ¹æ®ISOé˜¶æ®µè°ƒæ•´åŒè€³èŠ‚æ‹é¢‘ç‡
            beat_freq = self._get_binaural_frequency(music_params.iso_stage)
            binaural = self.generate_binaural_beats(base_freq, beat_freq, duration)
            
            # æ··åˆåŒè€³èŠ‚æ‹
            if self.channels == 2:
                combined_audio = np.column_stack((combined_audio, combined_audio))
            
            combined_audio = 0.7 * combined_audio + 0.3 * binaural
        else:
            if self.channels == 2:
                combined_audio = np.column_stack((combined_audio, combined_audio))
        
        return combined_audio
    
    def _generate_envelope(self, duration: float, attack: float = 0.1, 
                          decay: float = 0.1, sustain: float = 0.8, 
                          release: float = 0.1) -> np.ndarray:
        """ç”ŸæˆADSRåŒ…ç»œ"""
        total_samples = int(self.sample_rate * duration)
        envelope = np.zeros(total_samples)
        
        attack_samples = int(attack * total_samples)
        decay_samples = int(decay * total_samples)
        release_samples = int(release * total_samples)
        sustain_samples = total_samples - attack_samples - decay_samples - release_samples
        
        # Attack
        envelope[:attack_samples] = np.linspace(0, 1, attack_samples)
        
        # Decay
        envelope[attack_samples:attack_samples + decay_samples] = \
            np.linspace(1, sustain, decay_samples)
        
        # Sustain
        envelope[attack_samples + decay_samples:attack_samples + decay_samples + sustain_samples] = sustain
        
        # Release
        envelope[-release_samples:] = np.linspace(sustain, 0, release_samples)
        
        return envelope
    
    def _get_binaural_frequency(self, iso_stage: str) -> float:
        """æ ¹æ®ISOé˜¶æ®µè·å–åŒè€³èŠ‚æ‹é¢‘ç‡"""
        frequencies = {
            'synchronization': 10.0,  # Alphaæ³¢æ®µ
            'guidance': 8.0,          # Alpha-Thetaè¿‡æ¸¡
            'consolidation': 6.0      # Thetaæ³¢æ®µ
        }
        return frequencies.get(iso_stage, 8.0)

class ProceduralVideoGenerator:
    """ç¨‹åºåŒ–è§†é¢‘ç”Ÿæˆå™¨"""
    
    def __init__(self, config: GenerationLayerConfig):
        self.config = config
        self.width, self.height = config.video_resolution
        self.fps = config.video_fps
        
        # è§†é¢‘é£æ ¼é…ç½®
        self.style_configs = {
            'ambient': {
                'background_color': (20, 30, 50),
                'primary_color': (100, 150, 200),
                'animation_speed': 0.5
            },
            'abstract': {
                'background_color': (10, 10, 20),
                'primary_color': (150, 100, 200),
                'animation_speed': 0.8
            },
            'nature': {
                'background_color': (30, 50, 30),
                'primary_color': (100, 200, 100),
                'animation_speed': 0.3
            }
        }
        
        logger.info(f"ç¨‹åºåŒ–è§†é¢‘ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆï¼Œåˆ†è¾¨ç‡: {self.width}x{self.height}")
    
    def generate_frame(self, frame_index: int, music_params: MusicParameter) -> np.ndarray:
        """ç”Ÿæˆå•å¸§è§†é¢‘"""
        if not VIDEO_AVAILABLE:
            # è¿”å›çº¯è‰²å¸§
            return np.full((self.height, self.width, 3), 50, dtype=np.uint8)
        
        # åˆ›å»ºç”»å¸ƒ
        image = PIL.Image.new('RGB', (self.width, self.height), 
                             color=self.style_configs[self.config.video_style]['background_color'])
        draw = PIL.ImageDraw.Draw(image)
        
        # åŸºäºéŸ³ä¹å‚æ•°ç”Ÿæˆè§†è§‰æ•ˆæœ
        self._draw_emotion_visualization(draw, music_params, frame_index)
        
        # æ·»åŠ æ²»ç–—æ€§å…ƒç´ 
        if self.config.therapy_optimized:
            self._draw_therapy_elements(draw, music_params, frame_index)
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        frame = np.array(image)
        return frame
    
    def _draw_emotion_visualization(self, draw, music_params: MusicParameter, frame_index: int):
        """ç»˜åˆ¶æƒ…ç»ªå¯è§†åŒ–"""
        # åŸºäºæƒ…ç»ªæ˜ å°„ç»˜åˆ¶æ¸å˜èƒŒæ™¯
        center_x, center_y = self.width // 2, self.height // 2
        
        # æƒ…ç»ªå¼ºåº¦å½±å“åœ†å½¢å¤§å°
        radius = int(200 + 100 * abs(music_params.valence_mapping))
        
        # æƒ…ç»ªæ•ˆä»·å½±å“é¢œè‰²
        if music_params.valence_mapping > 0:
            color = (100, 150, 255)  # ç§¯ææƒ…ç»ª - è“è‰²
        else:
            color = (255, 150, 100)  # æ¶ˆææƒ…ç»ª - æ©™è‰²
        
        # ç»˜åˆ¶è„‰åŠ¨åœ†å½¢
        pulse_factor = 0.8 + 0.2 * np.sin(frame_index * 0.1)
        current_radius = int(radius * pulse_factor)
        
        draw.ellipse([center_x - current_radius, center_y - current_radius,
                     center_x + current_radius, center_y + current_radius],
                    fill=color, outline=None)
    
    def _draw_therapy_elements(self, draw, music_params: MusicParameter, frame_index: int):
        """ç»˜åˆ¶æ²»ç–—æ€§å…ƒç´ """
        # æ ¹æ®ISOé˜¶æ®µç»˜åˆ¶ä¸åŒçš„æ²»ç–—å…ƒç´ 
        if music_params.iso_stage == 'synchronization':
            self._draw_sync_patterns(draw, frame_index)
        elif music_params.iso_stage == 'guidance':
            self._draw_guidance_flow(draw, frame_index)
        elif music_params.iso_stage == 'consolidation':
            self._draw_consolidation_calm(draw, frame_index)
    
    def _draw_sync_patterns(self, draw, frame_index: int):
        """ç»˜åˆ¶åŒæ­¥æ¨¡å¼"""
        # ç»˜åˆ¶åŒæ­¥åœ†ç¯
        center_x, center_y = self.width // 2, self.height // 2
        for i in range(3):
            radius = 100 + i * 50
            alpha = int(100 * (1 - i * 0.3))
            color = (255, 255, 255, alpha)
            
            draw.ellipse([center_x - radius, center_y - radius,
                         center_x + radius, center_y + radius],
                        outline=color, width=2)
    
    def _draw_guidance_flow(self, draw, frame_index: int):
        """ç»˜åˆ¶å¼•å¯¼æµåŠ¨"""
        # ç»˜åˆ¶æµåŠ¨æ›²çº¿
        y_offset = int(50 * np.sin(frame_index * 0.05))
        
        for i in range(0, self.width, 20):
            x = i
            y = self.height // 2 + y_offset + int(20 * np.sin(i * 0.01))
            draw.ellipse([x - 5, y - 5, x + 5, y + 5], fill=(200, 200, 255))
    
    def _draw_consolidation_calm(self, draw, frame_index: int):
        """ç»˜åˆ¶å·©å›ºå®é™"""
        # ç»˜åˆ¶é™æ€æ˜Ÿç‚¹
        np.random.seed(42)  # å›ºå®šç§å­ç¡®ä¿æ˜Ÿç‚¹ä½ç½®ä¸€è‡´
        for _ in range(50):
            x = np.random.randint(0, self.width)
            y = np.random.randint(0, self.height)
            size = np.random.randint(1, 3)
            
            # æ˜Ÿç‚¹é—ªçƒæ•ˆæœ
            brightness = int(100 + 50 * np.sin(frame_index * 0.02 + x * 0.001))
            color = (brightness, brightness, brightness)
            
            draw.ellipse([x - size, y - size, x + size, y + size], fill=color)
    
    def generate_video_sequence(self, music_params: MusicParameter, 
                              duration: float) -> List[np.ndarray]:
        """ç”Ÿæˆè§†é¢‘åºåˆ—"""
        total_frames = int(duration * self.fps)
        frames = []
        
        logger.info(f"ğŸ¬ å¼€å§‹ç”Ÿæˆè§†é¢‘åºåˆ—ï¼Œå…±{total_frames}å¸§ï¼ˆé¢„è®¡è€—æ—¶: {total_frames/15:.1f}ç§’ï¼‰")
        
        for frame_index in range(total_frames):
            frame = self.generate_frame(frame_index, music_params)
            frames.append(frame)
            
            # æ¯10å¸§è®°å½•ä¸€æ¬¡è¿›åº¦ï¼ˆ15fpsæ—¶çº¦æ¯0.67ç§’ï¼‰
            if frame_index % 10 == 0 or frame_index == total_frames - 1:
                progress = (frame_index + 1) / total_frames * 100
                logger.info(f"ğŸ“¹ è§†é¢‘ç”Ÿæˆè¿›åº¦: {frame_index+1}/{total_frames} ({progress:.1f}%)")
        
        logger.info(f"âœ… è§†é¢‘åºåˆ—ç”Ÿæˆå®Œæˆ")
        return frames

class GenerationLayer(BaseLayer):
    """ç”Ÿæˆå±‚ - å®æ—¶éŸ³è§†é¢‘å†…å®¹ç”Ÿæˆ"""
    
    def __init__(self, config: GenerationLayerConfig):
        super().__init__(config)
        self.config = config
        self.layer_name = "generation_layer"
        
        # åˆå§‹åŒ–ç”Ÿæˆå™¨
        self.audio_generator = ProceduralAudioGenerator(config) if config.audio_enabled else None
        self.video_generator = ProceduralVideoGenerator(config) if config.video_enabled else None
        
        # åˆå§‹åŒ–ISOåŸåˆ™
        if config.iso_stage_aware:
            self.iso_principle = ISOPrinciple()
        else:
            self.iso_principle = None
        
        # æ€§èƒ½ç›‘æ§
        self.performance_monitor = PerformanceMonitor()
        
        # ç”Ÿæˆç¼“å­˜
        self.content_cache = {} if config.enable_caching else None
        
        logger.info(f"ç”Ÿæˆå±‚åˆå§‹åŒ–å®Œæˆ - éŸ³é¢‘: {config.audio_enabled}, è§†é¢‘: {config.video_enabled}")
    
    def _extract_music_parameters(self, input_data: Dict[str, Any]) -> MusicParameter:
        """ä»æ˜ å°„å±‚è¾“å‡ºæå–éŸ³ä¹å‚æ•°"""
        music_params_data = input_data.get('music_parameters', {})
        
        # å¦‚æœæ˜¯å­—å…¸æ ¼å¼ï¼Œè½¬æ¢ä¸ºMusicParameterå¯¹è±¡
        if isinstance(music_params_data, dict):
            return MusicParameter(music_params_data)
        elif hasattr(music_params_data, 'tempo_bpm'):
            # å·²ç»æ˜¯MusicParameterå¯¹è±¡
            return music_params_data
        else:
            # ä½¿ç”¨é»˜è®¤å‚æ•°
            logger.warning("æ— æ³•è§£æéŸ³ä¹å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            return MusicParameter()
    
    def _generate_audio_content(self, music_params: MusicParameter) -> Dict[str, Any]:
        """ç”ŸæˆéŸ³é¢‘å†…å®¹"""
        if not self.audio_generator:
            return {'error': 'éŸ³é¢‘ç”Ÿæˆå™¨æœªå¯ç”¨'}
        
        try:
            # ç”ŸæˆéŸ³é¢‘
            audio_data = self.audio_generator.generate_ambient_texture(
                music_params, 
                self.config.audio_duration
            )
            
            # å½’ä¸€åŒ–éŸ³é¢‘
            audio_data = audio_data / np.max(np.abs(audio_data))
            
            # è½¬æ¢ä¸ºå­—èŠ‚æµï¼ˆå¦‚æœéœ€è¦ï¼‰
            audio_bytes = None
            if AUDIO_AVAILABLE:
                try:
                    buffer = io.BytesIO()
                    sf.write(buffer, audio_data, self.config.output_sample_rate, format='WAV')
                    audio_bytes = buffer.getvalue()
                except Exception as e:
                    logger.warning(f"éŸ³é¢‘ç¼–ç å¤±è´¥: {e}")
            
            return {
                'audio_array': audio_data,
                'audio_bytes': audio_bytes,
                'sample_rate': self.config.output_sample_rate,
                'channels': self.config.output_channels,
                'duration': self.config.audio_duration,
                'format': 'WAV'
            }
            
        except Exception as e:
            logger.error(f"éŸ³é¢‘ç”Ÿæˆå¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _generate_video_content(self, music_params: MusicParameter) -> Dict[str, Any]:
        """ç”Ÿæˆè§†é¢‘å†…å®¹"""
        if not self.video_generator:
            return {'error': 'è§†é¢‘ç”Ÿæˆå™¨æœªå¯ç”¨'}
        
        try:
            # ç”Ÿæˆè§†é¢‘å¸§åºåˆ—
            frames = self.video_generator.generate_video_sequence(
                music_params, 
                self.config.video_duration
            )
            
            return {
                'frames': frames,
                'fps': self.config.video_fps,
                'resolution': self.config.video_resolution,
                'duration': self.config.video_duration,
                'total_frames': len(frames),
                'format': 'RGB'
            }
            
        except Exception as e:
            logger.error(f"è§†é¢‘ç”Ÿæˆå¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _synchronize_audio_video(self, audio_data: Dict[str, Any], 
                               video_data: Dict[str, Any]) -> Dict[str, Any]:
        """åŒæ­¥éŸ³è§†é¢‘å†…å®¹"""
        # æ£€æŸ¥æ—¶é•¿ä¸€è‡´æ€§
        audio_duration = audio_data.get('duration', 0)
        video_duration = video_data.get('duration', 0)
        
        if abs(audio_duration - video_duration) > 0.1:  # 100msè¯¯å·®å®¹å¿
            logger.warning(f"éŸ³è§†é¢‘æ—¶é•¿ä¸ä¸€è‡´: éŸ³é¢‘{audio_duration}s, è§†é¢‘{video_duration}s")
        
        # åˆ›å»ºåŒæ­¥å…ƒæ•°æ®
        sync_metadata = {
            'audio_duration': audio_duration,
            'video_duration': video_duration,
            'synchronized': True,
            'sync_method': 'temporal_alignment',
            'sync_accuracy': 1.0 - abs(audio_duration - video_duration) / max(audio_duration, video_duration)
        }
        
        return {
            'audio': audio_data,
            'video': video_data,
            'sync_metadata': sync_metadata
        }
    
    def _cache_content(self, cache_key: str, content: Dict[str, Any]):
        """ç¼“å­˜ç”Ÿæˆçš„å†…å®¹"""
        if self.content_cache is not None:
            # ç®€å•çš„LRUç¼“å­˜
            if len(self.content_cache) >= self.config.cache_size:
                # ç§»é™¤æœ€æ—§çš„æ¡ç›®
                oldest_key = next(iter(self.content_cache))
                del self.content_cache[oldest_key]
            
            self.content_cache[cache_key] = content
            logger.debug(f"å†…å®¹å·²ç¼“å­˜: {cache_key}")
    
    def _get_cached_content(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """è·å–ç¼“å­˜çš„å†…å®¹"""
        if self.content_cache is not None:
            return self.content_cache.get(cache_key)
        return None
    
    def _generate_cache_key(self, music_params: MusicParameter) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # åŸºäºéŸ³ä¹å‚æ•°ç”Ÿæˆå“ˆå¸Œé”®
        key_data = {
            'tempo': music_params.tempo_bpm,
            'key': music_params.key_signature,
            'iso_stage': music_params.iso_stage,
            'valence': music_params.valence_mapping,
            'arousal': music_params.arousal_mapping
        }
        
        import hashlib
        key_str = json.dumps(key_data, sort_keys=True)
        return hashlib.md5(key_str.encode()).hexdigest()
    
    async def _process_impl(self, input_data: LayerData) -> LayerData:
        """ç”Ÿæˆå±‚å¤„ç†å®ç°"""
        self.performance_monitor.start_timer("generation_layer_processing")
        
        try:
            # éªŒè¯è¾“å…¥æ•°æ®
            if not input_data.data:
                raise ValueError("è¾“å…¥æ•°æ®ä¸ºç©º")
            
            # æå–éŸ³ä¹å‚æ•°
            music_params = self._extract_music_parameters(input_data.data)
            
            # ç”Ÿæˆç¼“å­˜é”®
            cache_key = self._generate_cache_key(music_params)
            
            # æ£€æŸ¥ç¼“å­˜
            cached_content = self._get_cached_content(cache_key)
            if cached_content:
                logger.info(f"ä½¿ç”¨ç¼“å­˜å†…å®¹: {cache_key}")
                content = cached_content
            else:
                # ç”Ÿæˆæ–°å†…å®¹
                content = {}
                
                # ç”ŸæˆéŸ³é¢‘å†…å®¹
                if self.config.audio_enabled:
                    audio_content = self._generate_audio_content(music_params)
                    content['audio'] = audio_content
                
                # ç”Ÿæˆè§†é¢‘å†…å®¹
                if self.config.video_enabled:
                    video_content = self._generate_video_content(music_params)
                    content['video'] = video_content
                
                # éŸ³è§†é¢‘åŒæ­¥
                if self.config.audio_enabled and self.config.video_enabled:
                    if 'error' not in content['audio'] and 'error' not in content['video']:
                        content = self._synchronize_audio_video(content['audio'], content['video'])
                
                # ç¼“å­˜å†…å®¹
                self._cache_content(cache_key, content)
            
            # åˆ›å»ºè¾“å‡ºæ•°æ®
            output_data = LayerData(
                layer_name=self.layer_name,
                timestamp=datetime.now(),
                data={
                    'generated_content': content,
                    'music_parameters': music_params.__dict__,
                    'generation_info': {
                        'strategy': self.config.generation_strategy,
                        'content_type': self.config.content_type,
                        'cached': cached_content is not None,
                        'cache_key': cache_key
                    }
                },
                metadata={
                    'source_layer': input_data.layer_name,
                    'generation_strategy': self.config.generation_strategy,
                    'content_types': [t for t in ['audio', 'video'] if getattr(self.config, f"{t}_enabled")]
                },
                confidence=input_data.confidence
            )
            
            # è®°å½•å¤„ç†æ—¶é—´
            processing_time = self.performance_monitor.end_timer("generation_layer_processing")
            output_data.processing_time = processing_time
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.total_processed += 1
            self.total_processing_time += processing_time
            
            logger.info(f"ç”Ÿæˆå±‚å¤„ç†å®Œæˆ - å†…å®¹ç±»å‹: {self.config.content_type}, "
                       f"ç¼“å­˜: {cached_content is not None}, è€—æ—¶: {processing_time*1000:.1f}ms")
            
            return output_data
            
        except Exception as e:
            self.error_count += 1
            logger.error(f"ç”Ÿæˆå±‚å¤„ç†å¤±è´¥: {e}")
            
            # åˆ›å»ºé”™è¯¯è¾“å‡º
            error_data = LayerData(
                layer_name=self.layer_name,
                timestamp=datetime.now(),
                data={
                    'error': str(e),
                    'generated_content': {}
                },
                metadata={'error': True, 'source_layer': input_data.layer_name},
                confidence=0.0
            )
            
            processing_time = self.performance_monitor.end_timer("generation_layer_processing")
            error_data.processing_time = processing_time
            
            return error_data
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        if self.content_cache is not None:
            self.content_cache.clear()
            logger.info("ç”Ÿæˆå±‚ç¼“å­˜å·²æ¸…ç©º")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        if self.content_cache is not None:
            return {
                'cache_size': len(self.content_cache),
                'max_cache_size': self.config.cache_size,
                'cache_enabled': True
            }
        return {'cache_enabled': False}
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–ç”Ÿæˆå±‚çŠ¶æ€"""
        base_status = super().get_status()
        
        # æ·»åŠ ç”Ÿæˆå±‚ç‰¹æœ‰çš„çŠ¶æ€ä¿¡æ¯
        generation_status = {
            'generation_strategy': self.config.generation_strategy,
            'content_type': self.config.content_type,
            'audio_enabled': self.config.audio_enabled,
            'video_enabled': self.config.video_enabled,
            'audio_available': AUDIO_AVAILABLE,
            'video_available': VIDEO_AVAILABLE,
            'cache_stats': self.get_cache_stats(),
            'performance_stats': self.performance_monitor.get_all_stats()
        }
        
        base_status.update(generation_status)
        return base_status