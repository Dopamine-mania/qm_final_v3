#!/usr/bin/env python3
"""
ğŸŒ™ ç¡çœ ç–—æ„ˆAI - ç®€åŒ–å¢å¼ºæ¼”ç¤ºæ¨¡å¼
ä¿®å¤ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜ï¼Œæä¾›å®Œæ•´çš„è¿›åº¦åé¦ˆ
"""

import gradio as gr
import numpy as np
import cv2
import os
import tempfile
import time
from pathlib import Path
from datetime import datetime
import json

def update_progress_info(step_name, progress, total_steps, details=""):
    """ç”Ÿæˆè¿›åº¦ä¿¡æ¯"""
    if progress == 0:
        return "ğŸ¯ ç­‰å¾…å¼€å§‹å¤„ç†..."
    elif progress >= total_steps:
        return "âœ… å¤„ç†å®Œæˆï¼"
    else:
        progress_bar = "â–ˆ" * progress + "â–‘" * (total_steps - progress)
        return f"{step_name}\n[{progress_bar}] {progress}/{total_steps}\n{details}"

def generate_mock_audio_with_feedback(duration=15, sample_rate=44100):
    """ç”Ÿæˆæ¨¡æ‹ŸéŸ³é¢‘å¹¶è¿”å›è¿›åº¦ä¿¡æ¯"""
    print(f"ğŸµ å¼€å§‹ç”Ÿæˆ{duration}ç§’éŸ³é¢‘ï¼Œé‡‡æ ·ç‡{sample_rate}Hz")
    
    # æ¨¡æ‹ŸéŸ³é¢‘ç”Ÿæˆè¿‡ç¨‹
    stage_duration = duration // 3
    t = np.linspace(0, stage_duration, int(sample_rate * stage_duration))
    
    # ç¬¬ä¸€é˜¶æ®µï¼šåŒæ­¥æœŸ
    print("ğŸµ ç¬¬ä¸€é˜¶æ®µ - åŒæ­¥æœŸï¼šç”Ÿæˆé«˜é¢‘åŒ¹é…éŸ³é¢‘...")
    time.sleep(0.3)
    stage1_freq = 440  # A4
    stage1 = 0.3 * np.sin(2 * np.pi * stage1_freq * t) * np.exp(-t/5)
    
    # ç¬¬äºŒé˜¶æ®µï¼šå¼•å¯¼æœŸ
    print("ğŸµ ç¬¬äºŒé˜¶æ®µ - å¼•å¯¼æœŸï¼šç”Ÿæˆè¿‡æ¸¡å¼•å¯¼éŸ³é¢‘...")
    time.sleep(0.3)
    stage2_freq = 330  # E4
    stage2 = 0.2 * np.sin(2 * np.pi * stage2_freq * t) * np.exp(-t/8)
    
    # ç¬¬ä¸‰é˜¶æ®µï¼šå·©å›ºæœŸ
    print("ğŸµ ç¬¬ä¸‰é˜¶æ®µ - å·©å›ºæœŸï¼šç”Ÿæˆä½é¢‘æ”¾æ¾éŸ³é¢‘...")
    time.sleep(0.3)
    stage3_freq = 220  # A3
    stage3 = 0.1 * np.sin(2 * np.pi * stage3_freq * t) * np.exp(-t/12)
    
    # åˆå¹¶å’Œåå¤„ç†
    print("ğŸµ éŸ³é¢‘åå¤„ç†ï¼šåˆå¹¶ä¸‰é˜¶æ®µéŸ³é¢‘...")
    audio_array = np.concatenate([stage1, stage2, stage3])
    
    # æ·»åŠ ç¯å¢ƒéŸ³æ•ˆ
    noise = 0.05 * np.random.normal(0, 1, len(audio_array))
    audio_array = audio_array + noise
    
    # å½’ä¸€åŒ–
    audio_array = audio_array / np.max(np.abs(audio_array))
    
    return audio_array.astype(np.float32), sample_rate

def generate_mock_video_with_feedback(duration=15, fps=30):
    """ç”Ÿæˆæ¨¡æ‹Ÿè§†é¢‘å¹¶è¿”å›è¿›åº¦ä¿¡æ¯"""
    print(f"ğŸ¬ å¼€å§‹ç”Ÿæˆ{duration}ç§’è§†é¢‘ï¼Œå¸§ç‡{fps}fps")
    
    frame_count = int(duration * fps)
    frames = []
    width, height = 640, 480
    
    for i in range(frame_count):
        # æ¯30å¸§ï¼ˆ1ç§’ï¼‰è¾“å‡ºä¸€æ¬¡è¿›åº¦
        if i % 30 == 0:
            current_second = i // fps
            print(f"ğŸ¬ æ¸²æŸ“è¿›åº¦ï¼šç¬¬{current_second+1}ç§’ ({i+1}/{frame_count}å¸§)")
        
        # åˆ›å»ºæ¸å˜èƒŒæ™¯
        frame = np.zeros((height, width, 3), dtype=np.uint8)
        
        # æ—¶é—´è¿›åº¦
        progress = i / frame_count
        
        # ä¸‰é˜¶æ®µé¢œè‰²å˜åŒ–
        if progress < 0.33:  # ç¬¬ä¸€é˜¶æ®µï¼šåŒæ­¥æœŸ
            blue = int(255 * (1 - progress * 3))
            green = int(100 * progress * 3)
            red = int(50 * progress * 3)
            stage_name = "åŒæ­¥æœŸ"
        elif progress < 0.66:  # ç¬¬äºŒé˜¶æ®µï¼šå¼•å¯¼æœŸ
            stage_progress = (progress - 0.33) / 0.33
            blue = int(100 + 155 * (1 - stage_progress))
            green = int(100 * (1 - stage_progress))
            red = int(50 + 100 * stage_progress)
            stage_name = "å¼•å¯¼æœŸ"
        else:  # ç¬¬ä¸‰é˜¶æ®µï¼šå·©å›ºæœŸ
            stage_progress = (progress - 0.66) / 0.34
            blue = int(50 + 50 * (1 - stage_progress))
            green = int(20 * (1 - stage_progress))
            red = int(20 + 30 * (1 - stage_progress))
            stage_name = "å·©å›ºæœŸ"
        
        # å¡«å……æ¸å˜èƒŒæ™¯
        for y in range(height):
            for x in range(width):
                center_x, center_y = width // 2, height // 2
                distance = np.sqrt((x - center_x)**2 + (y - center_y)**2)
                max_distance = np.sqrt(center_x**2 + center_y**2)
                gradient = 1 - (distance / max_distance)
                
                frame[y, x] = [
                    int(blue * gradient),
                    int(green * gradient),
                    int(red * gradient)
                ]
        
        # æ·»åŠ å‘¼å¸æ•ˆæœåœ†åœˆ
        breathing_radius = 50 + 30 * np.sin(progress * 4 * np.pi)
        cv2.circle(frame, (width//2, height//2), int(breathing_radius), (255, 255, 255), 2)
        
        # æ·»åŠ é˜¶æ®µæ–‡å­—
        cv2.putText(frame, stage_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
        
        frames.append(frame)
    
    return frames, fps

def create_demo_video_with_feedback(audio_array, sample_rate, video_frames, fps):
    """åˆ›å»ºæ¼”ç¤ºè§†é¢‘å¹¶è¿”å›è¿›åº¦ä¿¡æ¯"""
    try:
        print("ğŸ¬ å¼€å§‹åˆæˆéŸ³ç”»åŒæ­¥è§†é¢‘...")
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        temp_dir = tempfile.mkdtemp()
        audio_path = os.path.join(temp_dir, "demo_audio.wav")
        video_path = os.path.join(temp_dir, "demo_video.mp4")
        output_path = os.path.join(temp_dir, "demo_synchronized.mp4")
        
        # ä¿å­˜éŸ³é¢‘
        print(f"ğŸ’¾ ä¿å­˜éŸ³é¢‘æ–‡ä»¶åˆ°: {audio_path}")
        import soundfile as sf
        sf.write(audio_path, audio_array, sample_rate)
        
        # åˆ›å»ºè§†é¢‘
        print(f"ğŸ¬ åˆ›å»ºè§†é¢‘æ–‡ä»¶ï¼Œå¤„ç†{len(video_frames)}å¸§...")
        if len(video_frames) > 0:
            frame_height, frame_width = video_frames[0].shape[:2]
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(video_path, fourcc, fps, (frame_width, frame_height))
            
            for i, frame in enumerate(video_frames):
                if i % 90 == 0:  # æ¯3ç§’æ›´æ–°ä¸€æ¬¡
                    print(f"ğŸ“ å†™å…¥è§†é¢‘å¸§ï¼š{i+1}/{len(video_frames)}")
                out.write(frame)
            
            out.release()
            
            # ä½¿ç”¨ffmpegåˆæˆ
            print("ğŸ”„ ä½¿ç”¨FFmpegè¿›è¡ŒéŸ³ç”»åŒæ­¥...")
            try:
                import subprocess
                ffmpeg_cmd = [
                    'ffmpeg', '-y', '-loglevel', 'error',
                    '-i', video_path,
                    '-i', audio_path,
                    '-c:v', 'libx264',
                    '-c:a', 'aac',
                    '-strict', 'experimental',
                    '-shortest',
                    output_path
                ]
                
                result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True)
                if result.returncode == 0:
                    print(f"âœ… éŸ³ç”»åŒæ­¥è§†é¢‘ç”ŸæˆæˆåŠŸ: {output_path}")
                    return output_path
                else:
                    print(f"âš ï¸ FFmpegä¸å¯ç”¨ï¼Œè¿”å›çº¯è§†é¢‘æ–‡ä»¶: {video_path}")
                    return video_path
            except Exception as e:
                print(f"âš ï¸ FFmpegé”™è¯¯: {str(e)}ï¼Œè¿”å›çº¯è§†é¢‘æ–‡ä»¶")
                return video_path
        else:
            print("âŒ æ²¡æœ‰è§†é¢‘å¸§ï¼Œç”Ÿæˆå¤±è´¥")
            return None
            
    except Exception as e:
        print(f"âŒ åˆ›å»ºè§†é¢‘å¤±è´¥: {str(e)}")
        return None

def enhanced_demo_processing_with_progress(user_input, progress_update):
    """å¢å¼ºçš„æ¼”ç¤ºæ¨¡å¼å¤„ç†ï¼Œå¸¦è¿›åº¦æ›´æ–°"""
    if not user_input or len(user_input.strip()) < 5:
        return "âš ï¸ è¯·è¾“å…¥è‡³å°‘5ä¸ªå­—ç¬¦çš„æƒ…ç»ªæè¿°", None, None, "è¾“å…¥å¤ªçŸ­"
    
    try:
        total_steps = 7
        current_step = 0
        
        # æ­¥éª¤1: æƒ…ç»ªè¯†åˆ«
        current_step += 1
        progress_info = update_progress_info("ğŸ§  æƒ…ç»ªè¯†åˆ«åˆ†æ", current_step, total_steps, "åˆ†æç”¨æˆ·è¾“å…¥çš„æƒ…ç»ªçŠ¶æ€...")
        print(f"æ­¥éª¤ {current_step}/{total_steps}: æƒ…ç»ªè¯†åˆ«åˆ†æ")
        time.sleep(0.5)
        
        # æ¨¡æ‹Ÿæƒ…ç»ªè¯†åˆ«
        emotions = {
            "ç„¦è™‘": {"confidence": 0.85, "type": "ç¡å‰ç„¦è™‘çŠ¶æ€"},
            "ç–²æƒ«": {"confidence": 0.82, "type": "èº«ä½“ç–²æƒ«çŠ¶æ€"},
            "çƒ¦èº": {"confidence": 0.88, "type": "æƒ…ç»ªçƒ¦èºçŠ¶æ€"},
            "å¹³é™": {"confidence": 0.75, "type": "ç›¸å¯¹å¹³é™çŠ¶æ€"},
            "å‹åŠ›": {"confidence": 0.90, "type": "å¿ƒç†å‹åŠ›çŠ¶æ€"}
        }
        
        detected_emotion = "ç„¦è™‘"
        for emotion_key in emotions.keys():
            if emotion_key in user_input:
                detected_emotion = emotion_key
                break
        
        emotion_info = emotions[detected_emotion]
        
        # æ­¥éª¤2: åˆ¶å®šæ²»ç–—æ–¹æ¡ˆ
        current_step += 1
        progress_info = update_progress_info("ğŸ“‹ åˆ¶å®šæ²»ç–—æ–¹æ¡ˆ", current_step, total_steps, f"åŸºäº{detected_emotion}æƒ…ç»ªåˆ¶å®šä¸‰é˜¶æ®µæ²»ç–—æ–¹æ¡ˆ...")
        print(f"æ­¥éª¤ {current_step}/{total_steps}: åˆ¶å®šæ²»ç–—æ–¹æ¡ˆ")
        time.sleep(0.5)
        
        # æ­¥éª¤3: ç”ŸæˆéŸ³é¢‘
        current_step += 1
        progress_info = update_progress_info("ğŸµ ç”Ÿæˆä¸‰é˜¶æ®µéŸ³é¢‘", current_step, total_steps, "ç”ŸæˆåŒæ­¥â†’å¼•å¯¼â†’å·©å›ºéŸ³é¢‘...")
        print(f"æ­¥éª¤ {current_step}/{total_steps}: ç”Ÿæˆä¸‰é˜¶æ®µéŸ³é¢‘")
        audio_array, sample_rate = generate_mock_audio_with_feedback(duration=15)
        
        # æ­¥éª¤4: ç”Ÿæˆè§†é¢‘
        current_step += 1
        progress_info = update_progress_info("ğŸ¬ ç”Ÿæˆç–—æ„ˆè§†é¢‘", current_step, total_steps, "ç”Ÿæˆä¸‰é˜¶æ®µè§†è§‰å†…å®¹...")
        print(f"æ­¥éª¤ {current_step}/{total_steps}: ç”Ÿæˆç–—æ„ˆè§†é¢‘")
        video_frames, fps = generate_mock_video_with_feedback(duration=15, fps=30)
        
        # æ­¥éª¤5: åˆæˆè§†é¢‘
        current_step += 1
        progress_info = update_progress_info("ğŸ¬ åˆæˆéŸ³ç”»åŒæ­¥", current_step, total_steps, "ä½¿ç”¨OpenCVå’ŒFFmpegåˆæˆ...")
        print(f"æ­¥éª¤ {current_step}/{total_steps}: åˆæˆéŸ³ç”»åŒæ­¥")
        video_path = create_demo_video_with_feedback(audio_array, sample_rate, video_frames, fps)
        
        # æ­¥éª¤6: åå¤„ç†
        current_step += 1
        progress_info = update_progress_info("ğŸ”§ åå¤„ç†ä¼˜åŒ–", current_step, total_steps, "ä¼˜åŒ–è§†é¢‘è´¨é‡å’ŒéŸ³é¢‘æ•ˆæœ...")
        print(f"æ­¥éª¤ {current_step}/{total_steps}: åå¤„ç†ä¼˜åŒ–")
        time.sleep(0.5)
        
        # æ­¥éª¤7: å®Œæˆ
        current_step += 1
        progress_info = update_progress_info("ğŸ‰ ç”Ÿæˆå®Œæˆ", current_step, total_steps, "éŸ³ç”»åŒæ­¥ç–—æ„ˆè§†é¢‘å·²å‡†å¤‡å°±ç»ªï¼")
        print(f"æ­¥éª¤ {current_step}/{total_steps}: ç”Ÿæˆå®Œæˆ")
        
        # ç»„ç»‡è¿”å›ä¿¡æ¯
        emotion_result = f"""ğŸ§  æƒ…ç»ªè¯†åˆ«ç»“æœ:
æƒ…ç»ªç±»å‹: {detected_emotion}
ç½®ä¿¡åº¦: {emotion_info['confidence']:.1%}
çŠ¶æ€æè¿°: {emotion_info['type']}
å¤„ç†æ—¶é—´: {datetime.now().strftime('%H:%M:%S')}"""
        
        if video_path:
            info_text = f"""âœ… æ¼”ç¤ºæ¨¡å¼ç”Ÿæˆå®Œæˆï¼

ğŸµ éŸ³é¢‘ä¿¡æ¯:
  - æ—¶é•¿: 15ç§’ (3ä¸ªé˜¶æ®µå„5ç§’)
  - é‡‡æ ·ç‡: {sample_rate}Hz
  - å£°é“: ç«‹ä½“å£°
  - ä¸‰é˜¶æ®µè®¾è®¡: åŒæ­¥(440Hz) â†’ å¼•å¯¼(330Hz) â†’ å·©å›º(220Hz)

ğŸ¬ è§†é¢‘ä¿¡æ¯:
  - æ€»å¸§æ•°: {len(video_frames)}å¸§
  - å¸§ç‡: {fps}fps
  - åˆ†è¾¨ç‡: 640x480
  - è§†è§‰æ•ˆæœ: æ¸å˜é¢œè‰² + å‘¼å¸åœ†åœˆ + é˜¶æ®µæ ‡è¯†

ğŸ”§ å¤„ç†æµç¨‹:
  - æ­¥éª¤1: æƒ…ç»ªè¯†åˆ«åˆ†æ âœ…
  - æ­¥éª¤2: åˆ¶å®šæ²»ç–—æ–¹æ¡ˆ âœ…
  - æ­¥éª¤3: ç”Ÿæˆä¸‰é˜¶æ®µéŸ³é¢‘ âœ…
  - æ­¥éª¤4: ç”Ÿæˆç–—æ„ˆè§†é¢‘ âœ…
  - æ­¥éª¤5: åˆæˆéŸ³ç”»åŒæ­¥ âœ…
  - æ­¥éª¤6: åå¤„ç†ä¼˜åŒ– âœ…
  - æ­¥éª¤7: ç”Ÿæˆå®Œæˆ âœ…

ğŸ“ æ³¨æ„äº‹é¡¹:
  - è¿™æ˜¯æ¼”ç¤ºæ¨¡å¼ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
  - çœŸå®ç‰ˆæœ¬éœ€è¦é…ç½®å•†ä¸šAPI
  - æ¼”ç¤ºéŸ³é¢‘é‡‡ç”¨æ•°å­¦åˆæˆ
  - å®Œæ•´å¤„ç†æ—¶é—´çº¦15-20ç§’"""
            
            return emotion_result, video_path, (sample_rate, audio_array), info_text
        else:
            return emotion_result, None, (sample_rate, audio_array), "âŒ è§†é¢‘ç”Ÿæˆå¤±è´¥"
    
    except Exception as e:
        error_msg = f"âŒ å¤„ç†é”™è¯¯: {str(e)}"
        print(error_msg)
        import traceback
        traceback.print_exc()
        return error_msg, None, None, f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}"

def create_simple_interface():
    """åˆ›å»ºç®€åŒ–ä½†åŠŸèƒ½å®Œæ•´çš„ç•Œé¢"""
    with gr.Blocks(
        title="ğŸŒ™ ç¡çœ ç–—æ„ˆAI - æ¼”ç¤ºæ¨¡å¼",
        theme=gr.themes.Soft(primary_hue="purple", secondary_hue="blue")
    ) as app:
        
        gr.HTML("""
        <div style="text-align: center; padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; color: white; margin-bottom: 20px;">
            <h1>ğŸŒ™ ç¡çœ ç–—æ„ˆAI - æ¼”ç¤ºæ¨¡å¼</h1>
            <p>å®Œæ•´çš„ä¸‰é˜¶æ®µéŸ³ä¹å™äº‹ç–—æ„ˆç³»ç»Ÿ</p>
            <p style="color: #ffeb3b;">âš ï¸ æ¼”ç¤ºæ¨¡å¼ï¼šä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼Œå±•ç¤ºå®Œæ•´åŠŸèƒ½</p>
        </div>
        """)
        
        with gr.Row():
            with gr.Column(scale=2):
                gr.Markdown("### ğŸ’­ æƒ…ç»ªè¾“å…¥")
                
                # å¿«é€Ÿé€‰æ‹©ç¤ºä¾‹
                example_emotions = [
                    "ğŸ˜° æˆ‘æ„Ÿåˆ°å¾ˆç„¦è™‘ï¼Œå¿ƒè·³åŠ é€Ÿï¼Œéš¾ä»¥å…¥ç¡",
                    "ğŸ˜´ æˆ‘å¾ˆç–²æƒ«ï¼Œä½†å¤§è„‘è¿˜åœ¨æ´»è·ƒï¼Œæ— æ³•æ”¾æ¾",
                    "ğŸ˜¤ æˆ‘æ„Ÿåˆ°çƒ¦èºä¸å®‰ï¼Œå®¹æ˜“è¢«å°äº‹å½±å“",
                    "ğŸ˜Œ æˆ‘æ¯”è¾ƒå¹³é™ï¼Œä½†å¸Œæœ›æ›´æ·±å±‚çš„æ”¾æ¾",
                    "ğŸ¤¯ æœ€è¿‘å‹åŠ›å¾ˆå¤§ï¼Œæ€»æ˜¯æ„Ÿåˆ°ç´§å¼ "
                ]
                
                emotion_examples = gr.Dropdown(
                    choices=example_emotions,
                    label="ğŸ­ å¿«é€Ÿé€‰æ‹©å¸¸è§æƒ…ç»ª",
                    value=example_emotions[0]
                )
                
                emotion_input = gr.Textbox(
                    label="âœï¸ æˆ–è‡ªå®šä¹‰æè¿°æ‚¨çš„æ„Ÿå—",
                    placeholder="æè¿°æ‚¨ç°åœ¨çš„æƒ…ç»ªçŠ¶æ€ã€èº«ä½“æ„Ÿå—ã€æ€ç»´çŠ¶æ€ç­‰...",
                    lines=4,
                    value="æˆ‘æ„Ÿåˆ°å¾ˆç„¦è™‘ï¼Œå¿ƒè·³åŠ é€Ÿï¼Œéš¾ä»¥å…¥ç¡"
                )
                
                process_btn = gr.Button("ğŸ¬ å¼€å§‹ç”ŸæˆéŸ³ç”»åŒæ­¥ç–—æ„ˆè§†é¢‘", variant="primary", size="lg")
                
                # è¿›åº¦æç¤º
                gr.HTML("""
                <div style="margin-top: 15px; padding: 10px; background-color: #f0f8ff; border-radius: 8px; border-left: 4px solid #1976d2;">
                    <strong>ğŸ“Š å¤„ç†æµç¨‹ï¼ˆçº¦15-20ç§’ï¼‰ï¼š</strong><br>
                    1. ğŸ§  æƒ…ç»ªè¯†åˆ«åˆ†æ<br>
                    2. ğŸ“‹ åˆ¶å®šæ²»ç–—æ–¹æ¡ˆ<br>
                    3. ğŸµ ç”Ÿæˆä¸‰é˜¶æ®µéŸ³é¢‘<br>
                    4. ğŸ¬ ç”Ÿæˆç–—æ„ˆè§†é¢‘<br>
                    5. ğŸ¬ åˆæˆéŸ³ç”»åŒæ­¥<br>
                    6. ğŸ”§ åå¤„ç†ä¼˜åŒ–<br>
                    7. ğŸ‰ ç”Ÿæˆå®Œæˆ
                </div>
                """)
            
            with gr.Column(scale=3):
                gr.Markdown("### ğŸ¬ ç”Ÿæˆç»“æœ")
                
                # æƒ…ç»ªè¯†åˆ«ç»“æœ
                emotion_result = gr.Textbox(
                    label="ğŸ§  æƒ…ç»ªè¯†åˆ«ç»“æœ",
                    lines=4,
                    interactive=False
                )
                
                # ä¸»è¦è¾“å‡ºï¼šéŸ³ç”»åŒæ­¥è§†é¢‘
                video_output = gr.Video(
                    label="ğŸ¬ ä¸‰é˜¶æ®µéŸ³ç”»åŒæ­¥ç–—æ„ˆè§†é¢‘",
                    height=400
                )
                
                # éŸ³é¢‘è¾“å‡º
                audio_output = gr.Audio(
                    label="ğŸµ ä¸‰é˜¶æ®µç–—æ„ˆéŸ³é¢‘",
                    type="numpy"
                )
                
                # è¯¦ç»†ä¿¡æ¯
                info_output = gr.Textbox(
                    label="ğŸ“Š è¯¦ç»†å¤„ç†ä¿¡æ¯",
                    lines=15,
                    interactive=False
                )
        
        # ä½¿ç”¨è¯´æ˜
        gr.HTML("""
        <div style="margin-top: 20px; padding: 15px; background: #f5f5f5; border-radius: 10px;">
            <h3>ğŸ¯ ä½¿ç”¨è¯´æ˜</h3>
            <ul>
                <li><strong>ğŸ§ å»ºè®®ä½©æˆ´è€³æœº</strong>ï¼šè·å¾—æœ€ä½³çš„ç«‹ä½“å£°æ•ˆæœ</li>
                <li><strong>ğŸ“± è§‚çœ‹è¿›åº¦</strong>ï¼šç»ˆç«¯ä¼šæ˜¾ç¤ºè¯¦ç»†çš„å¤„ç†è¿›åº¦</li>
                <li><strong>â±ï¸ å¤„ç†æ—¶é—´</strong>ï¼šæ•´ä¸ªè¿‡ç¨‹çº¦15-20ç§’</li>
                <li><strong>ğŸ¬ æœ€ç»ˆæ•ˆæœ</strong>ï¼š15ç§’éŸ³ç”»åŒæ­¥ç–—æ„ˆè§†é¢‘</li>
            </ul>
        </div>
        """)
        
        # äº‹ä»¶ç»‘å®š
        def update_input_from_dropdown(selected):
            return selected.split(" ", 1)[1] if " " in selected else selected
        
        emotion_examples.change(
            update_input_from_dropdown,
            inputs=emotion_examples,
            outputs=emotion_input
        )
        
        process_btn.click(
            lambda x: enhanced_demo_processing_with_progress(x, None),
            inputs=emotion_input,
            outputs=[emotion_result, video_output, audio_output, info_output]
        )
    
    return app

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨ç®€åŒ–å¢å¼ºæ¼”ç¤ºæ¨¡å¼...")
    print("ğŸ“Š åŒ…å«å®Œæ•´çš„è¿›åº¦æ˜¾ç¤ºï¼Œä½†ç®€åŒ–äº†å®šæ—¶æ›´æ–°")
    print("ğŸ¯ è¿›åº¦ä¿¡æ¯ä¸»è¦é€šè¿‡ç»ˆç«¯è¾“å‡ºæ˜¾ç¤º")
    
    app = create_simple_interface()
    app.launch(
        server_name="0.0.0.0",
        server_port=7865,
        share=True,
        debug=True
    )

if __name__ == "__main__":
    main()